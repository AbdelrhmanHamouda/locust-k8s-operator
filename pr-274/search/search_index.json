{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Performance testing that simply works","text":"<p>Utilize the full power of Locust in the cloud with a fully automated, cloud-native approach, creating professional and reliable performance tests in minutes.</p>      Get started in 5 minutes         View on GitHub"},{"location":"#find-your-path","title":"Find Your Path","text":"<ul> <li> <p> Evaluating Solutions?</p> <p>Compare the Locust Kubernetes Operator with alternatives in under 30 seconds.</p> <p> Compare alternatives</p> </li> <li> <p> Ready to Start?</p> <p>Deploy your first distributed load test on Kubernetes in 5 minutes with our step-by-step guide.</p> <p> Quick start guide</p> </li> <li> <p> Need API Details?</p> <p>Jump straight to the complete API field reference, resource configuration, and status lifecycle documentation.</p> <p> API Reference</p> </li> </ul>"},{"location":"#experience-the-power-of-v20","title":"\ud83d\ude80 Experience the Power of v2.0","text":"<ul> <li> <p> Rebuilt in Go</p> <p>Experience 60x faster startup times and a 4x smaller memory footprint. The entire operator has been rewritten in Go for maximum efficiency and reliability.</p> <p> Read the migration guide</p> </li> <li> <p> OpenTelemetry</p> <p>Gain deep visibility with built-in tracing and metrics. No sidecars required\u2014just pure, cloud-native observability.</p> <p> Learn more</p> </li> <li> <p> Secret Injection</p> <p>Securely manage your test credentials with native Kubernetes Secret and ConfigMap injection directly into your test pods.</p> <p> Learn more</p> </li> <li> <p> Volume Mounting</p> <p>Mount any storage volume to your master and worker pods for flexible test data and configuration management.</p> <p> Learn more</p> </li> </ul>"},{"location":"#build-for-cloud-native-performance-testing","title":"Build for cloud-native performance testing","text":"<p>The Operator is designed to unlock seamless &amp; effortless distributed performance testing in the cloud and enable continuous integration for CI/CD pipelines. By design, the entire system is cloud native and focuses on automation and CI practices. One strong feature about the system is its ability to horizontally scale to meet any required performance demands.</p>"},{"location":"#key-capabilities","title":"Key capabilities","text":"<ul> <li> <p> Cloud Native</p> <p>Leverage the full power of Kubernetes and cloud-native technologies for distributed performance testing.</p> <p> Learn more</p> </li> <li> <p> Automation &amp; CI</p> <p>Integrate performance testing directly into your CI/CD pipelines for continuous validation.</p> <p> Learn more</p> </li> <li> <p> Governance</p> <p>Maintain control over how resources are deployed and used in the cloud.</p> <p> Learn more</p> </li> <li> <p> Observability</p> <p>Gain insights into test results and infrastructure usage with built-in observability features.</p> <p> Learn more</p> </li> </ul> <p>Check out the full list of features!</p> <p></p>"},{"location":"#designed-for-teams-and-organizations","title":"Designed for teams and organizations","text":""},{"location":"#who-is-it-for","title":"Who is it for","text":"<p>It is built for performance engineers, DevOps teams, and organizations looking to integrate performance testing into their CI/CD pipelines.</p> <p></p>"},{"location":"#universal-deployment","title":"Universal deployment","text":"<p>Due to its design, the Operator can be deployed on any Kubernetes cluster. This means you can have a full cloud-native performance testing system anywhere in a matter of seconds.</p>"},{"location":"#scalable-resources","title":"Scalable resources","text":"<p>The only real limit to this approach is the amount of cluster resources a team or organization is willing to dedicate to performance testing. Scale up or down based on your needs.</p>"},{"location":"advanced_topics/","title":"Advanced Topics","text":"<p>Content reorganized</p> <p>These topics have been reorganized into focused How-To Guides for easier discovery and navigation. Find what you need using the mapping table below.</p>","tags":["advanced","configuration","how-to"]},{"location":"advanced_topics/#find-what-you-need","title":"Find what you need","text":"Topic New Location Kafka &amp; AWS MSK Configure Kafka integration Node Affinity Use node affinity Tolerations Configure tolerations Node Selector Use node selector Resource Management Configure resources Private Registry Use private registry Automatic Cleanup Configure TTL OpenTelemetry Configure OpenTelemetry Secret Injection Inject secrets Volume Mounting Mount volumes Separate Resource Specs API Reference <p>Browse all guides: How-To Guides</p>","tags":["advanced","configuration","how-to"]},{"location":"api_reference/","title":"API Reference","text":"<p>This document provides a complete reference for the LocustTest Custom Resource Definition (CRD).</p>","tags":["api","reference","crd","specification"]},{"location":"api_reference/#overview","title":"Overview","text":"Property Value Group <code>locust.io</code> Kind <code>LocustTest</code> Versions v2 (recommended), v1 (deprecated) Short Name <code>lotest</code> Scope Namespaced","tags":["api","reference","crd","specification"]},{"location":"api_reference/#locusttest-v2-recommended","title":"LocustTest v2 (Recommended)","text":"<p>The v2 API provides a cleaner, grouped configuration structure with new features.</p>","tags":["api","reference","crd","specification"]},{"location":"api_reference/#spec-fields","title":"Spec Fields","text":"","tags":["api","reference","crd","specification"]},{"location":"api_reference/#root-fields","title":"Root Fields","text":"Field Type Required Default Description <code>image</code> string Yes - Container image for Locust pods (e.g., <code>locustio/locust:2.20.0</code>) <code>imagePullPolicy</code> string No <code>IfNotPresent</code> Image pull policy: <code>Always</code>, <code>IfNotPresent</code>, <code>Never</code> <code>imagePullSecrets</code> []LocalObjectReference No - Secrets for pulling from private registries (specify as <code>- name: secret-name</code>) <code>master</code> MasterSpec Yes - Master pod configuration <code>worker</code> WorkerSpec Yes - Worker pod configuration <code>testFiles</code> TestFilesConfig No - ConfigMap references for test files <code>scheduling</code> SchedulingConfig No - Affinity, tolerations, nodeSelector <code>env</code> EnvConfig No - Environment variable injection <code>volumes</code> []corev1.Volume No - Additional volumes to mount <code>volumeMounts</code> []TargetedVolumeMount No - Volume mounts with target filtering <code>observability</code> ObservabilityConfig No - OpenTelemetry configuration","tags":["api","reference","crd","specification"]},{"location":"api_reference/#masterspec","title":"MasterSpec","text":"Field Type Required Default Description <code>command</code> string Yes - Locust command seed (e.g., <code>--locustfile /lotest/src/test.py --host https://example.com</code>) <code>resources</code> corev1.ResourceRequirements No From operator config CPU/memory requests and limits <code>labels</code> map[string]string No - Additional labels for master pod <code>annotations</code> map[string]string No - Additional annotations for master pod <code>autostart</code> bool No <code>true</code> Start test automatically when workers connect <code>autoquit</code> AutoquitConfig No <code>{enabled: true, timeout: 60}</code> Auto-quit behavior after test completion <code>extraArgs</code> []string No - Additional command-line arguments","tags":["api","reference","crd","specification"]},{"location":"api_reference/#workerspec","title":"WorkerSpec","text":"Field Type Required Default Description <code>command</code> string Yes - Locust command seed (e.g., <code>--locustfile /lotest/src/test.py</code>) <code>replicas</code> int32 Yes - Number of worker replicas (1-500) <code>resources</code> corev1.ResourceRequirements No From operator config CPU/memory requests and limits <code>labels</code> map[string]string No - Additional labels for worker pods <code>annotations</code> map[string]string No - Additional annotations for worker pods <code>extraArgs</code> []string No - Additional command-line arguments","tags":["api","reference","crd","specification"]},{"location":"api_reference/#autoquitconfig","title":"AutoquitConfig","text":"Field Type Required Default Description <code>enabled</code> bool No <code>true</code> Enable auto-quit after test completion <code>timeout</code> int32 No <code>60</code> Seconds to wait before quitting after test ends","tags":["api","reference","crd","specification"]},{"location":"api_reference/#testfilesconfig","title":"TestFilesConfig","text":"Field Type Required Default Description <code>configMapRef</code> string No - ConfigMap containing test files <code>libConfigMapRef</code> string No - ConfigMap containing library files <code>srcMountPath</code> string No <code>/lotest/src</code> Mount path for test files <code>libMountPath</code> string No <code>/opt/locust/lib</code> Mount path for library files","tags":["api","reference","crd","specification"]},{"location":"api_reference/#schedulingconfig","title":"SchedulingConfig","text":"Field Type Required Default Description <code>affinity</code> corev1.Affinity No - Standard Kubernetes affinity rules <code>tolerations</code> []corev1.Toleration No - Standard Kubernetes tolerations <code>nodeSelector</code> map[string]string No - Node selector labels","tags":["api","reference","crd","specification"]},{"location":"api_reference/#envconfig","title":"EnvConfig","text":"Field Type Required Default Description <code>configMapRefs</code> []ConfigMapEnvSource No - ConfigMaps to inject as environment variables <code>secretRefs</code> []SecretEnvSource No - Secrets to inject as environment variables <code>variables</code> []corev1.EnvVar No - Individual environment variables <code>secretMounts</code> []SecretMount No - Secrets to mount as files","tags":["api","reference","crd","specification"]},{"location":"api_reference/#configmapenvsource","title":"ConfigMapEnvSource","text":"Field Type Required Default Description <code>name</code> string Yes - ConfigMap name <code>prefix</code> string No - Prefix to add to all keys (e.g., <code>APP_</code>)","tags":["api","reference","crd","specification"]},{"location":"api_reference/#secretenvsource","title":"SecretEnvSource","text":"Field Type Required Default Description <code>name</code> string Yes - Secret name <code>prefix</code> string No - Prefix to add to all keys","tags":["api","reference","crd","specification"]},{"location":"api_reference/#secretmount","title":"SecretMount","text":"Field Type Required Default Description <code>name</code> string Yes - Secret name <code>mountPath</code> string Yes - Path to mount the secret <code>readOnly</code> bool No <code>true</code> Mount as read-only","tags":["api","reference","crd","specification"]},{"location":"api_reference/#targetedvolumemount","title":"TargetedVolumeMount","text":"Field Type Required Default Description <code>name</code> string Yes - Volume name (must match a volume in <code>volumes</code>) <code>mountPath</code> string Yes - Path to mount the volume <code>subPath</code> string No - Sub-path within the volume <code>readOnly</code> bool No <code>false</code> Mount as read-only <code>target</code> string No <code>both</code> Target pods: <code>master</code>, <code>worker</code>, or <code>both</code>","tags":["api","reference","crd","specification"]},{"location":"api_reference/#observabilityconfig","title":"ObservabilityConfig","text":"Field Type Required Default Description <code>openTelemetry</code> OpenTelemetryConfig No - OpenTelemetry configuration","tags":["api","reference","crd","specification"]},{"location":"api_reference/#opentelemetryconfig","title":"OpenTelemetryConfig","text":"Field Type Required Default Description <code>enabled</code> bool No <code>false</code> Enable OpenTelemetry integration <code>endpoint</code> string Required if enabled - OTel collector endpoint (e.g., <code>otel-collector:4317</code>) <code>protocol</code> string No <code>grpc</code> Protocol: <code>grpc</code> or <code>http/protobuf</code> <code>insecure</code> bool No <code>false</code> Use insecure connection <code>extraEnvVars</code> map[string]string No - Additional OTel environment variables","tags":["api","reference","crd","specification"]},{"location":"api_reference/#status-fields","title":"Status Fields","text":"Field Type Description <code>phase</code> string Current lifecycle phase: <code>Pending</code>, <code>Running</code>, <code>Succeeded</code>, <code>Failed</code> <code>observedGeneration</code> int64 Most recent generation observed by the controller <code>expectedWorkers</code> int32 Number of expected worker replicas (from spec) <code>connectedWorkers</code> int32 Approximate number of connected workers (from Job.Status.Active) <code>startTime</code> metav1.Time When the test transitioned to Running <code>completionTime</code> metav1.Time When the test reached Succeeded or Failed <code>conditions</code> []metav1.Condition Standard Kubernetes conditions (see below) <p>Note</p> <p><code>connectedWorkers</code> is an approximation derived from the worker Job's active pod count. It may briefly lag behind actual Locust worker connections.</p>","tags":["api","reference","crd","specification"]},{"location":"api_reference/#phase-lifecycle","title":"Phase Lifecycle","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; Pending: CR Created\n    Pending --&gt; Running: Master Job active\n    Running --&gt; Succeeded: Master Job completed\n    Running --&gt; Failed: Master Job failed\n    Pending --&gt; Failed: Pod health check failed (after grace period)\n    Running --&gt; Failed: Pod health check failed (after grace period)</code></pre> Phase Meaning What to do <code>Pending</code> Resources are being created (Service, master Job, worker Job). Initial state after CR creation. Also set during recovery after external resource deletion. Wait for resources to be scheduled. Check events if stuck. <code>Running</code> Master Job has at least one active pod. Test execution is in progress. <code>startTime</code> is set on this transition. Monitor worker connections and test progress. <code>Succeeded</code> Master Job completed successfully (exit code 0). <code>completionTime</code> is set. Collect results. CR can be deleted or kept for records. <code>Failed</code> Master Job failed, or pod health checks detected persistent failures after the 2-minute grace period. <code>completionTime</code> is set. Check pod logs and events for failure details. Delete and recreate to retry. <p>The operator waits 2 minutes after pod creation before reporting pod health failures. This prevents false alarms during normal startup activities like image pulling, volume mounting, and scheduling.</p>","tags":["api","reference","crd","specification"]},{"location":"api_reference/#condition-types","title":"Condition Types","text":"<p>Ready</p> Status Reason Meaning <code>True</code> <code>ResourcesCreated</code> All resources (Service, Jobs) created successfully <code>False</code> <code>ResourcesCreating</code> Resources are being created <code>False</code> <code>ResourcesFailed</code> Test failed, resources in error state <p>WorkersConnected</p> Status Reason Meaning <code>True</code> <code>AllWorkersConnected</code> All expected workers have active pods <code>False</code> <code>WaitingForWorkers</code> Initial state, waiting for worker pods <code>False</code> <code>WorkersMissing</code> Some workers not yet active (shows N/M count) <p>TestCompleted</p> Status Reason Meaning <code>True</code> <code>TestSucceeded</code> Test completed successfully <code>True</code> <code>TestFailed</code> Test completed with failure <code>False</code> <code>TestInProgress</code> Test has not finished <p>PodsHealthy</p> Status Reason Meaning <code>True</code> <code>PodsHealthy</code> All pods running normally <code>True</code> <code>PodsStarting</code> Within 2-minute grace period (not yet checking) <code>False</code> <code>ImagePullError</code> One or more pods cannot pull container image <code>False</code> <code>ConfigurationError</code> ConfigMap or Secret not found <code>False</code> <code>SchedulingError</code> Pod cannot be scheduled (node affinity, resources) <code>False</code> <code>CrashLoopBackOff</code> Container repeatedly crashing <code>False</code> <code>InitializationError</code> Init container failed <p>SpecDrifted</p> Status Reason Meaning <code>True</code> <code>SpecChangeIgnored</code> CR spec was modified after creation. Changes are ignored. Delete and recreate to apply. <p>Info</p> <p>The <code>SpecDrifted</code> condition only appears when a user edits the CR spec after initial creation. It serves as a reminder that tests are immutable.</p>","tags":["api","reference","crd","specification"]},{"location":"api_reference/#checking-status","title":"Checking Status","text":"<pre><code># Quick status overview\nkubectl get locusttest my-test\n\n# Detailed status with conditions\nkubectl get locusttest my-test -o jsonpath='{.status}' | jq .\n\n# Watch phase changes in real-time\nkubectl get locusttest my-test -w\n\n# Check specific condition\nkubectl get locusttest my-test -o jsonpath='{.status.conditions[?(@.type==\"Ready\")].status}'\n\n# Check worker connection progress\nkubectl get locusttest my-test -o jsonpath='{.status.connectedWorkers}/{.status.expectedWorkers}'\n</code></pre>","tags":["api","reference","crd","specification"]},{"location":"api_reference/#cicd-integration","title":"CI/CD Integration","text":"<p>Use <code>kubectl wait</code> to integrate LocustTest into CI/CD pipelines. The operator's status conditions follow standard Kubernetes conventions, making them compatible with any tool that supports <code>kubectl wait</code>.</p> <p>GitHub Actions example:</p> <pre><code>name: Load Test\non:\n  workflow_dispatch:\n\njobs:\n  load-test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Apply test\n        run: kubectl apply -f locusttest.yaml\n\n      - name: Wait for test completion\n        run: |\n          kubectl wait locusttest/my-test \\\n            --for=jsonpath='{.status.phase}'=Succeeded \\\n            --timeout=30m\n\n      - name: Check result\n        if: failure()\n        run: |\n          echo \"Test failed or timed out\"\n          kubectl describe locusttest my-test\n          kubectl logs -l performance-test-name=my-test --tail=50\n\n      - name: Cleanup\n        if: always()\n        run: kubectl delete locusttest my-test --ignore-not-found\n</code></pre> <p>Generic shell script example:</p> <pre><code>#!/bin/bash\nset -e\n\n# Apply test\nkubectl apply -f locusttest.yaml\n\n# Wait for completion (either Succeeded or Failed)\necho \"Waiting for test to complete...\"\nwhile true; do\n  PHASE=$(kubectl get locusttest my-test -o jsonpath='{.status.phase}' 2&gt;/dev/null)\n  case \"$PHASE\" in\n    Succeeded)\n      echo \"Test passed!\"\n      exit 0\n      ;;\n    Failed)\n      echo \"Test failed!\"\n      kubectl describe locusttest my-test\n      exit 1\n      ;;\n    *)\n      echo \"Phase: $PHASE - waiting...\"\n      sleep 10\n      ;;\n  esac\ndone\n</code></pre>","tags":["api","reference","crd","specification"]},{"location":"api_reference/#complete-v2-example","title":"Complete v2 Example","text":"<pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: comprehensive-test\nspec:\n  image: locustio/locust:2.20.0\n  imagePullPolicy: IfNotPresent\n\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://api.example.com --users 1000 --spawn-rate 50 --run-time 10m\"\n    resources:\n      requests:\n        memory: \"256Mi\"\n        cpu: \"100m\"\n      limits:\n        memory: \"512Mi\"\n        cpu: \"500m\"\n    labels:\n      role: master\n    autostart: true\n    autoquit:\n      enabled: true\n      timeout: 120\n\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 10\n    resources:\n      requests:\n        memory: \"512Mi\"\n        cpu: \"500m\"\n      limits:\n        memory: \"1Gi\"\n        cpu: \"1000m\"\n    labels:\n      role: worker\n\n  testFiles:\n    configMapRef: my-test-scripts\n    libConfigMapRef: my-lib-files\n\n  scheduling:\n    nodeSelector:\n      node-type: performance\n    tolerations:\n      - key: \"dedicated\"\n        operator: \"Equal\"\n        value: \"performance\"\n        effect: \"NoSchedule\"\n\n  env:\n    secretRefs:\n      - name: api-credentials\n        prefix: \"API_\"\n    configMapRefs:\n      - name: app-config\n    variables:\n      - name: LOG_LEVEL\n        value: \"INFO\"\n\n  volumes:\n    - name: test-data\n      persistentVolumeClaim:\n        claimName: test-data-pvc\n\n  volumeMounts:\n    - name: test-data\n      mountPath: /data\n      target: both\n\n  observability:\n    openTelemetry:\n      enabled: true\n      endpoint: \"otel-collector.monitoring:4317\"\n      protocol: \"grpc\"\n      extraEnvVars:\n        OTEL_SERVICE_NAME: \"load-test\"\n        OTEL_RESOURCE_ATTRIBUTES: \"environment=staging,team=platform\"\n</code></pre>","tags":["api","reference","crd","specification"]},{"location":"api_reference/#locusttest-v1-deprecated","title":"LocustTest v1 (Deprecated)","text":"<p>Deprecated</p> <p>The v1 API is deprecated and will be removed in v3.0. Use v2 for new deployments. See the Migration Guide for upgrade instructions.</p>","tags":["api","reference","crd","specification"]},{"location":"api_reference/#spec-fields-v1","title":"Spec Fields (v1)","text":"Field Type Required Default Description <code>masterCommandSeed</code> string Yes - Command seed for master pod <code>workerCommandSeed</code> string Yes - Command seed for worker pods <code>workerReplicas</code> int32 Yes - Number of worker replicas (1-500) <code>image</code> string Yes - Container image <code>imagePullPolicy</code> string No <code>IfNotPresent</code> Image pull policy <code>imagePullSecrets</code> []string No - Pull secrets <code>configMap</code> string No - ConfigMap for test files <code>libConfigMap</code> string No - ConfigMap for library files <code>labels</code> PodLabels No - Labels with <code>master</code> and <code>worker</code> maps <code>annotations</code> PodAnnotations No - Annotations with <code>master</code> and <code>worker</code> maps <code>affinity</code> LocustTestAffinity No - Custom affinity structure <code>tolerations</code> []LocustTestToleration No - Custom toleration structure","tags":["api","reference","crd","specification"]},{"location":"api_reference/#v1-example","title":"v1 Example","text":"<pre><code>apiVersion: locust.io/v1\nkind: LocustTest\nmetadata:\n  name: basic-test\nspec:\n  image: locustio/locust:2.20.0\n  masterCommandSeed: \"--locustfile /lotest/src/test.py --host https://example.com\"\n  workerCommandSeed: \"--locustfile /lotest/src/test.py\"\n  workerReplicas: 3\n  configMap: test-scripts\n</code></pre>","tags":["api","reference","crd","specification"]},{"location":"api_reference/#kubectl-commands","title":"Kubectl Commands","text":"<pre><code># List all LocustTests\nkubectl get locusttests\nkubectl get lotest  # short name\n\n# Describe a LocustTest\nkubectl describe locusttest &lt;name&gt;\n\n# Watch status changes\nkubectl get locusttest &lt;name&gt; -w\n\n# Delete a LocustTest\nkubectl delete locusttest &lt;name&gt;\n</code></pre>","tags":["api","reference","crd","specification"]},{"location":"api_reference/#printer-columns","title":"Printer Columns","text":"<p>When listing LocustTests, the following columns are displayed:</p> Column Description NAME Resource name PHASE Current phase (Pending/Running/Succeeded/Failed) WORKERS Requested worker count CONNECTED Connected worker count IMAGE Container image (priority column) AGE Time since creation","tags":["api","reference","crd","specification"]},{"location":"comparison/","title":"Comparison: Locust on Kubernetes","text":"<p>When running Locust load tests on Kubernetes, you have four main approaches to choose from:</p> <ol> <li>Locust Kubernetes Operator (this project) - Full lifecycle management via Custom Resource Definition (CRD)</li> <li>Official Locust Operator (locustio/k8s-operator) - Locust team operator</li> <li>k6 Operator (Grafana) - Distributed k6 testing on Kubernetes</li> <li>Manual Deployment - Raw Kubernetes manifests (Deployments, Services, ConfigMaps)</li> </ol> <p>This page helps you evaluate which approach fits your use case, with an objective feature comparison, performance benchmarks, decision guide, and migration paths.</p>"},{"location":"comparison/#feature-comparison","title":"Feature Comparison","text":"Feature This Operator Official Operator k6 Operator Manual Deploy Declarative CRD API \u2713 \u2713 \u2713 \u2717 Automated lifecycle \u2713 \u2713 \u2713 \u2717 Immutable test guarantee \u2713 \u2717 \u2717 \u2717 Validation webhooks \u2713 Not documented \u2713 \u2717 CI/CD integration (autoQuit) \u2713 Not documented \u2713 (cloud) Manual OpenTelemetry native \u2713 \u2717 \u2717 \u2717 Secret injection (envFrom) \u2713 Not documented \u2713 Manual Volume mounting \u2713 \u2713 (ConfigMap) \u2713 \u2713 Horizontal worker scaling \u2713 \u2713 \u2713 \u2713 Resource governance \u2713 (operator + CR) Not documented \u2713 \u2713 Status monitoring (conditions) \u2713 Not documented \u2713 \u2717 Pod health detection \u2713 \u2717 \u2717 \u2717 Leader election (HA) \u2713 Not documented \u2713 N/A Helm chart \u2713 \u2713 \u2713 \u2717 API versions supported v1 + v2 (conversion) Single version Single version N/A Documentation pages 20+ 1 Extensive N/A <p>Note: \"Not documented\" indicates features that may exist but are not described in the official documentation. The Official Locust Operator is maintained by the Locust core team.</p>"},{"location":"comparison/#why-choose-this-operator","title":"Why Choose This Operator","text":"<p>Battle-Tested Reliability</p> <ul> <li>Battle-Tested - Production use on AWS EKS since 2022</li> <li>Comprehensive documentation - Comprehensive coverage of getting started, API reference, architecture, security, FAQ, and migration guides</li> <li>Go-native performance - Sub-second startup time, 75 MB container image, 64 MB memory footprint</li> <li>Feature-rich capabilities - OpenTelemetry integration, validation webhooks, pod health monitoring, immutable test guarantee</li> <li>Active development - Continuous improvement with community feedback and contributions</li> </ul>"},{"location":"comparison/#performance-benchmarks","title":"Performance Benchmarks","text":"<p>All metrics measured from production deployment on AWS EKS. Container images measured via <code>docker images</code>, memory usage via Kubernetes metrics-server, startup time as duration to pod Ready state.</p>"},{"location":"comparison/#container-image-size","title":"Container Image Size","text":"Metric This Operator (Go) Previous Version (Java) Image size 75 MB 325 MB Reduction 77% smaller \u2014 <p>Source: <code>docker images</code> output</p>"},{"location":"comparison/#runtime-performance","title":"Runtime Performance","text":"Metric This Operator (Go) Previous Version (Java) Memory usage (idle) 64 MB 256 MB Startup time &lt; 1 second ~60 seconds <p>Source: Kubernetes metrics-server and pod Ready state timing</p> <p>Methodology: Measurements from production deployment on AWS EKS. Container images measured via <code>docker images</code>, memory via Kubernetes metrics-server, startup time as duration to pod Ready state.</p> <p>Note: Performance data for the Official Locust Operator and k6 Operator is not published by their maintainers.</p>"},{"location":"comparison/#decision-guide","title":"Decision Guide","text":"<p>Choose This Operator when...</p> <ul> <li>Running Locust tests in CI/CD pipelines regularly</li> <li>Need automated test lifecycle management (create, run, cleanup)</li> <li>Want immutability guarantees (no mid-test changes)</li> <li>Require OpenTelemetry observability</li> <li>Multiple teams sharing a cluster need governance and isolation</li> <li>Need pod health monitoring and status conditions</li> <li>Want validation webhooks to catch configuration errors before deployment</li> </ul> <p>Choose k6 Operator when...</p> <ul> <li>Using k6 (not Locust) for load testing</li> <li>Need Grafana Cloud integration for observability</li> <li>Prefer k6 scripting language and ecosystem</li> <li>Part of the Grafana observability stack</li> </ul> <p>Choose Manual Deployment when...</p> <ul> <li>Learning Locust on Kubernetes for the first time</li> <li>Need maximum flexibility and customization</li> <li>Running a one-off test in a development environment</li> <li>Want to understand the underlying Kubernetes primitives</li> <li>Have specific requirements not covered by existing solutions</li> </ul>"},{"location":"comparison/#migration-paths","title":"Migration Paths","text":""},{"location":"comparison/#from-manual-deployment-to-operator","title":"From Manual Deployment to Operator","text":"<p>If you're currently using raw Kubernetes manifests (Deployments, Services, ConfigMaps), migrating to the operator is straightforward:</p> <ol> <li>Keep your existing ConfigMap with test scripts</li> <li>Create a LocustTest CR that references your ConfigMap</li> <li>Deploy the CR - the operator handles the rest</li> </ol> <p>Example:</p> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: my-test\nspec:\n  image: locustio/locust:2.20.0\n  testFiles:\n    configMapRef: my-existing-configmap  # Reference your existing ConfigMap\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://example.com\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 5\n</code></pre> <p> Get started with the operator</p>"},{"location":"comparison/#from-helm-chart-to-operator","title":"From Helm Chart to Operator","text":"<p>If you're using the official Locust Helm chart, you can map your Helm values to LocustTest CR fields:</p> <ul> <li>Helm <code>image</code> \u2192 CR <code>spec.image</code></li> <li>Helm <code>master.args</code> \u2192 CR <code>spec.master.command</code></li> <li>Helm <code>worker.replicas</code> \u2192 CR <code>spec.worker.replicas</code></li> <li>Helm <code>locust_locustfile_configmap</code> \u2192 CR <code>spec.testFiles.configMapRef</code></li> <li>Helm <code>locust_lib_configmap</code> \u2192 CR <code>spec.testFiles.libConfigMapRef</code></li> </ul> <p>The operator provides additional capabilities like automated cleanup, validation webhooks, and OpenTelemetry integration that aren't available in the Helm chart.</p> <p> See detailed field mapping in the migration guide</p>"},{"location":"comparison/#from-v1-operator-to-v2-operator","title":"From v1 Operator to v2 Operator","text":"<p>If you're already using v1 of the Locust Kubernetes Operator, migration to v2 is seamless:</p> <ul> <li>Backward compatibility: v1 CRs continue to work via automatic conversion webhook</li> <li>New features: Access OpenTelemetry, secret injection, volume mounting, separate resource specs</li> <li>Performance: 75% smaller memory footprint, sub-second startup time</li> </ul> <p> Complete v1-to-v2 migration guide</p>"},{"location":"comparison/#ready-to-get-started","title":"Ready to Get Started?","text":"<p>The Locust Kubernetes Operator provides comprehensive lifecycle management for running Locust tests on Kubernetes, with features designed for CI/CD pipelines and production environments.</p> <p> Get started in 5 minutes</p>"},{"location":"contribute/","title":"Contributing &amp; Development","text":"","tags":["contributing","development","community","open source","collaboration"]},{"location":"contribute/#ways-to-contribute","title":"Ways to Contribute","text":"<p>There are several ways you can contribute to the Locust K8s Operator project:</p>","tags":["contributing","development","community","open source","collaboration"]},{"location":"contribute/#for-everyone","title":"For Everyone","text":"<ul> <li>Reporting Issues: Found a bug or have a feature request? Open an issue \ud83d\udc4b</li> <li>Documentation: Help improve the documentation by suggesting clarifications or additions</li> <li>Community Support: Answer questions and help others in the issue tracker</li> </ul>","tags":["contributing","development","community","open source","collaboration"]},{"location":"contribute/#for-developers","title":"For Developers","text":"<p>Note: The following sections are intended for developers who want to contribute code to the project. If you're just using the operator, you can skip these sections.</p> <ul> <li>Code Contributions: Implement new features or fix bugs</li> <li>Testing: Improve test coverage and test in different environments</li> <li>Review: Review pull requests from other contributors</li> </ul>","tags":["contributing","development","community","open source","collaboration"]},{"location":"contribute/#project-status","title":"Project Status","text":"<p>The project is actively maintained and is under continuous development and improvement. If you have any request or want to chat, kindly open a ticket. If you wish to contribute code and/or ideas, please review the development documentation below.</p>","tags":["contributing","development","community","open source","collaboration"]},{"location":"contribute/#technology-stack","title":"Technology Stack","text":"<p>The operator is built with Go using the controller-runtime framework. Key technologies:</p> <ul> <li>Language: Go 1.24+</li> <li>Framework: controller-runtime / Operator SDK</li> <li>Testing: envtest, Ginkgo, Kind</li> <li>Build: Make, Docker</li> <li>Deployment: Helm, Kustomize</li> </ul>","tags":["contributing","development","community","open source","collaboration"]},{"location":"contribute/#development-documentation","title":"Development Documentation","text":"<p>For developers contributing to the Locust K8s Operator project, we provide detailed documentation on various development aspects:</p> <ul> <li>Local Development Guide: Setting up your development environment</li> <li>Testing Guide: Running unit, integration, and E2E tests</li> <li>Pull Request Process: Guidelines for submitting code changes</li> <li>How It Works: Architecture overview</li> </ul> <p>You can also refer to the comprehensive CONTRIBUTING.MD file in the GitHub repository for more information.</p>","tags":["contributing","development","community","open source","collaboration"]},{"location":"faq/","title":"Frequently Asked Questions","text":"<p>This page answers the most common questions about operating the Locust Kubernetes Operator in production. For step-by-step tutorials, see Getting Started. For advanced configuration, see How-To Guides.</p>","tags":["faq","troubleshooting","guide"]},{"location":"faq/#test-lifecycle","title":"Test Lifecycle","text":"","tags":["faq","troubleshooting","guide"]},{"location":"faq/#why-cant-i-update-a-running-test","title":"Why can't I update a running test?","text":"<p>Tests are immutable by design. Once a LocustTest CR is created, the operator ignores all changes to the <code>spec</code> field and sets a <code>SpecDrifted</code> condition to indicate drift was detected.</p> <p>This ensures predictable behavior \u2014 each test run uses exactly the configuration it was created with, with no mid-flight configuration changes. See How Does It Work - Immutable Tests for the design rationale.</p> <p>To change test parameters, use the delete-and-recreate pattern:</p> <pre><code>kubectl delete locusttest my-test\n# Edit your YAML with desired changes\nkubectl apply -f locusttest.yaml\n</code></pre>","tags":["faq","troubleshooting","guide"]},{"location":"faq/#how-do-i-change-test-parameters","title":"How do I change test parameters?","text":"<p>Delete the LocustTest CR, edit your YAML file with the desired changes, and recreate it:</p> <pre><code>kubectl delete locusttest my-test\n# Edit locusttest.yaml (change image, replicas, commands, etc.)\nkubectl apply -f locusttest.yaml\n</code></pre> <p>The operator will create new Jobs with the updated configuration. Previous test results remain in your monitoring system (if using OpenTelemetry or metrics export).</p>","tags":["faq","troubleshooting","guide"]},{"location":"faq/#what-happens-if-i-edit-a-locusttest-cr-after-creation","title":"What happens if I edit a LocustTest CR after creation?","text":"<p>The operator detects spec changes but ignores them. It sets a <code>SpecDrifted</code> condition on the CR to indicate the spec has been modified:</p> <pre><code>kubectl get locusttest my-test -o jsonpath='{.status.conditions[?(@.type==\"SpecDrifted\")]}'\n</code></pre> <p>The test continues running with its original configuration. To apply changes, delete and recreate the CR.</p>","tags":["faq","troubleshooting","guide"]},{"location":"faq/#how-do-i-run-the-same-test-multiple-times","title":"How do I run the same test multiple times?","text":"<p>Delete and recreate the CR with the same YAML:</p> <pre><code>kubectl delete locusttest my-test\nkubectl apply -f locusttest.yaml  # Same file\n</code></pre> <p>Or use unique names with a suffix to keep test history:</p> <pre><code>kubectl apply -f locusttest-run-01.yaml\n# Later...\nkubectl apply -f locusttest-run-02.yaml\n</code></pre>","tags":["faq","troubleshooting","guide"]},{"location":"faq/#scaling","title":"Scaling","text":"","tags":["faq","troubleshooting","guide"]},{"location":"faq/#can-i-scale-workers-during-a-running-test","title":"Can I scale workers during a running test?","text":"<p>No, due to immutability. The worker replica count (<code>worker.replicas</code>) is set at test creation time and cannot be changed while the test runs.</p> <p>To run with different worker counts:</p> <pre><code>kubectl delete locusttest my-test\n# Edit YAML to update worker.replicas\nkubectl apply -f locusttest.yaml\n</code></pre> <p>Note: Locust's web UI shows real-time user distribution across connected workers regardless of the replica count.</p>","tags":["faq","troubleshooting","guide"]},{"location":"faq/#whats-the-maximum-number-of-workers","title":"What's the maximum number of workers?","text":"<p>The CRD enforces a maximum of 500 workers per LocustTest. This limit prevents accidental resource exhaustion.</p> <p>For larger scales:</p> <ul> <li>Run multiple LocustTest CRs against the same target (each test independently generates load)</li> <li>Use fewer workers with more users per worker (adjust <code>--users</code> and <code>--spawn-rate</code> in <code>master.command</code>)</li> </ul>","tags":["faq","troubleshooting","guide"]},{"location":"faq/#how-do-i-size-worker-resources","title":"How do I size worker resources?","text":"<p>Resource requirements depend on test complexity:</p> Test Type CPU per Worker Memory per Worker Notes Light HTTP tests 250m 128Mi Simple GET/POST requests Medium complexity 500m 256Mi JSON parsing, simple logic Heavy tests 1000m 512Mi-1Gi Complex business logic, large payloads <p>Start conservative and observe resource usage via <code>kubectl top pods</code>. See Advanced Topics - Resource Management for detailed sizing guidance.</p> <p>Resource Precedence</p> <p>The operator applies resources in order of specificity: (1) CR spec resources (highest), (2) Helm role-specific resources (<code>masterResources</code>/<code>workerResources</code>), (3) Helm unified resources (<code>locustPods.resources</code>).</p>","tags":["faq","troubleshooting","guide"]},{"location":"faq/#debugging","title":"Debugging","text":"","tags":["faq","troubleshooting","guide"]},{"location":"faq/#my-test-is-stuck-in-pending-phase","title":"My test is stuck in Pending phase","text":"<p>Check in this order:</p> <ol> <li>Check operator events: <code>kubectl describe locusttest &lt;test-name&gt;</code> \u2014 look for errors in the Events section</li> <li>Check pod status: <code>kubectl get pods -l performance-test-name=&lt;test-name&gt;</code> \u2014 look for scheduling errors or image pull failures</li> <li>Check PodsHealthy condition: <code>kubectl get locusttest &lt;test-name&gt; -o jsonpath='{.status.conditions[?(@.type==\"PodsHealthy\")]}'</code> \u2014 the operator reports pod issues here</li> <li>Check ConfigMap exists: If using <code>testFiles.configMapRef</code>, ensure the ConfigMap exists: <code>kubectl get configmap &lt;name&gt;</code></li> </ol> <p>The operator has a 2-minute grace period before reporting pod failures, allowing time for image pulls and startup.</p>","tags":["faq","troubleshooting","guide"]},{"location":"faq/#my-test-shows-failed-phase","title":"My test shows Failed phase","text":"<p>Check the failure reason:</p> <ol> <li>Check conditions: <code>kubectl describe locusttest &lt;test-name&gt;</code> \u2014 the Status section shows why it failed</li> <li>Check master logs: <code>kubectl logs &lt;test-name&gt;-master-&lt;hash&gt;</code> \u2014 Locust errors appear here</li> <li>Common causes:<ul> <li>Locustfile syntax error: Python errors in your test script</li> <li>Target host unreachable: Network connectivity issues</li> <li>ConfigMap not found: Missing test files</li> <li>Image pull failure: Invalid image name or missing pull secrets</li> </ul> </li> </ol>","tags":["faq","troubleshooting","guide"]},{"location":"faq/#workers-show-0n-connected","title":"Workers show 0/N connected","text":"<p>The <code>connectedWorkers</code> field is an approximation from <code>Job.Status.Active</code>. Workers need time to start, pull images, and connect to the master.</p> <p>Check worker connectivity:</p> <ol> <li>Verify worker pods are running: <code>kubectl get pods -l performance-test-pod-name=&lt;test-name&gt;-worker</code></li> <li>Verify master service exists: <code>kubectl get svc &lt;test-name&gt;-master</code></li> <li>Check worker logs: <code>kubectl logs &lt;test-name&gt;-worker-&lt;hash&gt;</code> \u2014 workers should show \"Connected to master\"</li> <li>Verify network connectivity: Workers connect to the master on port 5557</li> </ol> <p>Workers typically connect within 30-60 seconds after pod startup.</p>","tags":["faq","troubleshooting","guide"]},{"location":"faq/#how-do-i-access-the-locust-web-ui","title":"How do I access the Locust web UI?","text":"<p>Port-forward to the master service:</p> <pre><code>kubectl port-forward svc/&lt;test-name&gt;-master 8089:8089\n</code></pre> <p>Then visit http://localhost:8089 in your browser.</p> <p>Autostart Behavior</p> <p>If <code>autostart: true</code> (default), the test starts automatically and the web UI shows the running test. Set <code>autostart: false</code> to control test start from the web UI.</p>","tags":["faq","troubleshooting","guide"]},{"location":"faq/#configmap-not-found-error","title":"ConfigMap not found error","text":"<p>The operator detects missing ConfigMaps via pod health monitoring and reports the issue in the <code>PodsHealthy</code> condition.</p> <p>You can create the ConfigMap before or after the LocustTest CR:</p> <pre><code># Create ConfigMap from local files\nkubectl create configmap my-test-scripts --from-file=test.py=./test.py\n\n# If LocustTest already exists, the operator detects recovery automatically\n</code></pre> <p>The operator monitors pod status every 30 seconds and updates conditions when ConfigMaps become available.</p>","tags":["faq","troubleshooting","guide"]},{"location":"faq/#migration","title":"Migration","text":"","tags":["faq","troubleshooting","guide"]},{"location":"faq/#can-i-use-v1-and-v2-crs-at-the-same-time","title":"Can I use v1 and v2 CRs at the same time?","text":"<p>Yes, with the conversion webhook enabled. The operator automatically converts v1 CRs to v2 internally, allowing both versions to coexist.</p> <p>v1 CRs continue to work with their existing configuration. See Migration Guide for conversion details.</p>","tags":["faq","troubleshooting","guide"]},{"location":"faq/#do-i-need-to-recreate-existing-v1-tests","title":"Do I need to recreate existing v1 tests?","text":"<p>No, existing v1 tests continue to work. However, new features (OpenTelemetry integration, environment variable injection, volume mounts) require the v2 API.</p> <p>Migrate when you need v2-only features or when convenient. See Migration Guide for the conversion process.</p>","tags":["faq","troubleshooting","guide"]},{"location":"faq/#configuration","title":"Configuration","text":"","tags":["faq","troubleshooting","guide"]},{"location":"faq/#how-do-i-pass-extra-cli-arguments-to-locust","title":"How do I pass extra CLI arguments to Locust?","text":"<p>Use <code>master.extraArgs</code> and <code>worker.extraArgs</code> in the v2 API. These are appended after the command seed and operator-managed flags:</p> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nspec:\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://api.example.com\"\n    extraArgs:\n      - \"--loglevel\"\n      - \"DEBUG\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    extraArgs:\n      - \"--loglevel\"\n      - \"DEBUG\"\n    replicas: 5\n</code></pre> <p>Reserved Flags</p> <p>The operator manages these flags automatically: <code>--master</code>, <code>--worker</code>, <code>--master-host</code>, <code>--master-port</code>, <code>--expect-workers</code>, <code>--autostart</code>, <code>--autoquit</code>. Do not set them manually.</p>","tags":["faq","troubleshooting","guide"]},{"location":"faq/#what-resource-precedence-applies","title":"What resource precedence applies?","text":"<p>The operator applies resources in this order (first non-empty value wins):</p> <ol> <li>CR spec resources (highest precedence): Set in <code>LocustTest.spec.master.resources</code> or <code>LocustTest.spec.worker.resources</code></li> <li>Helm role-specific resources: Set in <code>values.yaml</code> as <code>locustPods.masterResources</code> or <code>locustPods.workerResources</code></li> <li>Helm unified resources: Set in <code>values.yaml</code> as <code>locustPods.resources</code></li> </ol> <p>This allows global defaults with role-specific overrides and per-test customization.</p>","tags":["faq","troubleshooting","guide"]},{"location":"faq/#observability","title":"Observability","text":"","tags":["faq","troubleshooting","guide"]},{"location":"faq/#should-i-use-opentelemetry-or-the-metrics-sidecar","title":"Should I use OpenTelemetry or the metrics sidecar?","text":"<p>Use OpenTelemetry for new deployments. It provides traces and metrics without requiring a sidecar container, reducing resource overhead.</p> <p>The metrics sidecar is maintained for legacy compatibility. Use it only if:</p> <ul> <li>Your monitoring stack doesn't support OTLP</li> <li>You have existing dashboards built on the CSV metrics format</li> </ul> <p>See Advanced Topics - OpenTelemetry Integration for configuration details.</p>","tags":["faq","troubleshooting","guide"]},{"location":"faq/#how-do-i-monitor-test-progress-programmatically","title":"How do I monitor test progress programmatically?","text":"<p>Use the LocustTest status conditions for automation:</p> <pre><code># Check if test is ready\nkubectl get locusttest my-test -o jsonpath='{.status.conditions[?(@.type==\"Ready\")].status}'\n\n# Get current phase\nkubectl get locusttest my-test -o jsonpath='{.status.phase}'\n\n# Get worker count\nkubectl get locusttest my-test -o jsonpath='{.status.connectedWorkers}/{.status.expectedWorkers}'\n</code></pre> <p>See API Reference - Status Fields for all available status information.</p>","tags":["faq","troubleshooting","guide"]},{"location":"features/","title":"Features","text":"<p>Everything the Locust Kubernetes Operator can do. Click any feature to learn how.</p> <ul> <li> <p> Cloud Native &amp; Kubernetes Integration</p> <p>Leverage the full power of Kubernetes for distributed performance testing. The operator is designed to be cloud-native, enabling seamless deployment and scaling on any Kubernetes cluster.</p> <p> How it works</p> </li> <li> <p> Automation &amp; CI/CD</p> <p>Integrate performance testing directly into your CI/CD pipelines. Automate the deployment, execution, and teardown of your Locust tests for continuous performance validation.</p> <p> CI/CD integration tutorial</p> </li> <li> <p> Governance &amp; Resource Management</p> <p>Maintain control over how resources are deployed and used. Configure resource requests and limits for Locust master and worker pods, and even disable CPU limits for performance-sensitive tests.</p> <p> Configure resources</p> </li> <li> <p> Observability &amp; Monitoring</p> <p>Gain insights into test results and infrastructure usage. The operator supports Prometheus metrics out-of-the-box and native OpenTelemetry integration.</p> <p> Configure OpenTelemetry \u00b7  Metrics reference</p> </li> <li> <p> Cost Optimization</p> <p>Optimize cloud costs by deploying resources only when needed and for as long as needed. The operator's automatic cleanup feature ensures that resources are terminated after a test run.</p> <p> Configure TTL</p> </li> <li> <p> Test Isolation &amp; Parallelism</p> <p>Run multiple tests in parallel with guaranteed isolation. Each test runs in its own set of resources, preventing any cross-test interference.</p> <p> How it works</p> </li> <li> <p> Private Image Registry Support</p> <p>Use images from private registries for your Locust tests. The operator supports <code>imagePullSecrets</code> and configurable <code>imagePullPolicy</code>.</p> <p> Use private registry</p> </li> <li> <p> Lib ConfigMap Support</p> <p>Mount lib directories via ConfigMap for your Locust tests. This feature allows you to include shared libraries and modules without modifying test files or patching images, similar to the helm chart's <code>locust_lib_configmap</code> functionality.</p> </li> <li> <p> Advanced Scheduling</p> <p>Control where your Locust pods are scheduled using Kubernetes affinity and taint tolerations. This allows you to run tests on dedicated nodes or in specific availability zones.</p> <p> Use node affinity \u00b7  Configure tolerations \u00b7  Use node selector</p> </li> <li> <p> Kafka &amp; AWS MSK Integration</p> <p>Seamlessly integrate with Kafka and AWS MSK for performance testing of event-driven architectures. The operator provides out-of-the-box support for authenticated Kafka.</p> <p> Configure Kafka</p> </li> <li> <p> Native OpenTelemetry Support</p> <p>Export traces and metrics directly from Locust using native OpenTelemetry integration. No sidecar required\u2014configure endpoints, protocols, and custom attributes directly in your CR.</p> <p> Configure OpenTelemetry</p> </li> <li> <p> Secret &amp; ConfigMap Injection</p> <p>Securely inject credentials, API keys, and configuration from Kubernetes Secrets and ConfigMaps. Supports environment variables and file mounts with automatic prefix handling.</p> <p> Inject secrets</p> </li> <li> <p> Flexible Volume Mounting</p> <p>Mount test data, certificates, and configuration files from PersistentVolumes, ConfigMaps, or Secrets. Target specific components (master, worker, or both) with fine-grained control.</p> <p> Mount volumes</p> </li> <li> <p> Separate Resource Specs</p> <p>Configure resources, labels, and annotations independently for master and worker pods. Optimize each component based on its specific needs.</p> <p> API Reference</p> </li> <li> <p> Enhanced Status Tracking</p> <p>Monitor test progress with rich status information including phase (Pending, Running, Succeeded, Failed), Kubernetes conditions, and worker connection status.</p> <p> Monitor test status \u00b7  API Reference</p> </li> </ul>","tags":["features","capabilities","cloud native","kubernetes","automation"]},{"location":"helm_deploy/","title":"HELM Deployment Guide","text":"<p>This guide provides comprehensive instructions for deploying the Locust Kubernetes Operator using its official Helm chart.</p>","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"helm_deploy/#quick-start","title":"Quick Start","text":"<p>For experienced users, here are the essential commands to get the operator running:</p> <pre><code>helm repo add locust-k8s-operator https://abdelrhmanhamouda.github.io/locust-k8s-operator/\nhelm repo update\nhelm install locust-operator locust-k8s-operator/locust-k8s-operator \\\n  --namespace locust-system --create-namespace\n</code></pre>","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"helm_deploy/#installation","title":"Installation","text":"","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"helm_deploy/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster (e.g., Minikube, GKE, EKS, AKS).</li> <li>Helm 3 installed on your local machine.</li> </ul>","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"helm_deploy/#step-1-add-the-helm-repository","title":"Step 1: Add the Helm Repository","text":"<p>First, add the Locust Kubernetes Operator Helm repository to your local Helm client:</p> <pre><code>helm repo add locust-k8s-operator https://abdelrhmanhamouda.github.io/locust-k8s-operator/\n</code></pre> <p>Next, update your local chart repository cache to ensure you have the latest version:</p> <pre><code>helm repo update\n</code></pre>","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"helm_deploy/#step-2-install-the-chart","title":"Step 2: Install the Chart","text":"<p>You can install the chart with a release name of your choice (e.g., <code>locust-operator</code>).</p> <p>Default Installation:</p> <p>To install the chart with the default configuration, run:</p> <pre><code>helm install locust-operator locust-k8s-operator/locust-k8s-operator \\\n  --namespace locust-system --create-namespace\n</code></pre> <p>Installation with a Custom Values File:</p> <p>For more advanced configurations, it's best to use a custom <code>values.yaml</code> file. Create a file named <code>my-values.yaml</code> and add your overrides:</p> v2 Helm Values (Recommended)v1 Helm Values (Deprecated) <pre><code># my-values.yaml\nreplicaCount: 2\n\nlocustPods:\n  resources:\n    limits:\n      cpu: \"2000m\"\n      memory: \"2048Mi\"\n    requests:\n      cpu: \"500m\"\n      memory: \"512Mi\"\n</code></pre> <pre><code># my-values.yaml (old format - still works via compatibility shims)\nreplicaCount: 2\n\nconfig:\n  loadGenerationPods:\n    resource:\n      cpuLimit: \"2000m\"\n      memLimit: \"2048Mi\"\n</code></pre> <p>Then, install the chart, specifying your custom values file and a target namespace:</p> <pre><code>helm install locust-operator locust-k8s-operator/locust-k8s-operator \\\n  --namespace locust-system \\\n  --create-namespace \\\n  -f my-values.yaml\n</code></pre>","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"helm_deploy/#verifying-the-installation","title":"Verifying the Installation","text":"<p>After installation, you can verify that the operator is running correctly by checking the pods in the target namespace:</p> <pre><code>kubectl get pods -n locust-system\n</code></pre> <p>You should see a pod with a name similar to <code>locust-operator-b5c9f4f7-xxxxx</code> in the <code>Running</code> state.</p> <p>To view the operator's logs, run:</p> <pre><code>kubectl logs -f -n locust-system -l app.kubernetes.io/name=locust-k8s-operator\n</code></pre>","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"helm_deploy/#configuration","title":"Configuration","text":"<p>The following tables list the configurable parameters of the Locust Operator Helm chart and their default values.</p> <p>v2.0 Changes</p> <p>The v2 Helm chart has been updated for the Go operator. Java-specific settings (Micronaut, JVM) have been removed. Backward compatibility shims are provided for common settings.</p>","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"helm_deploy/#deployment-settings","title":"Deployment Settings","text":"Parameter Description Default <code>replicaCount</code> Number of replicas for the operator deployment. <code>2</code> <code>image.repository</code> The repository of the Docker image. <code>lotest/locust-k8s-operator</code> <code>image.pullPolicy</code> The image pull policy. <code>IfNotPresent</code> <code>image.tag</code> Overrides the default image tag (defaults to the chart's <code>appVersion</code>). <code>\"\"</code> <code>image.pullSecrets</code> List of image pull secrets. <code>[]</code>","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"helm_deploy/#kubernetes-resources","title":"Kubernetes Resources","text":"Parameter Description Default <code>k8s.clusterRole.enabled</code> Deploy with a cluster-wide role (<code>true</code>) or a namespaced role (<code>false</code>). <code>true</code> <code>serviceAccount.create</code> Specifies whether a service account should be created. <code>true</code> <code>serviceAccount.name</code> The name of the service account to use. If empty and <code>serviceAccount.create</code> is <code>true</code>, a name is generated using the release name. If <code>serviceAccount.create</code> is <code>false</code>, defaults to <code>default</code>. <code>\"\"</code> <code>serviceAccount.annotations</code> Annotations to add to the service account. <code>{}</code>","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"helm_deploy/#operator-resources","title":"Operator Resources","text":"<p>The Go operator requires significantly fewer resources than the Java version:</p> Parameter Description Default <code>resources.limits.memory</code> Operator memory limit. <code>256Mi</code> <code>resources.limits.cpu</code> Operator CPU limit. <code>500m</code> <code>resources.requests.memory</code> Operator memory request. <code>64Mi</code> <code>resources.requests.cpu</code> Operator CPU request. <code>10m</code>","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"helm_deploy/#feature-toggles","title":"Feature Toggles","text":"Parameter Description Default <code>leaderElection.enabled</code> Enable leader election for HA deployments. <code>true</code> <code>metrics.enabled</code> Enable Prometheus metrics endpoint. <code>false</code> <code>metrics.port</code> Metrics server port. <code>8080</code> <code>metrics.secure</code> Use HTTPS for metrics endpoint. <code>false</code> <code>webhook.enabled</code> Enable conversion webhook (requires cert-manager). <code>false</code>","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"helm_deploy/#webhook-configuration-optional","title":"Webhook Configuration (optional)","text":"<p>The conversion webhook is meant for cases where both the old and new CRDs are present in the cluster.  Required when <code>webhook.enabled: true</code>:</p> Parameter Description Default <code>webhook.port</code> Webhook server port. <code>9443</code> <code>webhook.certManager.enabled</code> Use cert-manager for TLS certificate management. <code>true</code> <p>Note</p> <p>The conversion webhook requires cert-manager to be installed in your cluster for automatic TLS certificate management.</p>","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"helm_deploy/#locust-pod-configuration","title":"Locust Pod Configuration","text":"Parameter Description Default <code>locustPods.resources.requests.cpu</code> CPU request for Locust pods. <code>250m</code> <code>locustPods.resources.requests.memory</code> Memory request for Locust pods. <code>128Mi</code> <code>locustPods.resources.requests.ephemeralStorage</code> Ephemeral storage request for Locust pods. <code>30M</code> <code>locustPods.resources.limits.cpu</code> CPU limit for Locust pods. Set to <code>\"\"</code> to unbind. <code>1000m</code> <code>locustPods.resources.limits.memory</code> Memory limit for Locust pods. Set to <code>\"\"</code> to unbind. <code>1024Mi</code> <code>locustPods.resources.limits.ephemeralStorage</code> Ephemeral storage limit for Locust pods. <code>50M</code> <code>locustPods.affinityInjection</code> Enable affinity injection from CRs. <code>true</code> <code>locustPods.tolerationsInjection</code> Enable tolerations injection from CRs. <code>true</code>","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"helm_deploy/#metrics-exporter","title":"Metrics Exporter","text":"Parameter Description Default <code>locustPods.metricsExporter.image</code> Metrics Exporter Docker image. <code>containersol/locust_exporter:v0.5.0</code> <code>locustPods.metricsExporter.port</code> Metrics Exporter port. <code>9646</code> <code>locustPods.metricsExporter.pullPolicy</code> Image pull policy for the metrics exporter. <code>IfNotPresent</code> <code>locustPods.metricsExporter.resources.requests.cpu</code> CPU request for metrics exporter. <code>100m</code> <code>locustPods.metricsExporter.resources.requests.memory</code> Memory request for metrics exporter. <code>64Mi</code> <code>locustPods.metricsExporter.resources.requests.ephemeralStorage</code> Ephemeral storage request for metrics exporter. <code>30M</code> <code>locustPods.metricsExporter.resources.limits.cpu</code> CPU limit for metrics exporter. <code>250m</code> <code>locustPods.metricsExporter.resources.limits.memory</code> Memory limit for metrics exporter. <code>128Mi</code> <code>locustPods.metricsExporter.resources.limits.ephemeralStorage</code> Ephemeral storage limit for metrics exporter. <code>50M</code> <p>Tip</p> <p>When using OpenTelemetry (<code>spec.observability.openTelemetry.enabled: true</code>), the metrics exporter sidecar is not deployed.</p>","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"helm_deploy/#job-configuration","title":"Job Configuration","text":"Parameter Description Default <code>locustPods.ttlSecondsAfterFinished</code> TTL for finished jobs. Set to <code>\"\"</code> to disable. <code>\"\"</code>","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"helm_deploy/#kafka-configuration","title":"Kafka Configuration","text":"Parameter Description Default <code>kafka.enabled</code> Enable Kafka configuration injection. <code>false</code> <code>kafka.bootstrapServers</code> Kafka bootstrap servers. <code>localhost:9092</code> <code>kafka.security.enabled</code> Enable Kafka security. <code>false</code> <code>kafka.security.protocol</code> Security protocol (<code>SASL_SSL</code>, <code>SASL_PLAINTEXT</code>, etc.). <code>SASL_PLAINTEXT</code> <code>kafka.security.saslMechanism</code> SASL mechanism. <code>SCRAM-SHA-512</code> <code>kafka.security.jaasConfig</code> JAAS configuration string. <code>\"\"</code> <code>kafka.credentials.secretName</code> Name of secret containing Kafka credentials. <code>\"\"</code> <code>kafka.credentials.usernameKey</code> Key in secret for username. <code>username</code> <code>kafka.credentials.passwordKey</code> Key in secret for password. <code>password</code>","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"helm_deploy/#opentelemetry-collector-optional","title":"OpenTelemetry Collector (Optional)","text":"<p>Deploy an OTel Collector alongside the operator:</p> Parameter Description Default <code>otelCollector.enabled</code> Deploy OTel Collector. <code>false</code> <code>otelCollector.image</code> Collector image. <code>otel/opentelemetry-collector-contrib:0.145.0</code> <code>otelCollector.replicas</code> Number of collector replicas. <code>1</code> <code>otelCollector.resources.requests.cpu</code> CPU request for collector. <code>50m</code> <code>otelCollector.resources.requests.memory</code> Memory request for collector. <code>64Mi</code> <code>otelCollector.resources.limits.cpu</code> CPU limit for collector. <code>200m</code> <code>otelCollector.resources.limits.memory</code> Memory limit for collector. <code>256Mi</code> <code>otelCollector.config</code> OTel Collector configuration (YAML string). See values.yaml","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"helm_deploy/#pod-scheduling","title":"Pod Scheduling","text":"Parameter Description Default <code>nodeSelector</code> Node selector for scheduling the operator pod. <code>{}</code> <code>tolerations</code> Tolerations for scheduling the operator pod. <code>[]</code> <code>affinity</code> Affinity rules for scheduling the operator pod. <code>{}</code> <code>podAnnotations</code> Annotations to add to the operator pod. <code>{}</code>","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"helm_deploy/#backward-compatibility","title":"Backward Compatibility","text":"<p>The following v1 paths are still supported via helper functions:</p> Old Path (v1) New Path (v2) <code>config.loadGenerationPods.resource.cpuRequest</code> <code>locustPods.resources.requests.cpu</code> <code>config.loadGenerationPods.resource.memLimit</code> <code>locustPods.resources.limits.memory</code> <code>config.loadGenerationPods.affinity.enableCrInjection</code> <code>locustPods.affinityInjection</code> <code>config.loadGenerationPods.kafka.*</code> <code>kafka.*</code> <p>Removed Settings</p> <p>The following Java-specific settings have been removed and have no effect in v2:</p> <ul> <li><code>appPort</code> - Fixed at 8081</li> <li><code>micronaut.*</code> - No Micronaut in Go operator</li> <li><code>livenessProbe.*</code> / <code>readinessProbe.*</code> - Fixed probes on <code>/healthz</code> and <code>/readyz</code></li> </ul>","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"helm_deploy/#upgrading-the-chart","title":"Upgrading the Chart","text":"<p>To upgrade an existing release to a new version, use the <code>helm upgrade</code> command:</p> <pre><code>helm upgrade locust-operator locust-k8s-operator/locust-k8s-operator -f my-values.yaml\n</code></pre>","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"helm_deploy/#uninstalling-the-chart","title":"Uninstalling the Chart","text":"<p>To uninstall and delete the <code>locust-operator</code> deployment, run:</p> <pre><code>helm uninstall locust-operator\n</code></pre> <p>This command will remove all the Kubernetes components associated with the chart and delete the release.</p>","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"helm_deploy/#next-steps","title":"Next Steps","text":"<p>Once the operator is installed, you're ready to start running performance tests! Head over to the Getting Started guide to learn how to deploy your first <code>LocustTest</code>.</p>","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"how_does_it_work/","title":"How Does It Work","text":"<p>To run a performance test, basic configuration is provided through a simple and intuitive Kubernetes custom resource. Once deployed, the Operator does all the heavy work of creating and scheduling the resources while making sure that all created load generation pods can effectively communicate with each other.</p>"},{"location":"how_does_it_work/#demo","title":"Demo","text":"<p>Since a \"Picture Is Worth a Thousand Words\", here is a gif! </p>"},{"location":"how_does_it_work/#steps-performed-in-demo","title":"Steps performed in demo","text":"<ul> <li> Test ConfigMap created in cluster</li> <li> LocustTest CR deployed into the cluster</li> <li> The Operator creating, configuring and scheduling test resources on CR creation event</li> <li> The Operator cleaning up test resources after test CR has been removed</li> </ul>"},{"location":"how_does_it_work/#architecture-overview","title":"Architecture Overview","text":"<p>The Locust K8s Operator is built using Go with the controller-runtime framework, following the standard Kubernetes operator pattern.</p> <p>When you create a LocustTest CR, the controller picks it up and orchestrates the creation of all necessary resources. Here's how the pieces fit together:</p> <pre><code>graph TB\n    User[User creates LocustTest CR] --&gt;|applies| CR[LocustTest Custom Resource]\n    CR --&gt;|watches| Controller[Operator Controller]\n    Controller --&gt;|creates &amp; owns| Service[Master Service]\n    Controller --&gt;|creates &amp; owns| MasterJob[Master Job]\n    Controller --&gt;|creates &amp; owns| WorkerJob[Worker Job]\n    MasterJob --&gt;|creates| MasterPod[Master Pod]\n    WorkerJob --&gt;|creates| WorkerPods[Worker Pods 1..N]\n    Controller --&gt;|watches| Jobs[Job Status Changes]\n    Controller --&gt;|watches| Pods[Pod Events]\n    Controller --&gt;|updates| Status[Status Subresource]\n    Status --&gt;|reflects| Phase[Phase: Pending \u2192 Running \u2192 Succeeded/Failed]\n    Status --&gt;|tracks| Conditions[Conditions: Ready, PodsHealthy, etc.]</code></pre> <p>The controller maintains full ownership of created resources through owner references, ensuring automatic cleanup when the LocustTest CR is deleted.</p>"},{"location":"how_does_it_work/#reconciliation-loop","title":"Reconciliation Loop","text":"<p>The operator follows an event-driven reconciliation pattern. Reconciliation is triggered by:</p> <ul> <li>LocustTest CR events: create, update, or delete operations</li> <li>Owned Job status changes: when the master or worker Job completes or fails</li> <li>Pod state changes: when pods enter CrashLoopBackOff, fail to schedule, or encounter errors</li> </ul> <p>The controller implements a phase-based state machine to track test lifecycle:</p> <pre><code>stateDiagram-v2\n    [*] --&gt; Pending: CR created\n    Pending --&gt; Running: Resources created successfully\n    Running --&gt; Succeeded: Master Job completes (exit 0)\n    Running --&gt; Failed: Master Job fails OR pods unhealthy\n    Pending --&gt; Failed: Resource creation error\n    Failed --&gt; Pending: External deletion triggers recovery\n    Succeeded --&gt; [*]\n    Failed --&gt; [*]\n\n    note right of Pending\n        Creates master Service\n        Creates master Job\n        Creates worker Job\n    end note\n\n    note right of Running\n        Monitors Job completion\n        Tracks pod health\n        Updates worker counts\n    end note</code></pre>"},{"location":"how_does_it_work/#what-happens-in-each-phase","title":"What Happens in Each Phase","text":"<p>Pending \u2014 The controller creates three core resources:</p> <ul> <li>A master Service (headless, for worker-to-master communication)</li> <li>A master Job (single pod running Locust master)</li> <li>A worker Job (N pods running Locust workers)</li> </ul> <p>All resources have owner references pointing to the LocustTest CR. Once creation succeeds, the phase transitions to Running.</p> <p>Running \u2014 The controller monitors:</p> <ul> <li>Job completion status (success or failure)</li> <li>Pod health across all master and worker pods</li> <li>Worker connection counts (approximate, from Job status)</li> </ul> <p>Succeeded/Failed \u2014 Terminal states. The test has completed or encountered unrecoverable errors. Resources remain until CR deletion.</p>"},{"location":"how_does_it_work/#status-updates-are-conflict-safe","title":"Status Updates are Conflict-Safe","text":"<p>The controller uses a retry-on-conflict pattern for all status updates. If two reconcile loops try to update status simultaneously (e.g., from a Job event and a Pod event), the controller automatically retries with the latest resource version. This prevents status overwrites and ensures eventual consistency.</p>"},{"location":"how_does_it_work/#self-healing-behavior","title":"Self-Healing Behavior","text":"<p>If external tools delete the Service or Jobs while the test is Running, the controller detects the missing resources and transitions back to Pending. On the next reconcile, it recreates everything from scratch. This self-healing ensures tests can recover from accidental <code>kubectl delete</code> operations.</p>"},{"location":"how_does_it_work/#validation-webhooks","title":"Validation Webhooks","text":"<p>Before a LocustTest CR reaches the controller, it passes through a ValidatingWebhookConfiguration that intercepts create and update requests. The webhook validates:</p> <p>What Gets Validated</p> <ul> <li>CR name length: Ensures generated resource names (like <code>{name}-worker</code>) stay under the 63-character Kubernetes limit</li> <li>Secret mount path conflicts: Prevents users from mounting secrets into reserved paths like <code>/lotest/src</code> (where test files live)</li> <li>Volume name conflicts: Blocks use of reserved volume names like <code>secret-*</code> or <code>locust-lib</code></li> <li>OpenTelemetry configuration: When OTel is enabled, the webhook enforces that <code>endpoint</code> is provided</li> </ul> <p>The webhook catches misconfigurations before they hit the controller, giving users immediate feedback with clear error messages. This design prevents invalid CRs from cluttering the cluster or causing cryptic pod errors.</p>"},{"location":"how_does_it_work/#pod-health-monitoring","title":"Pod Health Monitoring","text":"<p>The controller doesn't just watch Jobs \u2014 it actively monitors pod health to surface issues early.</p>"},{"location":"how_does_it_work/#how-pod-watching-works","title":"How Pod Watching Works","text":"<p>Jobs create Pods, so the ownership chain is: <code>LocustTest \u2192 Job \u2192 Pod</code>. Since Pods aren't directly owned by the LocustTest, we use a custom mapping function in the controller setup:</p> <pre><code>Watches(&amp;corev1.Pod{},\n    handler.EnqueueRequestsFromMapFunc(r.mapPodToLocustTest),\n)\n</code></pre> <p>This function walks the owner chain: when a Pod event occurs, it finds the owning Job, then finds the LocustTest that owns the Job, and triggers a reconcile on that LocustTest.</p>"},{"location":"how_does_it_work/#grace-period-for-startup","title":"Grace Period for Startup","text":"<p>Pods take time to start \u2014 scheduling, image pulls, volume mounts \u2014 so the controller applies a 2-minute grace period after the oldest pod is created. During this window, pod failures are ignored to avoid false positives during normal startup.</p> <p>After the grace period expires, the controller analyzes all pods for:</p> <ul> <li>CrashLoopBackOff: Container repeatedly crashing</li> <li>ImagePullBackOff: Can't pull the specified image</li> <li>CreateContainerConfigError: Missing ConfigMap or invalid volume mounts</li> <li>Scheduling errors: No nodes available, insufficient resources, etc.</li> </ul> <p>When unhealthy pods are detected, the controller adds a condition to the LocustTest status with the failure reason and affected pod names. For ConfigMap errors, it even extracts the missing ConfigMap name and suggests creating it.</p>"},{"location":"how_does_it_work/#self-healing-from-external-deletion","title":"Self-Healing from External Deletion","text":"<p>If a user or automation deletes the master Service or any Job while the test is Running, the controller detects the missing resource during the next reconcile. It immediately transitions the LocustTest back to Pending and recreates all resources from scratch. This ensures tests can recover from accidental deletions without manual intervention.</p>"},{"location":"how_does_it_work/#leader-election-high-availability","title":"Leader Election &amp; High Availability","text":"<p>When running multiple replicas of the operator (recommended for production), leader election ensures only one instance actively reconciles resources at a time.</p>"},{"location":"how_does_it_work/#how-it-works","title":"How It Works","text":"<p>The operator uses Kubernetes Lease objects for leader election. When <code>--leader-elect</code> is enabled (the default in the Helm chart), all replicas compete for leadership:</p> <ul> <li>One instance acquires the lease and becomes the active leader</li> <li>Other replicas become standby followers, ready to take over</li> <li>If the leader pod crashes or is evicted, a standby acquires the lease within seconds</li> <li>The new leader picks up reconciliation seamlessly using the LocustTest status</li> </ul>"},{"location":"how_does_it_work/#why-this-matters","title":"Why This Matters","text":"<p>Leader election prevents:</p> <ul> <li>Split-brain scenarios: Multiple controllers trying to create the same resources</li> <li>Resource conflicts: Two controllers racing to update status</li> <li>Operator downtime: If one replica fails, another takes over instantly</li> </ul> <p>The default Helm deployment runs 2 replicas with leader election enabled, providing high availability without resource waste.</p> <p>Leader Election in Development</p> <p>Local development typically runs with <code>--leader-elect=false</code> for simplicity. Multi-replica setups with leader election are primarily for production resilience.</p>"},{"location":"how_does_it_work/#key-design-decisions","title":"Key Design Decisions","text":""},{"location":"how_does_it_work/#immutable-tests","title":"Immutable Tests","text":"<p>Tests are immutable by design. Once a LocustTest CR is created, updates to its <code>spec</code> are ignored by the operator. The operator sets a <code>SpecDrifted</code> condition on the CR to indicate when spec changes have been detected but not applied.</p> <p>To change test parameters (image, commands, replicas, etc.), delete and recreate the CR:</p> <pre><code># Delete the existing test\nkubectl delete locusttest &lt;test-name&gt;\n\n# Edit your CR YAML with the desired changes, then re-apply\nkubectl apply -f locusttest-cr.yaml\n</code></pre> <p>This design ensures:</p> <ul> <li>Predictable behavior \u2014 each test run uses exactly the configuration it was created with</li> <li>Clean test isolation \u2014 no mid-flight configuration drift</li> <li>Simple lifecycle \u2014 create, run, observe, delete</li> </ul>"},{"location":"how_does_it_work/#owner-references","title":"Owner References","text":"<p>All created resources (Jobs, Services) have owner references pointing to the LocustTest CR. This enables:</p> <ul> <li>Automatic garbage collection on CR deletion</li> <li>Clear resource ownership in <code>kubectl get</code></li> <li>No orphaned resources</li> </ul>"},{"location":"how_does_it_work/#status-tracking","title":"Status Tracking","text":"<p>The operator maintains rich status information:</p> <pre><code>status:\n  phase: Running\n  expectedWorkers: 5\n  connectedWorkers: 5\n  startTime: \"2026-01-15T10:00:00Z\"\n  conditions:\n    - type: Ready\n      status: \"True\"\n      lastTransitionTime: \"2026-01-15T10:00:05Z\"\n      reason: AllWorkersConnected\n      message: \"All 5 workers connected to master\"\n</code></pre>"},{"location":"integration-testing/","title":"Testing Guide","text":"<p>This document describes the comprehensive testing setup for the Locust K8s Operator, covering unit tests, integration tests (envtest), and end-to-end tests.</p>"},{"location":"integration-testing/#overview","title":"Overview","text":"<p>The operator uses a multi-layered testing strategy:</p> Test Type Framework Scope Speed Unit Tests Go testing Individual functions Fast (~seconds) Integration Tests envtest Controller + API Server Medium (~30s) E2E Tests Ginkgo + Kind Full cluster deployment Slow (~5-10min)"},{"location":"integration-testing/#test-structure","title":"Test Structure","text":"<pre><code>locust-k8s-operator/\n\u251c\u2500\u2500 api/\n\u2502   \u251c\u2500\u2500 v1/\n\u2502   \u2502   \u2514\u2500\u2500 *_test.go              # v1 API tests\n\u2502   \u2514\u2500\u2500 v2/\n\u2502       \u251c\u2500\u2500 *_test.go              # v2 API tests\n\u2502       \u2514\u2500\u2500 locusttest_webhook_test.go  # Webhook validation tests\n\u251c\u2500\u2500 internal/\n\u2502   \u251c\u2500\u2500 config/\n\u2502   \u2502   \u2514\u2500\u2500 config_test.go         # Configuration tests\n\u2502   \u251c\u2500\u2500 controller/\n\u2502   \u2502   \u251c\u2500\u2500 suite_test.go          # envtest setup\n\u2502   \u2502   \u251c\u2500\u2500 locusttest_controller_test.go  # Unit tests\n\u2502   \u2502   \u2514\u2500\u2500 integration_test.go    # Integration tests\n\u2502   \u2514\u2500\u2500 resources/\n\u2502       \u251c\u2500\u2500 job_test.go            # Job builder tests\n\u2502       \u251c\u2500\u2500 service_test.go        # Service builder tests\n\u2502       \u251c\u2500\u2500 labels_test.go         # Label builder tests\n\u2502       \u251c\u2500\u2500 env_test.go            # Environment builder tests\n\u2502       \u2514\u2500\u2500 command_test.go        # Command builder tests\n\u2514\u2500\u2500 test/\n    \u2514\u2500\u2500 e2e/\n        \u251c\u2500\u2500 e2e_suite_test.go      # E2E test setup\n        \u2514\u2500\u2500 e2e_test.go            # E2E test scenarios\n</code></pre>"},{"location":"integration-testing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Go 1.24+: Required for running tests</li> <li>Docker: Required for E2E tests (Kind)</li> <li>Kind: Required for E2E tests</li> </ul>"},{"location":"integration-testing/#running-tests","title":"Running Tests","text":""},{"location":"integration-testing/#unit-integration-tests-envtest","title":"Unit &amp; Integration Tests (envtest)","text":"<p>The primary test command runs both unit tests and integration tests using envtest:</p> <pre><code># Run all tests with coverage\nmake test\n\n# Run tests with verbose output\ngo test ./... -v\n\n# Run specific package tests\ngo test ./internal/resources/... -v\ngo test ./internal/controller/... -v\ngo test ./api/v2/... -v\n\n# Run specific test by name\ngo test ./internal/controller/... -v -run TestReconcile\n\n# Generate coverage report\nmake test\ngo tool cover -html=cover.out -o coverage.html\n</code></pre>"},{"location":"integration-testing/#e2e-tests-kind","title":"E2E Tests (Kind)","text":"<p>End-to-end tests run against a real Kubernetes cluster using Kind:</p> <pre><code># Run E2E tests (creates Kind cluster automatically)\nmake test-e2e\n\n# Run E2E tests with verbose output\nKIND_CLUSTER=locust-test go test ./test/e2e/ -v -ginkgo.v\n\n# Cleanup E2E test cluster\nmake cleanup-test-e2e\n</code></pre>"},{"location":"integration-testing/#ci-pipeline","title":"CI Pipeline","text":"<p>All tests run automatically in GitHub Actions:</p> <pre><code># Run the same checks as CI locally\nmake ci\n\n# This runs:\n# - make lint (golangci-lint)\n# - make test (unit + integration tests)\n</code></pre>"},{"location":"integration-testing/#test-fixtures","title":"Test Fixtures","text":"<p>Test fixtures and sample data are located in:</p> <ul> <li><code>internal/testdata/</code> - Test fixtures for unit tests</li> <li><code>config/samples/</code> - Sample CRs for integration/E2E tests</li> </ul>"},{"location":"integration-testing/#troubleshooting","title":"Troubleshooting","text":""},{"location":"integration-testing/#common-issues","title":"Common Issues","text":""},{"location":"integration-testing/#envtest-binary-issues","title":"envtest Binary Issues","text":"<pre><code># Re-download envtest binaries\nmake setup-envtest\n\n# Verify binaries are installed\nls bin/k8s/\n</code></pre>"},{"location":"integration-testing/#test-timeouts","title":"Test Timeouts","text":"<pre><code># Increase timeout for slow systems\ngo test ./... -v -timeout 10m\n</code></pre>"},{"location":"integration-testing/#kind-cluster-issues","title":"Kind Cluster Issues","text":"<pre><code># Check if cluster exists\nkind get clusters\n\n# Delete and recreate\nkind delete cluster --name locust-k8s-operator-test-e2e\nmake test-e2e\n</code></pre>"},{"location":"integration-testing/#debug-mode","title":"Debug Mode","text":"<p>Run tests with verbose logging: <pre><code># Verbose test output\ngo test ./internal/controller/... -v -ginkgo.v\n\n# With debug logs from controller\ngo test ./internal/controller/... -v -args -zap-log-level=debug\n</code></pre></p>"},{"location":"integration-testing/#writing-new-tests","title":"Writing New Tests","text":""},{"location":"integration-testing/#guidelines","title":"Guidelines","text":"<ol> <li>Unit tests: Test pure functions in isolation</li> <li>Integration tests: Test controller behavior with envtest</li> <li>E2E tests: Test user-facing scenarios in real cluster</li> </ol>"},{"location":"integration-testing/#test-naming-conventions","title":"Test Naming Conventions","text":"<pre><code>// Unit tests: Test&lt;FunctionName&gt;_&lt;Scenario&gt;\nfunc TestBuildMasterJob_WithEnvConfig(t *testing.T) {}\n\n// Integration tests: Describe/Context/It\nDescribe(\"LocustTest Controller\", func() {\n    Context(\"When creating a LocustTest\", func() {\n        It(\"Should create master Job\", func() {})\n    })\n})\n</code></pre>"},{"location":"integration-testing/#adding-integration-tests","title":"Adding Integration Tests","text":"<ol> <li>Add test to <code>internal/controller/integration_test.go</code></li> <li>Use <code>k8sClient</code> for Kubernetes operations</li> <li>Use <code>Eventually</code> for async assertions</li> <li>Clean up resources in <code>AfterEach</code></li> </ol>"},{"location":"integration-testing/#related-documentation","title":"Related Documentation","text":"<ul> <li>Local Development - Development setup</li> <li>Contributing - Contribution guidelines</li> <li>Pull Request Process - PR workflow</li> </ul>"},{"location":"license/","title":"License","text":"<p>Open source licensed under Apache-2.0 license (see LICENSE file for details).</p>"},{"location":"local-development/","title":"Local Development Guide","text":"<p>This guide describes the setup and workflow for local development on the Locust K8s Operator project. It's intended for developers who want to contribute code changes.</p>"},{"location":"local-development/#development-setup","title":"Development Setup","text":""},{"location":"local-development/#prerequisites","title":"Prerequisites","text":"<ul> <li>Go 1.24+: Required for building the operator</li> <li>Docker: Running Docker daemon for building images</li> <li>kubectl: Kubernetes CLI for cluster interaction</li> <li>Kind or Minikube: Local Kubernetes cluster for testing</li> <li>Helm 3.x: For chart packaging and installation</li> </ul>"},{"location":"local-development/#initial-setup","title":"Initial Setup","text":"<ol> <li> <p>Clone the repository:    <pre><code>git clone https://github.com/AbdelrhmanHamouda/locust-k8s-operator.git\ncd locust-k8s-operator\n</code></pre></p> </li> <li> <p>Install dependencies and tools:    <pre><code># Download Go dependencies\nmake tidy\n\n# Install development tools (controller-gen, envtest, etc.)\nmake controller-gen\nmake envtest\nmake kustomize\n</code></pre></p> </li> </ol>"},{"location":"local-development/#development-guidelines","title":"Development Guidelines","text":"<ul> <li> <p>This project follows the Conventional Commits standard to automate Semantic Versioning and Keep A Changelog with Commitizen.</p> </li> <li> <p>All code should include appropriate tests. See the integration testing guide for details on the test setup.</p> </li> </ul>"},{"location":"local-development/#common-development-commands","title":"Common Development Commands","text":"<p>The project uses a <code>Makefile</code> for common development tasks. Run <code>make help</code> to see all available targets.</p>"},{"location":"local-development/#build-test","title":"Build &amp; Test","text":"<pre><code># Build the operator binary\nmake build\n\n# Run all tests (unit + integration via envtest)\nmake test\n\n# Run linter\nmake lint\n\n# Run linter with auto-fix\nmake lint-fix\n\n# Run all CI checks locally\nmake ci\n</code></pre>"},{"location":"local-development/#code-generation","title":"Code Generation","text":"<pre><code># Generate CRDs, RBAC, and webhook manifests\nmake manifests\n\n# Generate DeepCopy implementations\nmake generate\n\n# Format code\nmake fmt\n\n# Run go vet\nmake vet\n</code></pre>"},{"location":"local-development/#running-locally","title":"Running Locally","text":"<pre><code># Run the operator locally against your current kubeconfig cluster\nmake run\n\n# Install CRDs into the cluster\nmake install\n\n# Uninstall CRDs from the cluster\nmake uninstall\n</code></pre>"},{"location":"local-development/#local-testing-with-kind","title":"Local Testing with Kind","text":"<p>For local development and testing, Kind (Kubernetes in Docker) is the recommended approach.</p>"},{"location":"local-development/#steps","title":"Steps","text":"<ol> <li>Create a Kind Cluster</li> </ol> <pre><code>kind create cluster --name locust-dev\n</code></pre> <ol> <li>Build and Load the Docker Image</li> </ol> <pre><code># Build the Docker image\nmake docker-build IMG=locust-k8s-operator:dev\n\n# Load the image into Kind\nkind load docker-image locust-k8s-operator:dev --name locust-dev\n</code></pre> <ol> <li>Deploy the Operator</li> </ol> <p>Option A: Using kustomize (for development):    <pre><code># Deploy CRDs and operator\nmake deploy IMG=locust-k8s-operator:dev\n</code></pre></p> <p>Option B: Using Helm (for production-like testing):    <pre><code># Package the Helm chart\nhelm package ../charts/locust-k8s-operator\n\n# Install with local image\nhelm install locust-operator locust-k8s-operator-*.tgz \\\n  --set image.repository=locust-k8s-operator \\\n  --set image.tag=dev \\\n  --set image.pullPolicy=IfNotPresent\n</code></pre></p> <ol> <li>Verify the Deployment</li> </ol> <p>!!! note \"Development vs Production Namespaces\"        The <code>make deploy</code> command generates a namespace based on your project name. For production deployments, use the <code>locust-system</code> namespace as documented in the Helm Deployment Guide.</p> <pre><code># Check pods in the generated namespace\nkubectl get pods -A | grep locust\n\n# Follow operator logs\nkubectl logs -f -n &lt;namespace&gt; deployment/&lt;deployment-name&gt;\n</code></pre> <ol> <li>Test with a Sample CR</li> </ol> <pre><code># Create a test ConfigMap with a simple Locust script\nkubectl create configmap locust-test --from-literal=locustfile.py='\nfrom locust import HttpUser, task\nclass TestUser(HttpUser):\n    @task\n    def hello(self):\n        self.client.get(\"/\")\n'\n\n# Apply a sample LocustTest CR\nkubectl apply -f config/samples/locust_v2_locusttest.yaml\n\n# Watch the resources\nkubectl get locusttests,jobs,pods -w\n</code></pre> <ol> <li>Cleanup</li> </ol> <pre><code># Remove the operator\nmake undeploy\n\n# Delete the Kind cluster\nkind delete cluster --name locust-dev\n</code></pre>"},{"location":"local-development/#writing-documentation","title":"Writing Documentation","text":"<p>All documentation is located under the <code>docs/</code> directory. The documentation is hosted on GitHub Pages and updated automatically with each release. To manage and build the documentation, the project uses MkDocs &amp; Material for MkDocs framework.</p>"},{"location":"local-development/#preview-documentation-locally","title":"Preview Documentation Locally","text":"<pre><code># Install MkDocs (if not installed)\npip install mkdocs mkdocs-material\n\n# Serve documentation locally\nmkdocs serve\n\n# Build documentation\nmkdocs build --strict\n</code></pre> <p>During development, the CI workflow will build the documentation as part of the validation.</p>"},{"location":"metrics_and_dashboards/","title":"Metrics &amp; Dashboards","text":"","tags":["monitoring","metrics","dashboards","prometheus","observability"]},{"location":"metrics_and_dashboards/#opentelemetry-metrics-traces","title":"OpenTelemetry Metrics &amp; Traces","text":"<p>New in v2.0</p> <p>Native OpenTelemetry support is available in the v2 API.</p>","tags":["monitoring","metrics","dashboards","prometheus","observability"]},{"location":"metrics_and_dashboards/#native-opentelemetry-support","title":"Native OpenTelemetry Support","text":"<p>Locust 2.x includes native OpenTelemetry support, which the operator can configure automatically. This provides both metrics and distributed tracing without requiring the metrics exporter sidecar.</p>","tags":["monitoring","metrics","dashboards","prometheus","observability"]},{"location":"metrics_and_dashboards/#configuring-otel","title":"Configuring OTel","text":"<p>Enable OpenTelemetry in your LocustTest CR:</p> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: otel-test\nspec:\n  image: locustio/locust:2.20.0\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://example.com\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 5\n  observability:\n    openTelemetry:\n      enabled: true\n      endpoint: \"http://otel-collector.monitoring:4317\"\n      protocol: \"grpc\"\n</code></pre> <p>See Advanced Topics - OpenTelemetry for detailed configuration options.</p>","tags":["monitoring","metrics","dashboards","prometheus","observability"]},{"location":"metrics_and_dashboards/#otel-collector-setup","title":"OTel Collector Setup","text":"<p>For a complete observability setup, deploy an OTel Collector. Example configuration:</p> <pre><code># otel-collector-config.yaml\nreceivers:\n  otlp:\n    protocols:\n      grpc:\n        endpoint: 0.0.0.0:4317\n      http:\n        endpoint: 0.0.0.0:4318\n\nexporters:\n  prometheus:\n    endpoint: 0.0.0.0:8889\n  otlphttp:\n    endpoint: http://jaeger-collector:4318\n    tls:\n      insecure: true\n\nservice:\n  pipelines:\n    metrics:\n      receivers: [otlp]\n      exporters: [prometheus]\n    traces:\n      receivers: [otlp]\n      exporters: [otlphttp]\n</code></pre> <p>Tip</p> <p>The Helm chart includes an optional OTel Collector deployment. Enable it with <code>otelCollector.enabled: true</code>.</p>","tags":["monitoring","metrics","dashboards","prometheus","observability"]},{"location":"metrics_and_dashboards/#operator-metrics","title":"Operator Metrics","text":"<p>The Go operator can expose controller-runtime metrics (disabled by default). When enabled, metrics are served on the configured port (default: 8080):</p> Metric Description <code>controller_runtime_reconcile_total</code> Total reconciliations <code>controller_runtime_reconcile_errors_total</code> Reconciliation errors <code>controller_runtime_reconcile_time_seconds</code> Reconciliation duration <code>workqueue_depth</code> Current queue depth <code>workqueue_adds_total</code> Items added to queue <p>These metrics can be scraped by Prometheus using the standard <code>/metrics</code> endpoint on the operator pod.</p>","tags":["monitoring","metrics","dashboards","prometheus","observability"]},{"location":"metrics_and_dashboards/#enabling-operator-metrics","title":"Enabling Operator Metrics","text":"<p>Enable metrics in your Helm values:</p> <pre><code>metrics:\n  enabled: true\n</code></pre> <p>Then configure Prometheus to scrape the operator:</p> <pre><code>scrape_configs:\n  - job_name: 'locust-operator'\n    kubernetes_sd_configs:\n      - role: pod\n    relabel_configs:\n      - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_name]\n        action: keep\n        regex: locust-k8s-operator\n      - source_labels: [__address__]\n        action: replace\n        regex: ([^:]+)(?::\\d+)?\n        replacement: $1:8080\n        target_label: __address__\n</code></pre>","tags":["monitoring","metrics","dashboards","prometheus","observability"]},{"location":"metrics_and_dashboards/#servicemonitor-prometheus-operator","title":"ServiceMonitor (Prometheus Operator)","text":"<p>If using the Prometheus Operator, create a ServiceMonitor to automatically discover and scrape operator metrics:</p> HTTP MetricsHTTPS Metrics (Recommended) <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: locust-operator-metrics\n  namespace: locust-operator-system\n  labels:\n    app.kubernetes.io/name: locust-k8s-operator\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: locust-k8s-operator\n  endpoints:\n    - port: metrics\n      path: /metrics\n      interval: 30s\n</code></pre> <p>Ensure Helm values have: <pre><code>metrics:\n  enabled: true\n  secure: false  # HTTP metrics\n</code></pre></p> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: locust-operator-metrics\n  namespace: locust-operator-system\n  labels:\n    app.kubernetes.io/name: locust-k8s-operator\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: locust-k8s-operator\n  endpoints:\n    - port: metrics\n      path: /metrics\n      interval: 30s\n      scheme: https\n      bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token\n      tlsConfig:\n        insecureSkipVerify: true  # For development only\n</code></pre> <p>Ensure Helm values have: <pre><code>metrics:\n  enabled: true\n  secure: true  # Enable HTTPS metrics (default: false)\n</code></pre></p> <p>Production TLS</p> <p>For production, use cert-manager to manage TLS certificates instead of <code>insecureSkipVerify: true</code>. See the operator's <code>config/prometheus/</code> directory for examples.</p>","tags":["monitoring","metrics","dashboards","prometheus","observability"]},{"location":"metrics_and_dashboards/#operator-metrics-queries","title":"Operator Metrics Queries","text":"<p>Useful PromQL queries for operator monitoring:</p> <pre><code># Reconciliation rate (per second)\nrate(controller_runtime_reconcile_total[5m])\n\n# Reconciliation error rate\nrate(controller_runtime_reconcile_errors_total[5m])\n\n# Average reconciliation duration\nrate(controller_runtime_reconcile_time_seconds_sum[5m]) \n  / rate(controller_runtime_reconcile_time_seconds_count[5m])\n\n# Current workqueue depth\nworkqueue_depth\n\n# Queue processing rate\nrate(workqueue_adds_total[5m])\n</code></pre>","tags":["monitoring","metrics","dashboards","prometheus","observability"]},{"location":"metrics_and_dashboards/#locust-test-metrics","title":"Locust Test Metrics","text":"<p>Two Metrics Approaches - Choose One</p> <p>The operator provides two mutually exclusive methods for collecting Locust test metrics:</p> <p>1. Prometheus Exporter Sidecar (default, v1 &amp; v2 API)</p> <ul> <li>Uses <code>containersol/locust_exporter</code> sidecar on port 9646</li> <li>Exposes Prometheus-formatted metrics</li> <li>Works with Prometheus scraping</li> <li>Documented in this section below</li> </ul> <p>2. Native OpenTelemetry (v2 API only)</p> <ul> <li>Locust exports directly via OTLP protocol</li> <li>No sidecar container needed</li> <li>Metrics sent to OTel Collector</li> <li>See OpenTelemetry section above</li> </ul> <p>When OTel is enabled, the exporter sidecar is NOT deployed. All Prometheus exporter documentation below only applies to non-OTel mode.</p>","tags":["monitoring","metrics","dashboards","prometheus","observability"]},{"location":"metrics_and_dashboards/#metrics-exporter-sidecar-non-otel-mode","title":"Metrics Exporter Sidecar (Non-OTel Mode)","text":"<p>When OpenTelemetry is not enabled, the operator automatically injects a Prometheus metrics exporter sidecar into the Locust master pod. This exporter scrapes Locust's built-in stats endpoint and exposes metrics in Prometheus format.</p> <p>What the Operator Creates Automatically:</p> <ol> <li>Metrics Exporter Sidecar Container:</li> <li>Image: <code>containersol/locust_exporter:v0.5.0</code></li> <li>Port: 9646</li> <li> <p>Path: <code>/metrics</code></p> </li> <li> <p>Kubernetes Service: <code>&lt;test-name&gt;-master</code></p> </li> <li>Includes metrics port 9646</li> <li> <p>Provides stable DNS endpoint</p> </li> <li> <p>Pod Annotations (for Prometheus auto-discovery):    <pre><code>prometheus.io/scrape: \"true\"\nprometheus.io/path: \"/metrics\"\nprometheus.io/port: \"9646\"\n</code></pre></p> </li> </ol> <p>No manual setup required - the operator handles everything.</p>","tags":["monitoring","metrics","dashboards","prometheus","observability"]},{"location":"metrics_and_dashboards/#available-locust-metrics","title":"Available Locust Metrics","text":"<p>The exporter provides these key metrics from Locust:</p> Metric Type Description <code>locust_requests_total</code> Counter Total number of requests <code>locust_requests_current_rps</code> Gauge Current requests per second <code>locust_requests_current_fail_per_sec</code> Gauge Current failures per second <code>locust_requests_avg_response_time</code> Gauge Average response time (ms) <code>locust_requests_min_response_time</code> Gauge Minimum response time (ms) <code>locust_requests_max_response_time</code> Gauge Maximum response time (ms) <code>locust_requests_avg_content_length</code> Gauge Average response size (bytes) <code>locust_users</code> Gauge Current number of simulated users <code>locust_errors</code> Counter Total errors by type <p>For the complete list, see the locust_exporter documentation.</p>","tags":["monitoring","metrics","dashboards","prometheus","observability"]},{"location":"metrics_and_dashboards/#locust-metrics-queries","title":"Locust Metrics Queries","text":"<p>Useful PromQL queries for load test monitoring:</p> <pre><code># Total request rate across all endpoints\nsum(rate(locust_requests_total[1m]))\n\n# Error rate\nsum(rate(locust_errors[1m]))\n\n# Average response time\navg(locust_requests_avg_response_time)\n\n# Max response time (locust_exporter exposes gauges, not histograms)\nmax(locust_requests_max_response_time)\n\n# Current active users\nsum(locust_users)\n\n# Request rate by endpoint\nsum(rate(locust_requests_total[1m])) by (name, method)\n\n# Error percentage\n100 * sum(rate(locust_errors[1m])) \n  / sum(rate(locust_requests_total[1m]))\n</code></pre>","tags":["monitoring","metrics","dashboards","prometheus","observability"]},{"location":"metrics_and_dashboards/#integration-examples","title":"Integration Examples","text":"<p>The metrics are automatically exposed by the operator-created Service and pod annotations. Simply configure your monitoring tools to discover them:</p> <p>Prometheus: Configure Kubernetes service discovery to scrape pods with <code>prometheus.io/scrape: \"true\"</code> annotation. The operator adds these annotations automatically - no manual configuration of individual tests needed.</p> <p>Grafana: Connect to your Prometheus datasource and create dashboards using the PromQL queries above. Import panels from existing Locust dashboard examples.</p> <p>NewRelic: Deploy a Prometheus agent configured to scrape Kubernetes pods with <code>prometheus.io/scrape: true</code> and forward metrics to NewRelic. See Issue #118 for production deployment patterns.</p> <p>DataDog: Configure the DataDog agent's Prometheus integration to auto-discover and scrape pods with <code>prometheus.io/*</code> annotations. The DataDog agent automatically finds operator-created test pods.</p> <p>Production Deployment</p> <p>For large-scale deployments, see Issue #118 which documents production patterns used with thousands of tests in NewRelic and DataDog environments.</p>","tags":["monitoring","metrics","dashboards","prometheus","observability"]},{"location":"migration/","title":"Migration Guide: v1 to v2","text":"<p>This guide helps existing users of the Locust Kubernetes Operator upgrade from v1 to v2. The v2 release is a complete rewrite in Go, bringing significant performance improvements and new features.</p>","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#overview","title":"Overview","text":"","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#why-we-rewrote-in-go","title":"Why We Rewrote in Go","text":"<p>The v2 operator was rewritten from Java to Go for several key reasons:</p> Aspect Java (v1) Go (v2) Memory footprint ~256MB ~64MB Startup time ~60 seconds &lt;1 second Framework Java Operator SDK Operator SDK / controller-runtime Ecosystem alignment Minority Majority of K8s operators","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#what-changes-for-users","title":"What Changes for Users","text":"<ul> <li>API Version: New <code>locust.io/v2</code> API with grouped configuration</li> <li>Backward Compatibility: v1 CRs continue to work via automatic conversion</li> <li>New Features: OpenTelemetry, secret injection, volume mounting, separate resource specs</li> <li>Helm Chart: Updated values structure (backward compatible)</li> </ul>","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#compatibility-guarantees","title":"Compatibility Guarantees","text":"<ul> <li>v1 API: Fully supported via conversion webhook (deprecated, will be removed in v3)</li> <li>Existing CRs: Work without modification</li> <li>Helm Values: Backward compatibility shims for common settings</li> </ul>","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#before-you-begin","title":"Before You Begin","text":"","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes 1.25+</li> <li>Helm 3.x</li> <li>cert-manager v1.14+ (required for conversion webhook)</li> </ul>","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#backup-recommendations","title":"Backup Recommendations","text":"<p>Before upgrading, back up your existing resources:</p> <pre><code># Export all LocustTest CRs\nkubectl get locusttests -A -o yaml &gt; locusttests-backup.yaml\n\n# Export operator Helm values\nhelm get values locust-operator -n &lt;namespace&gt; &gt; values-backup.yaml\n</code></pre> <p>Critical: Webhook Required for v1 API Compatibility</p> <p>If you have existing v1 <code>LocustTest</code> CRs, the conversion webhook is required for them to continue working after upgrading to v2. Without it, v1 CRs will fail CRD schema validation.</p> <p>You must:</p> <ol> <li>Install cert-manager before upgrading</li> <li>Enable the webhook during upgrade: <code>--set webhook.enabled=true</code></li> <li>Verify the webhook is running after upgrade</li> </ol> <p>If you only use v2 CRs (or are starting fresh), the webhook is optional.</p>","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#step-1-update-helm-chart","title":"Step 1: Update Helm Chart","text":"","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#upgrade-command","title":"Upgrade Command","text":"<pre><code># Update Helm repository\nhelm repo update locust-k8s-operator\n\n# Upgrade to v2 (with webhook for v1 CR compatibility)\nhelm upgrade locust-operator locust-k8s-operator/locust-k8s-operator \\\n  --namespace locust-system \\\n  --version 2.0.0 \\\n  --set webhook.enabled=true\n\n# If you don't need v1 API compatibility, you can omit --set webhook.enabled=true\n</code></pre> <p>CRD Upgrade</p> <p>Helm automatically upgrades the CRD when using <code>helm upgrade</code>. The v2 CRD includes conversion webhook configuration when webhooks are enabled, allowing the API server to convert between v1 and v2 formats transparently.</p>","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#new-helm-values","title":"New Helm Values","text":"<p>The v2 chart introduces a cleaner structure. Key changes:</p> Old Path (v1) New Path (v2) Notes <code>config.loadGenerationPods.resource.cpuRequest</code> <code>locustPods.resources.requests.cpu</code> Backward compatible <code>config.loadGenerationPods.resource.memLimit</code> <code>locustPods.resources.limits.memory</code> Backward compatible <code>config.loadGenerationPods.affinity.enableCrInjection</code> <code>locustPods.affinityInjection</code> Backward compatible <code>micronaut.*</code> N/A Removed (Java-specific) <code>appPort</code> N/A Fixed at 8081 N/A <code>webhook.enabled</code> New: Enable conversion webhook N/A <code>leaderElection.enabled</code> New: Enable leader election","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#operator-resource-defaults","title":"Operator Resource Defaults","text":"<p>The Go operator controller requires significantly fewer resources than the Java version:</p> <pre><code>resources:\n  limits:\n    memory: 256Mi\n    cpu: 500m\n  requests:\n    memory: 64Mi\n    cpu: 10m\n</code></pre>","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#step-2-verify-existing-crs","title":"Step 2: Verify Existing CRs","text":"<p>The conversion webhook automatically converts v1 CRs to v2 format when stored. Verify your existing CRs work:</p> <pre><code># List all LocustTests\nkubectl get locusttests -A\n\n# Check a specific CR\nkubectl describe locusttest &lt;name&gt;\n</code></pre>","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#verify-conversion","title":"Verify Conversion","text":"<p>You can read a v1 CR as v2 to verify conversion:</p> <pre><code># Read as v2 (even if created as v1)\nkubectl get locusttest &lt;name&gt; -o yaml | grep \"apiVersion:\"\n# Should show: apiVersion: locust.io/v2\n</code></pre> <p>Deprecation Warning</p> <p>When using the v1 API, you'll see a deprecation warning in kubectl output. This is expected and indicates the conversion webhook is working.</p>","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#step-3-migrate-crs-to-v2-format-recommended","title":"Step 3: Migrate CRs to v2 Format (Recommended)","text":"<p>While v1 CRs continue to work, migrating to v2 format is recommended to access new features.</p>","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#field-mapping-reference","title":"Field Mapping Reference","text":"v1 Field v2 Field Notes <code>masterCommandSeed</code> <code>master.command</code> Direct mapping <code>workerCommandSeed</code> <code>worker.command</code> Direct mapping <code>workerReplicas</code> <code>worker.replicas</code> Direct mapping <code>image</code> <code>image</code> No change <code>imagePullPolicy</code> <code>imagePullPolicy</code> No change <code>imagePullSecrets</code> <code>imagePullSecrets</code> No change <code>configMap</code> <code>testFiles.configMapRef</code> Grouped under testFiles <code>libConfigMap</code> <code>testFiles.libConfigMapRef</code> Grouped under testFiles <code>labels.master</code> <code>master.labels</code> Grouped under master <code>labels.worker</code> <code>worker.labels</code> Grouped under worker <code>annotations.master</code> <code>master.annotations</code> Grouped under master <code>annotations.worker</code> <code>worker.annotations</code> Grouped under worker <code>affinity.nodeAffinity</code> <code>scheduling.affinity</code> Uses native K8s Affinity \u26a0\ufe0f<sup>1</sup> <code>tolerations</code> <code>scheduling.tolerations</code> Uses native K8s Tolerations N/A <code>master.resources</code> New: Separate resource specs for master N/A <code>worker.resources</code> New: Separate resource specs for worker N/A <code>master.extraArgs</code> New: Additional CLI arguments for master N/A <code>worker.extraArgs</code> New: Additional CLI arguments for worker N/A <code>master.autostart</code> Auto-added during conversion (default: true) N/A <code>master.autoquit</code> Auto-added during conversion (enabled: true, timeout: 60s)","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#example-transformation","title":"Example Transformation","text":"v1 Format (Deprecated)v2 Format <pre><code>apiVersion: locust.io/v1\nkind: LocustTest\nmetadata:\n  name: example-test\nspec:\n  image: locustio/locust:2.20.0\n  masterCommandSeed: --locustfile /lotest/src/test.py --host https://example.com\n  workerCommandSeed: --locustfile /lotest/src/test.py\n  workerReplicas: 5\n  configMap: test-scripts\n  labels:\n    master:\n      team: platform\n    worker:\n      team: platform\n</code></pre> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: example-test\nspec:\n  image: locustio/locust:2.20.0\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://example.com\"\n    labels:\n      team: platform\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 5\n    labels:\n      team: platform\n  testFiles:\n    configMapRef: test-scripts\n</code></pre>","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#lossy-conversion-details","title":"Lossy Conversion Details","text":"<p>V2-Only Fields Not Preserved in V1</p> <p>When reading v2 CRs as v1 (or during rollback to v1), the following v2-exclusive fields will be lost:</p> <p>Master/Worker Configuration:</p> <ul> <li><code>master.resources</code> - Separate resource specs for master pod</li> <li><code>worker.resources</code> - Separate resource specs for worker pod</li> <li><code>master.extraArgs</code> - Additional CLI arguments for master</li> <li><code>worker.extraArgs</code> - Additional CLI arguments for worker</li> <li><code>master.autostart</code> - Autostart configuration</li> <li><code>master.autoquit</code> - Autoquit configuration</li> </ul> <p>Test Files:</p> <ul> <li><code>testFiles.srcMountPath</code> - Custom mount path for test files</li> <li><code>testFiles.libMountPath</code> - Custom mount path for library files</li> </ul> <p>Scheduling:</p> <ul> <li><code>scheduling.nodeSelector</code> - Node selector (v1 only supports nodeAffinity)</li> <li>Complex affinity rules (see warning above)</li> </ul> <p>Environment &amp; Secrets:</p> <ul> <li><code>env.configMapRefs</code> - ConfigMap environment injection</li> <li><code>env.secretRefs</code> - Secret environment injection</li> <li><code>env.variables</code> - Individual environment variables</li> <li><code>env.secretMounts</code> - Secret file mounts</li> </ul> <p>Volumes:</p> <ul> <li><code>volumes</code> - Volume definitions</li> <li><code>volumeMounts</code> - Volume mounts with target selection</li> </ul> <p>Observability:</p> <ul> <li><code>observability.openTelemetry</code> - OpenTelemetry configuration</li> </ul> <p>Status:</p> <ul> <li>All <code>status</code> subresource fields (v1 has no status implementation)</li> </ul> <p>Recommendation: Before rolling back from v2 to v1, backup your v2 CRs to preserve this configuration.</p>","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#step-4-leverage-new-features","title":"Step 4: Leverage New Features","text":"<p>After migrating to v2, you can use new features:</p>","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#opentelemetry-support","title":"OpenTelemetry Support","text":"<pre><code>spec:\n  observability:\n    openTelemetry:\n      enabled: true\n      endpoint: \"otel-collector.monitoring:4317\"\n      protocol: \"grpc\"\n</code></pre> <p> Learn more about OpenTelemetry</p>","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#secret-configmap-injection","title":"Secret &amp; ConfigMap Injection","text":"<pre><code>spec:\n  env:\n    secretRefs:\n      - name: api-credentials\n        prefix: \"API_\"\n    configMapRefs:\n      - name: app-config\n    variables:\n      - name: TARGET_HOST\n        value: \"https://api.example.com\"\n</code></pre> <p> Learn more about Environment Injection</p>","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#volume-mounting","title":"Volume Mounting","text":"<pre><code>spec:\n  volumes:\n    - name: test-data\n      persistentVolumeClaim:\n        claimName: test-data-pvc\n  volumeMounts:\n    - name: test-data\n      mountPath: /data\n      target: both  # master, worker, or both\n</code></pre> <p> Learn more about Volume Mounting</p>","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#separate-resource-specs","title":"Separate Resource Specs","text":"<pre><code>spec:\n  master:\n    resources:\n      requests:\n        memory: \"256Mi\"\n        cpu: \"100m\"\n  worker:\n    resources:\n      requests:\n        memory: \"512Mi\"\n        cpu: \"500m\"\n</code></pre> <p> Learn more about Separate Resources</p>","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#troubleshooting","title":"Troubleshooting","text":"","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#common-issues","title":"Common Issues","text":"","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#conversion-webhook-not-working","title":"Conversion Webhook Not Working","text":"<p>Symptom: v1 CRs fail with schema validation errors</p> <p>Solution: Ensure cert-manager is installed and the webhook is enabled:</p> <pre><code># Check cert-manager\nkubectl get pods -n cert-manager\n\n# Enable webhook in Helm\nhelm upgrade locust-operator locust-k8s-operator/locust-k8s-operator \\\n  --set webhook.enabled=true\n</code></pre>","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#resources-not-created","title":"Resources Not Created","text":"<p>Symptom: LocustTest CR created but no Jobs/Services appear</p> <p>Solution: Check operator logs:</p> <pre><code>kubectl logs -n locust-system -l app.kubernetes.io/name=locust-k8s-operator\n</code></pre>","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#status-not-updating","title":"Status Not Updating","text":"<p>Symptom: LocustTest status remains empty</p> <p>Solution: Verify RBAC permissions include <code>locusttests/status</code>:</p> <pre><code>kubectl auth can-i update locusttests/status --as=system:serviceaccount:locust-system:locust-operator\n</code></pre>","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#how-to-get-help","title":"How to Get Help","text":"<ul> <li>GitHub Issues</li> </ul>","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#rollback-procedure","title":"Rollback Procedure","text":"<p>If you need to revert to v1:</p> <pre><code># Rollback Helm release\nhelm rollback locust-operator &lt;previous-revision&gt; -n locust-system\n\n# Or reinstall v1\nhelm install locust-operator locust-k8s-operator/locust-k8s-operator \\\n  --version 1.1.1 \\\n  -f values-backup.yaml\n</code></pre> <p>Note</p> <p>After rollback, v2-specific fields in CRs will be lost. Ensure you have backups of any v2-only configurations.</p> <ol> <li> <p>Affinity Conversion Note: When converting v2 \u2192 v1, complex affinity rules may be simplified. Only <code>NodeSelectorOpIn</code> operators are preserved, and only the first value from multi-value expressions is kept. Pod affinity/anti-affinity and preferred scheduling rules are not preserved in v1.\u00a0\u21a9</p> </li> </ol>","tags":["migration","upgrade","v2","guide"]},{"location":"pull-request-process/","title":"Pull Request Process","text":"<p>This document outlines the process for submitting pull requests to the Locust K8s Operator project. Following these guidelines helps maintain code quality and ensures a smooth review process.</p>"},{"location":"pull-request-process/#before-creating-a-pull-request","title":"Before Creating a Pull Request","text":"<ol> <li> <p>Discuss Changes First: Before making significant changes, please discuss the proposed changes via an issue in the GitHub repository.</p> </li> <li> <p>Follow Coding Conventions: Ensure your code follows the project's coding standards and conventions.</p> </li> <li> <p>Write Tests: All new features or bug fixes should be covered by appropriate tests. See the testing guide for details on the testing setup.</p> </li> </ol>"},{"location":"pull-request-process/#pull-request-workflow","title":"Pull Request Workflow","text":"<ol> <li> <p>Fork and Clone: Fork the repository and clone it locally.</p> </li> <li> <p>Create a Branch: Create a branch for your changes using a descriptive name:    <pre><code>git checkout -b feature/your-feature-name\n</code></pre></p> </li> <li> <p>Make Your Changes: Implement your changes, following Go coding standards.</p> </li> <li> <p>Commit Your Changes: Use the Conventional Commits standard for commit messages. This is important as the commit messages directly influence the content of the CHANGELOG.md and version increments.</p> </li> </ol> <p>Examples of good commit messages:    <pre><code>feat: add support for OpenTelemetry metrics export\nfix: correct volume mount path validation\ndocs: update API reference for v2 fields\nrefactor: simplify resource builder functions\ntest: add integration tests for env injection\n</code></pre></p> <ol> <li> <p>Run Tests Locally: Run tests and linting to ensure your changes don't break existing functionality:    <pre><code>cd locust-k8s-operator\n\n# Run all CI checks (lint + tests)\nmake ci\n\n# Or run individually:\nmake lint        # Run linter\nmake test        # Run unit + integration tests\nmake test-e2e    # Run E2E tests (requires Docker)\n</code></pre></p> </li> <li> <p>Generate Manifests: If you modified API types, regenerate manifests:    <pre><code>make generate    # Generate DeepCopy methods\nmake manifests   # Generate CRDs, RBAC, webhooks\n</code></pre></p> </li> <li> <p>Submit Your Pull Request: Push your branch to your fork and submit a pull request to the main repository.</p> </li> </ol>"},{"location":"pull-request-process/#pull-request-requirements","title":"Pull Request Requirements","text":""},{"location":"pull-request-process/#code-quality","title":"Code Quality","text":"<ul> <li> Code follows Go conventions and project style</li> <li> No linting errors (<code>make lint</code> passes)</li> <li> All tests pass (<code>make test</code> passes)</li> <li> New code has appropriate test coverage (\u226580% for new packages)</li> </ul>"},{"location":"pull-request-process/#documentation","title":"Documentation","text":"<ul> <li> API changes are reflected in <code>docs/api_reference.md</code></li> <li> New features are documented in <code>docs/features.md</code> or <code>docs/advanced_topics.md</code></li> <li> Breaking changes are noted in the PR description</li> <li> Helm chart updates include <code>docs/helm_deploy.md</code> changes</li> </ul>"},{"location":"pull-request-process/#commit-messages","title":"Commit Messages","text":"<ul> <li> Follow Conventional Commits standard</li> <li> Each commit represents a logical unit of change</li> <li> Commit messages are clear and descriptive</li> </ul>"},{"location":"pull-request-process/#tests","title":"Tests","text":"<ul> <li> Unit tests for new/modified functions</li> <li> Integration tests for controller behavior changes</li> <li> Existing tests updated if behavior changes</li> <li> No test regressions</li> </ul>"},{"location":"pull-request-process/#ci-pipeline-checks","title":"CI Pipeline Checks","text":"<p>The following checks run automatically on each PR:</p> Check Description Command Lint golangci-lint static analysis <code>make lint</code> Test Unit + integration tests <code>make test</code> Build Binary compilation <code>make build</code> Manifests CRD/RBAC generation <code>make manifests</code> <p>All checks must pass before a PR can be merged.</p>"},{"location":"pull-request-process/#review-process","title":"Review Process","text":"<ol> <li> <p>Initial Review: Maintainers will review your PR to ensure it meets the project's requirements.</p> </li> <li> <p>CI Checks: The CI system will run tests and other checks against your PR. Make sure these pass.</p> </li> <li> <p>Feedback: Maintainers may request changes or improvements to your PR.</p> </li> <li> <p>Merge: Once approved and CI passes, a maintainer will merge your PR.</p> </li> </ol>"},{"location":"pull-request-process/#after-your-pr-is-merged","title":"After Your PR is Merged","text":"<ol> <li> <p>Update Your Fork: Keep your fork up to date with the main repository:    <pre><code>git checkout master\ngit pull upstream master\ngit push origin master\n</code></pre></p> </li> <li> <p>Celebrate: Thank you for contributing to the Locust K8s Operator project! Your efforts help make the project better for everyone.</p> </li> </ol> <p>Remember that this is a collaborative open-source project. Constructive feedback and discussions are welcome, and all interactions should adhere to the project's Code of Conduct.</p>"},{"location":"security/","title":"Security Best Practices","text":"<p>This guide covers security best practices for deploying and operating the Locust Kubernetes Operator in production environments. It provides practical examples for RBAC configuration, secret management, and security hardening.</p>","tags":["security","rbac","secrets","guide"]},{"location":"security/#operator-rbac-permissions","title":"Operator RBAC Permissions","text":"","tags":["security","rbac","secrets","guide"]},{"location":"security/#what-the-operator-needs","title":"What the Operator Needs","text":"<p>The operator follows least-privilege principles. It requires specific permissions to manage LocustTest resources and create load test infrastructure.</p> Resource Verbs Purpose <code>locusttests</code> get, list, watch, update, patch Watch CRs and reconcile state <code>locusttests/status</code> get, update, patch Report test status <code>locusttests/finalizers</code> update Manage deletion lifecycle <code>configmaps</code> get, list, watch Read test files and library code <code>secrets</code> get, list, watch Read credentials for env injection <code>services</code> get, list, watch, create, delete Master service for worker communication <code>pods</code> get, list, watch Monitor pod health for status reporting <code>events</code> create, patch Report status changes and errors <code>jobs</code> get, list, watch, create, delete Master and worker pods (immutable pattern) <code>leases</code> get, list, watch, create, update, patch Leader election (only when HA enabled) <p>Read-Only Secret Access</p> <p>The operator never creates or modifies ConfigMaps or Secrets. It only reads them to populate environment variables and volume mounts in test pods. Users manage Secret creation and rotation.</p>","tags":["security","rbac","secrets","guide"]},{"location":"security/#namespace-scoped-vs-cluster-scoped","title":"Namespace-Scoped vs Cluster-Scoped","text":"<p>The operator supports two RBAC modes:</p> <p>ClusterRole (<code>k8s.clusterRole.enabled: true</code>, default)</p> <ul> <li>Operator manages LocustTest CRs in all namespaces</li> <li>Use when multiple teams share one operator deployment</li> <li>Typical for platform teams managing centralized performance testing</li> </ul> <p>Role (<code>k8s.clusterRole.enabled: false</code>)</p> <ul> <li>Operator limited to its deployment namespace</li> <li>Use for single-tenant deployments or strict namespace isolation</li> <li>Typical for security-sensitive environments</li> </ul> <p>Configure the mode in Helm values:</p> <pre><code># values.yaml\nk8s:\n  clusterRole:\n    enabled: false  # Restrict to operator namespace only\n</code></pre>","tags":["security","rbac","secrets","guide"]},{"location":"security/#user-rbac-for-test-creators","title":"User RBAC for Test Creators","text":"<p>Users who create and manage LocustTest CRs need different permissions than the operator itself. Here are minimal RBAC examples:</p> <p>Test Creator Role (create and manage performance tests):</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: locusttest-creator\n  namespace: performance-testing\nrules:\n  # Create and manage LocustTest CRs\n  - apiGroups: [\"locust.io\"]\n    resources: [\"locusttests\"]\n    verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"]\n  # Create ConfigMaps for test files\n  - apiGroups: [\"\"]\n    resources: [\"configmaps\"]\n    verbs: [\"get\", \"list\", \"create\", \"update\", \"delete\"]\n  # View pods for debugging\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"list\"]\n  # View events for status monitoring\n  - apiGroups: [\"\"]\n    resources: [\"events\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: locusttest-creator-binding\n  namespace: performance-testing\nsubjects:\n  - kind: User\n    name: jane.doe\n    apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: locusttest-creator\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> <p>Test Viewer Role (read-only access for monitoring):</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: locusttest-viewer\n  namespace: performance-testing\nrules:\n  # View LocustTest CRs\n  - apiGroups: [\"locust.io\"]\n    resources: [\"locusttests\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n  # View pods and logs\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\", \"events\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n</code></pre> <p>Users with this role can monitor test status and view logs but cannot create or modify tests.</p>","tags":["security","rbac","secrets","guide"]},{"location":"security/#secret-management","title":"Secret Management","text":"","tags":["security","rbac","secrets","guide"]},{"location":"security/#injecting-secrets-into-tests","title":"Injecting Secrets into Tests","text":"<p>The operator provides three approaches for injecting secrets into Locust test pods:</p> Method Best For Configuration Secret environment variables (<code>env.secretRefs</code>) API keys, tokens, passwords Mounts all keys from a Secret as environment variables Secret file mounts (<code>env.secretMounts</code>) Certificates, key files, config files Mounts Secret keys as files in the container filesystem Individual secret references (<code>env.variables[].valueFrom.secretKeyRef</code>) Specific keys from a secret Fine-grained control over which keys to inject <p>Quick Example (secret environment variables):</p> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nspec:\n  image: locustio/locust:2.20.0\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://api.example.com\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 5\n  env:\n    secretRefs:\n      - name: api-credentials  # All keys become env vars\n</code></pre> <p>See Advanced Topics - Environment Variables for detailed examples of all three approaches.</p>","tags":["security","rbac","secrets","guide"]},{"location":"security/#secret-rotation","title":"Secret Rotation","text":"<p>Because tests are immutable, running tests continue to use the secret values they started with. Secret rotation requires recreating the test.</p> <p>Rotation Process:</p> <ol> <li> <p>Update the Secret in Kubernetes with new credentials:    <pre><code>kubectl create secret generic api-credentials \\\n  --from-literal=API_TOKEN=new-token-value \\\n  --dry-run=client -o yaml | kubectl apply -f -\n</code></pre></p> </li> <li> <p>Delete the LocustTest CR:    <pre><code>kubectl delete locusttest my-test\n</code></pre></p> </li> <li> <p>Recreate the LocustTest CR \u2014 new pods pick up updated secret values:    <pre><code>kubectl apply -f locusttest.yaml\n</code></pre></p> </li> </ol> <p>Scheduled Rotation</p> <p>For automated secret rotation, integrate with external secrets management tools (see next section) that synchronize secrets on a schedule.</p>","tags":["security","rbac","secrets","guide"]},{"location":"security/#external-secrets-integration","title":"External Secrets Integration","text":"<p>The operator works seamlessly with External Secrets Operator for automatic secret synchronization from external secrets managers (AWS Secrets Manager, HashiCorp Vault, Google Secret Manager, Azure Key Vault, etc.).</p> <p>Example (AWS Secrets Manager integration):</p> <pre><code># ExternalSecret syncs from AWS Secrets Manager to a K8s Secret\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: load-test-credentials\n  namespace: performance-testing\nspec:\n  refreshInterval: 1h  # Sync every hour\n  secretStoreRef:\n    name: aws-secretsmanager\n    kind: ClusterSecretStore\n  target:\n    name: load-test-credentials  # K8s Secret name\n  data:\n    - secretKey: API_TOKEN\n      remoteRef:\n        key: /perf-testing/api-token\n    - secretKey: DB_PASSWORD\n      remoteRef:\n        key: /perf-testing/db-password\n---\n# LocustTest references the synced Secret\napiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: my-test\nspec:\n  image: locustio/locust:2.20.0\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://api.example.com\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 5\n  env:\n    secretRefs:\n      - name: load-test-credentials  # Uses synced Secret\n</code></pre> <p>Secret Source Agnostic</p> <p>The operator doesn't care how Secrets are created. You can use External Secrets Operator, Sealed Secrets, Vault Agent, manual <code>kubectl create secret</code>, or any other method.</p>","tags":["security","rbac","secrets","guide"]},{"location":"security/#pod-security","title":"Pod Security","text":"","tags":["security","rbac","secrets","guide"]},{"location":"security/#operator-pod-security","title":"Operator Pod Security","text":"<p>The operator runs with a hardened security context by default, meeting Kubernetes Pod Security Standards \"restricted\" profile:</p> <pre><code># From values.yaml (default configuration)\nsecurityContext:\n  runAsNonRoot: true\n  runAsUser: 65532  # Non-root user\n  readOnlyRootFilesystem: true\n  allowPrivilegeEscalation: false\n  capabilities:\n    drop:\n      - ALL\n  seccompProfile:\n    type: RuntimeDefault\n</code></pre> <p>These settings are enabled by default in the Helm chart. No additional configuration is required.</p>","tags":["security","rbac","secrets","guide"]},{"location":"security/#test-pod-security","title":"Test Pod Security","text":"<p>Test pods run the user-provided Locust image. Security depends on the image you use.</p> <p>Recommendations:</p> <ul> <li>Use the official <code>locustio/locust</code> image or build a hardened variant</li> <li>Avoid running test containers as root</li> <li>Set resource limits to prevent resource exhaustion:   <pre><code>master:\n  resources:\n    limits:\n      cpu: 2000m\n      memory: 2Gi\nworker:\n  resources:\n    limits:\n      cpu: 1000m\n      memory: 1Gi\n</code></pre></li> </ul> <p>Test pods inherit the default security context from Helm values (<code>locustPods.securityContext</code>). Override per-test if needed.</p>","tags":["security","rbac","secrets","guide"]},{"location":"security/#network-security","title":"Network Security","text":"","tags":["security","rbac","secrets","guide"]},{"location":"security/#master-worker-communication","title":"Master-Worker Communication","text":"<p>Master and worker pods communicate internally within the cluster:</p> <ul> <li>Port 5557: Master listens for worker connections (internal only)</li> <li>Port 8089: Web UI on master pod (use port-forward for access)</li> </ul> <p>For production use:</p> <ul> <li>Do not expose port 8089 externally \u2014 use <code>kubectl port-forward</code> for temporary access</li> <li>If using NetworkPolicies, ensure master and worker pods can communicate</li> </ul>","tags":["security","rbac","secrets","guide"]},{"location":"security/#networkpolicy-example","title":"NetworkPolicy Example","text":"<p>Restrict pod communication to within the same test:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: locust-internal\n  namespace: performance-testing\nspec:\n  podSelector:\n    matchLabels:\n      performance-test-name: my-test\n  policyTypes:\n    - Ingress\n    - Egress\n  ingress:\n    # Allow communication between pods in the same test\n    - from:\n        - podSelector:\n            matchLabels:\n              performance-test-name: my-test\n      ports:\n        - port: 5557  # Worker -&gt; Master\n        - port: 8089  # Web UI (optional)\n  egress:\n    # Allow all egress (tests need to reach target systems)\n    - {}\n</code></pre> <p>Egress Requirements</p> <p>Test pods need egress access to reach the target system under test. The example above allows unrestricted egress. Restrict further if required by your security policies.</p>","tags":["security","rbac","secrets","guide"]},{"location":"security/#service-mesh-compatibility","title":"Service Mesh Compatibility","text":"<p>The operator is compatible with service mesh solutions (Istio, Linkerd). However:</p> <ul> <li>Master-worker communication on port 5557 must work within the mesh</li> <li>Ensure sidecar injection doesn't break pod startup (adjust readiness probes if needed)</li> <li>Test traffic to external targets may require egress configuration in the mesh</li> </ul>","tags":["security","rbac","secrets","guide"]},{"location":"security/#image-security","title":"Image Security","text":"","tags":["security","rbac","secrets","guide"]},{"location":"security/#using-private-registries","title":"Using Private Registries","text":"<p>If Locust images are in a private registry, configure image pull secrets:</p> <p>Helm Configuration:</p> <pre><code># values.yaml\nimage:\n  pullSecrets:\n    - name: my-registry-secret\n</code></pre> <p>Create the pull secret:</p> <pre><code>kubectl create secret docker-registry my-registry-secret \\\n  --docker-server=registry.example.com \\\n  --docker-username=user \\\n  --docker-password=pass \\\n  --docker-email=user@example.com \\\n  -n performance-testing\n</code></pre>","tags":["security","rbac","secrets","guide"]},{"location":"security/#image-scanning","title":"Image Scanning","text":"<p>Scan Locust images for vulnerabilities before use:</p> <pre><code># Example with Trivy\ntrivy image locustio/locust:2.20.0\n</code></pre> <p>Build custom hardened images if the official image doesn't meet security requirements.</p>","tags":["security","rbac","secrets","guide"]},{"location":"security/#audit-and-compliance","title":"Audit and Compliance","text":"","tags":["security","rbac","secrets","guide"]},{"location":"security/#operator-audit-logging","title":"Operator Audit Logging","text":"<p>Kubernetes audit logs capture all operator actions. Enable audit logging at the cluster level to track:</p> <ul> <li>LocustTest CR creation/deletion</li> <li>Job creation by the operator</li> <li>Secret access attempts</li> </ul>","tags":["security","rbac","secrets","guide"]},{"location":"security/#compliance-considerations","title":"Compliance Considerations","text":"<ul> <li>PCI-DSS: Ensure Secrets are encrypted at rest (etcd encryption)</li> <li>SOC 2: Log all operator actions via audit logs</li> <li>GDPR: Avoid storing personal data in LocustTest CRs or test results</li> </ul>","tags":["security","rbac","secrets","guide"]},{"location":"security/#additional-resources","title":"Additional Resources","text":"<ul> <li>Getting Started \u2014 Initial setup and first test</li> <li>How-To Guides \u2014 Environment variables, volumes, resource management</li> <li>API Reference \u2014 Complete CR specification</li> <li>Kubernetes RBAC Documentation</li> <li>External Secrets Operator</li> </ul>","tags":["security","rbac","secrets","guide"]},{"location":"getting_started/","title":"Quick Start (5 minutes)","text":"<p>Get your first distributed load test running on Kubernetes.</p>","tags":["quickstart","tutorial","getting started"]},{"location":"getting_started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes cluster (any: Minikube, Kind, GKE, EKS, AKS)</li> <li>kubectl configured</li> <li>Helm 3 installed</li> </ul>","tags":["quickstart","tutorial","getting started"]},{"location":"getting_started/#1-install-the-operator","title":"1. Install the operator","text":"<pre><code># Add the Helm repository\nhelm repo add locust-k8s-operator https://abdelrhmanhamouda.github.io/locust-k8s-operator/\nhelm repo update\n\n# Install the operator into a dedicated namespace\nhelm install locust-operator locust-k8s-operator/locust-k8s-operator \\\n  --namespace locust-system \\\n  --create-namespace\n</code></pre> <p>Installs the operator into a dedicated namespace. Takes ~30 seconds.</p>","tags":["quickstart","tutorial","getting started"]},{"location":"getting_started/#2-create-a-test-script","title":"2. Create a test script","text":"<pre><code>cat &gt; demo_test.py &lt;&lt; 'EOF'\nfrom locust import HttpUser, task\n\nclass DemoUser(HttpUser):\n    @task  # Define a task that users will execute\n    def get_homepage(self):\n        # Simple test that requests the homepage repeatedly\n        self.client.get(\"/\")\nEOF\n</code></pre> <p>Simple test that requests the homepage repeatedly.</p>","tags":["quickstart","tutorial","getting started"]},{"location":"getting_started/#3-deploy-the-test-as-configmap","title":"3. Deploy the test as ConfigMap","text":"<pre><code># Make your test script available to Kubernetes pods\nkubectl create configmap demo-test --from-file=demo_test.py\n</code></pre> <p>Makes your test script available to Kubernetes pods.</p>","tags":["quickstart","tutorial","getting started"]},{"location":"getting_started/#4-run-the-load-test","title":"4. Run the load test","text":"<pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: demo\nspec:\n  image: locustio/locust:2.20.0  # Use a specific version\n  testFiles:\n    configMapRef: demo-test  # Reference the test script ConfigMap\n  master:\n    # Run with 10 users, spawning 2 per second, for 1 minute\n    command: \"--locustfile /lotest/src/demo_test.py --host https://httpbin.org --users 10 --spawn-rate 2 --run-time 1m\"\n  worker:\n    command: \"--locustfile /lotest/src/demo_test.py\"\n    replicas: 2  # Distribute load across 2 worker pods\nEOF\n</code></pre> <p>Creates a distributed test with 10 simulated users across 2 worker pods.</p>","tags":["quickstart","tutorial","getting started"]},{"location":"getting_started/#5-watch-the-results","title":"5. Watch the results","text":"<pre><code># Monitor test progress\nkubectl get locusttest demo -w\n</code></pre> <p>You'll see output like this:</p> <pre><code>NAME   PHASE     EXPECTED   CONNECTED   AGE\ndemo   Pending   2          0           2s\ndemo   Running   2          2           15s\ndemo   Succeeded 2          2           75s\n</code></pre> <p>To access the Locust web UI and view real-time statistics:</p> <pre><code># Forward the master pod's port to your local machine\nkubectl port-forward job/demo-master 8089:8089\n</code></pre> <p>Then open http://localhost:8089 in your browser to see request statistics, response times, and charts.</p>","tags":["quickstart","tutorial","getting started"]},{"location":"getting_started/#cleanup","title":"Cleanup","text":"<pre><code>kubectl delete locusttest demo\nkubectl delete configmap demo-test\n</code></pre>","tags":["quickstart","tutorial","getting started"]},{"location":"getting_started/#whats-next","title":"What's Next?","text":"<ul> <li>Your First Load Test - Build a realistic test with multiple scenarios (10 minutes)</li> <li>CI/CD Integration - Automate tests in your pipeline (15 minutes)</li> <li>API Reference - Complete LocustTest CR specification</li> </ul>","tags":["quickstart","tutorial","getting started"]},{"location":"how-to-guides/","title":"How-To Guides","text":"<p>Task-oriented recipes for specific goals. Each guide walks you through a complete solution from start to finish.</p>","tags":["how-to","guides","recipes"]},{"location":"how-to-guides/#configuration","title":"Configuration","text":"<p>Set up and configure your load tests:</p> <ul> <li>Configure resource limits and requests \u2014 Control CPU and memory allocation for master and worker pods</li> <li>Use a private image registry \u2014 Pull Locust images from private registries with authentication</li> <li>Mount volumes to test pods \u2014 Attach data, certificates, or configuration files from various sources</li> <li>Configure Kafka and AWS MSK integration \u2014 Set up authenticated Kafka access for event-driven testing</li> <li>Configure automatic cleanup with TTL \u2014 Automatically remove finished jobs and pods after tests complete</li> </ul>","tags":["how-to","guides","recipes"]},{"location":"how-to-guides/#observability","title":"Observability","text":"<p>Monitor and measure test performance:</p> <ul> <li>Configure OpenTelemetry integration \u2014 Export traces and metrics using native OTel support</li> <li>Monitor test status and health \u2014 Track test progress, phase transitions, conditions, and pod health</li> <li>Set up Prometheus monitoring \u2014 Collect and visualize test metrics with Prometheus and Grafana</li> </ul>","tags":["how-to","guides","recipes"]},{"location":"how-to-guides/#scaling","title":"Scaling","text":"<p>Scale tests for high load and optimize resource placement:</p> <ul> <li>Scale worker replicas for high load \u2014 Size worker replicas based on simulated user count</li> <li>Use node affinity for dedicated test nodes \u2014 Target specific nodes using labels and affinity rules</li> <li>Configure tolerations for tainted nodes \u2014 Schedule pods on nodes with taints</li> <li>Use node selector for simple node targeting \u2014 Target nodes using simple label matching</li> </ul>","tags":["how-to","guides","recipes"]},{"location":"how-to-guides/#security","title":"Security","text":"<p>Secure your tests and manage credentials:</p> <ul> <li>Inject secrets into test pods \u2014 Use Kubernetes secrets for API keys, tokens, and credentials</li> <li>Configure pod security settings \u2014 Set security contexts, RBAC, and network policies for test pods</li> <li>Secure container registry access \u2014 Authenticate with private container registries</li> </ul>","tags":["how-to","guides","recipes"]},{"location":"how-to-guides/configuration/configure-kafka/","title":"Configure Kafka and AWS MSK integration","text":"<p>Configure Locust pods to connect to authenticated Kafka clusters, including AWS MSK, for performance testing of event-driven architectures.</p>","tags":["configuration","kafka","aws msk","integration"]},{"location":"how-to-guides/configuration/configure-kafka/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kafka cluster or AWS MSK cluster accessible from Kubernetes</li> <li>Kafka credentials (username/password for SASL authentication)</li> <li>Basic understanding of Kafka security protocols</li> </ul>","tags":["configuration","kafka","aws msk","integration"]},{"location":"how-to-guides/configuration/configure-kafka/#two-level-configuration-model","title":"Two-level configuration model","text":"<p>The operator supports two approaches to Kafka configuration:</p> <p>1. Operator-level (centralized): Configure Kafka credentials once during operator installation. The operator automatically injects these as environment variables into all Locust pods. Test creators don't need to manage credentials.</p> <p>2. Per-test (override): Specify Kafka configuration in individual LocustTest CRs using <code>spec.env.variables</code>. This overrides operator-level configuration for specific tests.</p> <p>Priority: Per-test configuration overrides operator-level defaults.</p>","tags":["configuration","kafka","aws msk","integration"]},{"location":"how-to-guides/configuration/configure-kafka/#configure-at-operator-level-helm","title":"Configure at operator level (Helm)","text":"<p>Set Kafka credentials during operator installation:</p> <pre><code># values.yaml\nkafka:\n  enabled: true\n  bootstrapServers: \"kafka-broker1:9092,kafka-broker2:9092\"\n  security:\n    enabled: true\n    protocol: \"SASL_SSL\"        # PLAINTEXT, SASL_PLAINTEXT, SASL_SSL, or SSL\n    saslMechanism: \"SCRAM-SHA-512\"  # PLAINTEXT, SCRAM-SHA-256, or SCRAM-SHA-512\n  credentials:\n    secretName: \"kafka-credentials\"    # Name of K8s Secret containing credentials\n    usernameKey: \"username\"            # Key in Secret for username (default: \"username\")\n    passwordKey: \"password\"            # Key in Secret for password (default: \"password\")\n</code></pre> <p>Install or upgrade the operator:</p> <pre><code>helm upgrade --install locust-operator locust-k8s-operator/locust-k8s-operator \\\n  --namespace locust-system \\\n  -f values.yaml\n</code></pre> <p>All Locust pods will automatically receive Kafka environment variables.</p> <p>For AWS MSK:</p> <pre><code># values.yaml for AWS MSK with IAM authentication\nkafka:\n  enabled: true\n  bootstrapServers: \"b-1.mycluster.kafka.us-east-1.amazonaws.com:9096\"\n  security:\n    enabled: true\n    protocol: \"SASL_SSL\"\n    saslMechanism: \"SCRAM-SHA-512\"  # Or AWS_MSK_IAM for IAM auth\n  credentials:\n    secretName: \"msk-credentials\"      # Name of K8s Secret containing MSK credentials\n    usernameKey: \"username\"            # Key in Secret for username (default: \"username\")\n    passwordKey: \"password\"            # Key in Secret for password (default: \"password\")\n</code></pre>","tags":["configuration","kafka","aws msk","integration"]},{"location":"how-to-guides/configuration/configure-kafka/#configure-per-test-override","title":"Configure per-test (override)","text":"<p>Override operator-level Kafka configuration for specific tests:</p> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: kafka-test\nspec:\n  image: locustio/locust:2.20.0\n  testFiles:\n    configMapRef: kafka-test-script\n  master:\n    command: \"--locustfile /lotest/src/kafka_test.py --host https://api.example.com\"\n  worker:\n    command: \"--locustfile /lotest/src/kafka_test.py\"\n    replicas: 5\n  env:\n    variables:\n      - name: KAFKA_BOOTSTRAP_SERVERS\n        value: \"different-kafka:9092\"  # Override operator setting\n      - name: KAFKA_SECURITY_ENABLED\n        value: \"true\"\n      - name: KAFKA_SECURITY_PROTOCOL_CONFIG\n        value: \"SASL_SSL\"\n      - name: KAFKA_SASL_MECHANISM\n        value: \"SCRAM-SHA-256\"\n      - name: KAFKA_USERNAME\n        value: \"test-specific-user\"\n      - name: KAFKA_PASSWORD\n        valueFrom:  # Reference secret for password\n          secretKeyRef:\n            name: kafka-test-creds\n            key: password\n</code></pre> <p>Create the secret:</p> <pre><code>kubectl create secret generic kafka-test-creds \\\n  --from-literal=password='my-kafka-password'\n</code></pre> <p>Apply the test:</p> <pre><code>kubectl apply -f locusttest-kafka.yaml\n</code></pre>","tags":["configuration","kafka","aws msk","integration"]},{"location":"how-to-guides/configuration/configure-kafka/#available-environment-variables","title":"Available environment variables","text":"<p>When Kafka configuration is enabled, these environment variables are available in Locust pods:</p> Variable Description Example values <code>KAFKA_BOOTSTRAP_SERVERS</code> Kafka broker addresses <code>broker1:9092,broker2:9092</code> <code>KAFKA_SECURITY_ENABLED</code> Whether security is enabled <code>true</code>, <code>false</code> <code>KAFKA_SECURITY_PROTOCOL_CONFIG</code> Security protocol <code>PLAINTEXT</code>, <code>SASL_PLAINTEXT</code>, <code>SASL_SSL</code>, <code>SSL</code> <code>KAFKA_SASL_MECHANISM</code> Authentication mechanism <code>PLAINTEXT</code>, <code>SCRAM-SHA-256</code>, <code>SCRAM-SHA-512</code> <code>KAFKA_USERNAME</code> Kafka username <code>kafka-user</code> <code>KAFKA_PASSWORD</code> Kafka password <code>kafka-password</code>","tags":["configuration","kafka","aws msk","integration"]},{"location":"how-to-guides/configuration/configure-kafka/#use-in-locust-test-script","title":"Use in Locust test script","text":"<p>Access Kafka environment variables in your test:</p> <pre><code># kafka_test.py\nimport os\nfrom locust import User, task, between\nfrom kafka import KafkaProducer, KafkaConsumer\nimport json\n\nclass KafkaUser(User):\n    wait_time = between(1, 3)\n\n    def on_start(self):\n        \"\"\"Initialize Kafka producer using operator-injected config.\"\"\"\n        bootstrap_servers = os.getenv('KAFKA_BOOTSTRAP_SERVERS').split(',')\n\n        security_enabled = os.getenv('KAFKA_SECURITY_ENABLED', 'false').lower() == 'true'\n\n        if security_enabled:\n            # Use authenticated connection\n            self.producer = KafkaProducer(\n                bootstrap_servers=bootstrap_servers,\n                security_protocol=os.getenv('KAFKA_SECURITY_PROTOCOL_CONFIG', 'SASL_SSL'),\n                sasl_mechanism=os.getenv('KAFKA_SASL_MECHANISM', 'SCRAM-SHA-512'),\n                sasl_plain_username=os.getenv('KAFKA_USERNAME'),\n                sasl_plain_password=os.getenv('KAFKA_PASSWORD'),\n                value_serializer=lambda v: json.dumps(v).encode('utf-8')\n            )\n        else:\n            # Use unauthenticated connection\n            self.producer = KafkaProducer(\n                bootstrap_servers=bootstrap_servers,\n                value_serializer=lambda v: json.dumps(v).encode('utf-8')\n            )\n\n    @task\n    def produce_message(self):\n        \"\"\"Send a message to Kafka.\"\"\"\n        message = {\n            'user_id': 12345,\n            'action': 'view_product',\n            'timestamp': '2026-02-12T10:30:00Z'\n        }\n\n        future = self.producer.send('user-events', value=message)\n        try:\n            record_metadata = future.get(timeout=10)\n            # Track success\n            self.environment.events.request.fire(\n                request_type=\"KAFKA\",\n                name=\"produce_message\",\n                response_time=future._elapsed_ms,\n                response_length=len(str(message)),\n                exception=None,\n                context={}\n            )\n        except Exception as e:\n            # Track failure\n            self.environment.events.request.fire(\n                request_type=\"KAFKA\",\n                name=\"produce_message\",\n                response_time=0,\n                response_length=0,\n                exception=e,\n                context={}\n            )\n</code></pre>","tags":["configuration","kafka","aws msk","integration"]},{"location":"how-to-guides/configuration/configure-kafka/#verify-kafka-configuration","title":"Verify Kafka configuration","text":"<p>Check that environment variables are injected:</p> <pre><code># Get a worker pod name\nWORKER_POD=$(kubectl get pod -l performance-test-pod-name=kafka-test-worker -o jsonpath='{.items[0].metadata.name}')\n\n# Verify Kafka environment variables\nkubectl exec $WORKER_POD -- env | grep KAFKA_\n</code></pre> <p>Expected output:</p> <pre><code>KAFKA_BOOTSTRAP_SERVERS=kafka-broker1:9092,kafka-broker2:9092\nKAFKA_SECURITY_ENABLED=true\nKAFKA_SECURITY_PROTOCOL_CONFIG=SASL_SSL\nKAFKA_SASL_MECHANISM=SCRAM-SHA-512\nKAFKA_USERNAME=kafka-user\nKAFKA_PASSWORD=kafka-password\n</code></pre>","tags":["configuration","kafka","aws msk","integration"]},{"location":"how-to-guides/configuration/configure-kafka/#troubleshoot-connection-issues","title":"Troubleshoot connection issues","text":"<p>Authentication failed:</p> <pre><code>kafka.errors.NoBrokersAvailable: NoBrokersAvailable\n</code></pre> <p>Check credentials:</p> <pre><code>kubectl exec $WORKER_POD -- env | grep KAFKA_USERNAME\nkubectl exec $WORKER_POD -- env | grep KAFKA_PASSWORD\n</code></pre> <p>Verify credentials are correct in your Kafka cluster.</p> <p>Connection timeout:</p> <pre><code>kafka.errors.KafkaTimeoutError: KafkaTimeoutError\n</code></pre> <p>Check network connectivity:</p> <pre><code># Test connection from pod\nkubectl exec $WORKER_POD -- nc -zv kafka-broker1 9092\n</code></pre> <p>Verify:</p> <ul> <li>Bootstrap servers address is correct</li> <li>Network policies allow egress to Kafka</li> <li>Kafka cluster is reachable from Kubernetes</li> </ul> <p>Wrong protocol:</p> <pre><code>kafka.errors.BrokerResponseError: SASL_AUTHENTICATION_FAILED\n</code></pre> <p>Verify <code>KAFKA_SECURITY_PROTOCOL_CONFIG</code> matches your Kafka cluster setup:</p> <pre><code>kubectl exec $WORKER_POD -- env | grep KAFKA_SECURITY_PROTOCOL_CONFIG\n</code></pre>","tags":["configuration","kafka","aws msk","integration"]},{"location":"how-to-guides/configuration/configure-kafka/#whats-next","title":"What's next","text":"<ul> <li>Inject secrets \u2014 Manage Kafka credentials using Kubernetes secrets</li> <li>Scale worker replicas \u2014 Size workers for high Kafka throughput</li> <li>Configure resources \u2014 Ensure pods have enough resources for Kafka connections</li> </ul>","tags":["configuration","kafka","aws msk","integration"]},{"location":"how-to-guides/configuration/configure-resources/","title":"Configure resource limits and requests","text":"<p>Resource configuration ensures your load tests have the resources they need without consuming excessive cluster capacity.</p>","tags":["configuration","resources","performance"]},{"location":"how-to-guides/configuration/configure-resources/#prerequisites","title":"Prerequisites","text":"<ul> <li>Locust Kubernetes Operator installed</li> <li>Basic understanding of Kubernetes resource requests and limits</li> </ul>","tags":["configuration","resources","performance"]},{"location":"how-to-guides/configuration/configure-resources/#set-global-defaults-via-helm","title":"Set global defaults via Helm","text":"<p>Configure default resources for all tests during operator installation:</p> <pre><code># values.yaml\nlocustPods:\n  resources:\n    requests:\n      cpu: \"250m\"        # Guaranteed CPU\n      memory: \"256Mi\"    # Guaranteed memory\n    limits:\n      cpu: \"1000m\"       # Maximum CPU\n      memory: \"512Mi\"    # Maximum memory\n</code></pre> <p>Install or upgrade the operator:</p> <pre><code>helm upgrade --install locust-operator locust-k8s-operator/locust-k8s-operator \\\n  --namespace locust-system \\\n  -f values.yaml\n</code></pre> <p>These defaults apply to all Locust pods unless overridden in individual CRs.</p>","tags":["configuration","resources","performance"]},{"location":"how-to-guides/configuration/configure-resources/#configure-per-test-resources","title":"Configure per-test resources","text":"<p>Override defaults for specific tests using the v2 API. Master and worker pods can have different resource configurations:</p> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: resource-optimized-test\nspec:\n  image: locustio/locust:2.20.0\n  testFiles:\n    configMapRef: my-test\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://api.example.com\"\n    resources:\n      requests:\n        memory: \"256Mi\"    # Master needs less memory\n        cpu: \"100m\"        # Master is not CPU-intensive\n      limits:\n        memory: \"512Mi\"\n        cpu: \"500m\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 10\n    resources:\n      requests:\n        memory: \"512Mi\"    # Workers need more memory for load generation\n        cpu: \"500m\"        # Workers are CPU-intensive\n      limits:\n        memory: \"1Gi\"\n        cpu: \"1000m\"\n</code></pre> <p>Apply the configuration:</p> <pre><code>kubectl apply -f locusttest-resources.yaml\n</code></pre>","tags":["configuration","resources","performance"]},{"location":"how-to-guides/configuration/configure-resources/#disable-cpu-limits-for-performance-tests","title":"Disable CPU limits for performance tests","text":"<p>CPU limits can cause throttling in performance-sensitive tests. Disable them by omitting the CPU limit field:</p> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: no-cpu-limit-test\nspec:\n  image: locustio/locust:2.20.0\n  testFiles:\n    configMapRef: my-test\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://api.example.com\"\n    resources:\n      requests:\n        memory: \"256Mi\"\n        cpu: \"100m\"\n      limits:\n        memory: \"512Mi\"\n        # No CPU limit - allows maximum performance\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 10\n    resources:\n      requests:\n        memory: \"512Mi\"\n        cpu: \"500m\"\n      limits:\n        memory: \"1Gi\"\n        # No CPU limit - workers can use all available CPU\n</code></pre> <p>When to disable CPU limits:</p> <ul> <li>High-throughput performance tests (&gt;5000 RPS)</li> <li>Benchmarking scenarios where you need maximum performance</li> <li>Tests with bursty traffic patterns</li> </ul> <p>Risk: Pods can consume all available CPU on the node, potentially affecting other workloads. Use with node affinity to isolate tests on dedicated nodes.</p>","tags":["configuration","resources","performance"]},{"location":"how-to-guides/configuration/configure-resources/#resource-sizing-guidelines","title":"Resource sizing guidelines","text":"<p>Master pod:</p> <ul> <li>CPU: 100-500m (master coordinates, doesn't generate load)</li> <li>Memory: 256-512Mi (depends on test complexity and UI usage)</li> <li>Usually 1 replica</li> </ul> <p>Worker pod:</p> <ul> <li>CPU: 500-1000m per worker (depends on test script complexity)</li> <li>Memory: 256-512Mi per worker (depends on data handling)</li> <li>Scale workers based on user count (see Scale worker replicas)</li> </ul> <p>Example sizing for 1000 users:</p> <pre><code>master:\n  resources:\n    requests:\n      memory: \"256Mi\"\n      cpu: \"200m\"\n    limits:\n      memory: \"512Mi\"\n      cpu: \"500m\"\n\nworker:\n  replicas: 20  # ~50 users per worker\n  resources:\n    requests:\n      memory: \"512Mi\"\n      cpu: \"500m\"\n    limits:\n      memory: \"1Gi\"\n      # CPU limit omitted for performance\n</code></pre>","tags":["configuration","resources","performance"]},{"location":"how-to-guides/configuration/configure-resources/#verify-resource-configuration","title":"Verify resource configuration","text":"<p>Check actual resource specs on running pods:</p> <pre><code># Get master pod name\nMASTER_POD=$(kubectl get pod -l performance-test-pod-name=resource-optimized-test-master -o jsonpath='{.items[0].metadata.name}')\n\n# Verify resource configuration\nkubectl describe pod $MASTER_POD | grep -A 10 \"Limits:\\|Requests:\"\n</code></pre> <p>Expected output:</p> <pre><code>Limits:\n  memory:  512Mi\nRequests:\n  cpu:     100m\n  memory:  256Mi\n</code></pre>","tags":["configuration","resources","performance"]},{"location":"how-to-guides/configuration/configure-resources/#monitor-resource-usage","title":"Monitor resource usage","text":"<p>Check actual resource consumption:</p> <pre><code># Real-time resource usage\nkubectl top pod -l performance-test-name=resource-optimized-test\n\n# Watch resource usage during test\nkubectl top pod -l performance-test-name=resource-optimized-test --watch\n</code></pre> <p>If pods consistently hit memory limits, they'll be OOMKilled. If they hit CPU limits, they'll be throttled (slower performance).</p>","tags":["configuration","resources","performance"]},{"location":"how-to-guides/configuration/configure-resources/#whats-next","title":"What's next","text":"<ul> <li>Scale worker replicas \u2014 Calculate worker count for high-load scenarios</li> <li>Use node affinity \u2014 Run resource-intensive tests on dedicated nodes</li> <li>Configure tolerations \u2014 Schedule tests on high-performance node pools</li> </ul>","tags":["configuration","resources","performance"]},{"location":"how-to-guides/configuration/configure-ttl/","title":"Configure automatic cleanup with TTL","text":"<p>Automatically clean up finished master and worker jobs and their pods after tests complete using Kubernetes TTL (time-to-live).</p>","tags":["configuration","cleanup","ttl","automation"]},{"location":"how-to-guides/configuration/configure-ttl/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes 1.12+ (TTL controller support)</li> <li>Locust Kubernetes Operator installed</li> </ul>","tags":["configuration","cleanup","ttl","automation"]},{"location":"how-to-guides/configuration/configure-ttl/#what-gets-cleaned-up","title":"What gets cleaned up","text":"<p>When TTL is configured:</p> <ul> <li>\u2713 Cleaned up: Master and worker Jobs and their Pods</li> <li>\u2717 Kept: LocustTest CR, ConfigMaps, Secrets, Services</li> </ul> <p>This allows you to review test results (via <code>kubectl get locusttest</code>) while automatically removing resource-consuming pods.</p>","tags":["configuration","cleanup","ttl","automation"]},{"location":"how-to-guides/configuration/configure-ttl/#set-ttl-via-helm-values","title":"Set TTL via Helm values","text":"<p>Configure TTL for all tests during operator installation:</p> <pre><code># values.yaml\nlocustPods:\n  ttlSecondsAfterFinished: 3600  # Clean up after 1 hour\n</code></pre> <p>Install or upgrade the operator:</p> <pre><code>helm upgrade --install locust-operator locust-k8s-operator/locust-k8s-operator \\\n  --namespace locust-system \\\n  -f values.yaml\n</code></pre> <p>All LocustTest resources will inherit this TTL value.</p>","tags":["configuration","cleanup","ttl","automation"]},{"location":"how-to-guides/configuration/configure-ttl/#set-ttl-via-cli","title":"Set TTL via CLI","text":"<p>Override TTL at installation time:</p> <pre><code>helm install locust-operator locust-k8s-operator/locust-k8s-operator \\\n  --namespace locust-system \\\n  --set locustPods.ttlSecondsAfterFinished=7200  # 2 hours\n</code></pre>","tags":["configuration","cleanup","ttl","automation"]},{"location":"how-to-guides/configuration/configure-ttl/#common-ttl-values","title":"Common TTL values","text":"Value Duration Use case <code>0</code> Immediate CI/CD where you collect results before cleanup <code>300</code> 5 minutes Quick tests where results are exported immediately <code>3600</code> 1 hour Standard tests with manual result review <code>7200</code> 2 hours Long tests with delayed result analysis <code>86400</code> 24 hours Tests requiring extensive post-analysis <code>\"\"</code> (empty) Never Development or when using external cleanup","tags":["configuration","cleanup","ttl","automation"]},{"location":"how-to-guides/configuration/configure-ttl/#disable-ttl","title":"Disable TTL","text":"<p>To disable automatic cleanup:</p> <pre><code># values.yaml\nlocustPods:\n  ttlSecondsAfterFinished: \"\"  # Empty string disables TTL\n</code></pre> <p>Or omit the field entirely:</p> <pre><code># values.yaml\nlocustPods:\n  # ttlSecondsAfterFinished not set - no TTL\n</code></pre> <p>Without TTL, jobs and pods persist until manually deleted.</p>","tags":["configuration","cleanup","ttl","automation"]},{"location":"how-to-guides/configuration/configure-ttl/#verify-ttl-configuration","title":"Verify TTL configuration","text":"<p>Check that TTL is set on created jobs:</p> <pre><code># Run a test\nkubectl apply -f locusttest.yaml\n\n# Check master job TTL\nkubectl get job -l performance-test-pod-name=my-test-master -o yaml | grep ttlSecondsAfterFinished\n</code></pre> <p>Expected output:</p> <pre><code>ttlSecondsAfterFinished: 3600\n</code></pre>","tags":["configuration","cleanup","ttl","automation"]},{"location":"how-to-guides/configuration/configure-ttl/#watch-automatic-cleanup","title":"Watch automatic cleanup","text":"<p>Monitor cleanup in action:</p> <pre><code># List all jobs with timestamps\nkubectl get jobs -o wide --watch\n\n# After TTL expires, jobs transition to deleted\n</code></pre> <p>Verify cleanup occurred:</p> <pre><code># Jobs should be gone after TTL\nkubectl get jobs -l performance-test-name=my-test\n\n# Pods should also be gone\nkubectl get pods -l performance-test-name=my-test\n\n# But CR still exists\nkubectl get locusttest my-test\n</code></pre>","tags":["configuration","cleanup","ttl","automation"]},{"location":"how-to-guides/configuration/configure-ttl/#example-cicd-with-immediate-cleanup","title":"Example: CI/CD with immediate cleanup","text":"<p>For CI/CD pipelines where you collect results before cleanup:</p> <pre><code># values.yaml\nlocustPods:\n  ttlSecondsAfterFinished: 0  # Clean up immediately after completion\n</code></pre> <p>In your pipeline:</p> <pre><code># Run the test\nkubectl apply -f locusttest.yaml\n\n# Wait for completion\nkubectl wait --for=jsonpath='{.status.phase}'=Succeeded \\\n  locusttest/ci-test --timeout=10m\n\n# Collect results BEFORE cleanup happens\nkubectl logs job/ci-test-master &gt; results.log\n\n# Jobs and pods will be cleaned up within seconds\n# CR persists for historical tracking\n</code></pre>","tags":["configuration","cleanup","ttl","automation"]},{"location":"how-to-guides/configuration/configure-ttl/#example-development-with-manual-cleanup","title":"Example: Development with manual cleanup","text":"<p>During development, disable TTL to inspect pods:</p> <pre><code># values.yaml\nlocustPods:\n  ttlSecondsAfterFinished: \"\"  # Disable automatic cleanup\n</code></pre> <p>Clean up manually when done:</p> <pre><code># Delete just the test\nkubectl delete locusttest my-test\n\n# Or delete all test resources\nkubectl delete locusttest --all\n</code></pre>","tags":["configuration","cleanup","ttl","automation"]},{"location":"how-to-guides/configuration/configure-ttl/#backward-compatibility","title":"Backward compatibility","text":"<p>The operator maintains backward compatibility with the old configuration path:</p> <pre><code># Old path (still supported)\nconfig:\n  loadGenerationJobs:\n    ttlSecondsAfterFinished: 3600\n\n# New path (recommended)\nlocustPods:\n  ttlSecondsAfterFinished: 3600\n</code></pre> <p>Helper functions in the Helm chart ensure both paths work. Use the new path for future configurations.</p>","tags":["configuration","cleanup","ttl","automation"]},{"location":"how-to-guides/configuration/configure-ttl/#how-ttl-works","title":"How TTL works","text":"<p>Kubernetes TTL controller monitors finished jobs:</p> <ol> <li>Test completes (phase: Succeeded or Failed)</li> <li>Job transitions to finished state</li> <li>TTL countdown starts</li> <li>After TTL seconds, controller deletes job</li> <li>Cascading deletion removes dependent pods</li> </ol> <p>Important: TTL countdown starts when the job finishes, not when it starts.</p>","tags":["configuration","cleanup","ttl","automation"]},{"location":"how-to-guides/configuration/configure-ttl/#whats-next","title":"What's next","text":"<ul> <li>Scale worker replicas \u2014 Size tests appropriately to minimize wasted resources</li> <li>Configure resources \u2014 Set resource limits to prevent cluster exhaustion</li> <li>Configure OpenTelemetry \u2014 Export metrics before cleanup</li> </ul>","tags":["configuration","cleanup","ttl","automation"]},{"location":"how-to-guides/configuration/mount-volumes/","title":"Mount volumes to test pods","text":"<p>Mount test data, TLS certificates, or configuration files into Locust pods from PersistentVolumes, ConfigMaps, Secrets, or EmptyDir.</p> <p>v2 API only</p> <p>Volume mounting is only available in the v2 API.</p>","tags":["configuration","volumes","storage"]},{"location":"how-to-guides/configuration/mount-volumes/#prerequisites","title":"Prerequisites","text":"<ul> <li>Locust Kubernetes Operator v2.0+ installed</li> <li>Volume source created (PVC, ConfigMap, or Secret)</li> </ul>","tags":["configuration","volumes","storage"]},{"location":"how-to-guides/configuration/mount-volumes/#mount-a-persistentvolumeclaim","title":"Mount a PersistentVolumeClaim","text":"<p>Use a PVC to share large test data files across pods:</p> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: pvc-volume-test\nspec:\n  image: locustio/locust:2.20.0\n  testFiles:\n    configMapRef: my-test\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://api.example.com\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 3\n  volumes:  # Define the volume\n    - name: test-data\n      persistentVolumeClaim:\n        claimName: test-data-pvc  # Must exist in same namespace\n  volumeMounts:  # Mount into pods\n    - name: test-data\n      mountPath: /data  # Access files at /data in containers\n      target: both      # Mount to both master and worker pods\n</code></pre> <p>Create the PVC first:</p> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: test-data-pvc\nspec:\n  accessModes:\n    - ReadOnlyMany  # Multiple pods can read\n  resources:\n    requests:\n      storage: 10Gi\n</code></pre> <p>Apply both:</p> <pre><code>kubectl apply -f pvc.yaml\nkubectl apply -f locusttest-pvc.yaml\n</code></pre>","tags":["configuration","volumes","storage"]},{"location":"how-to-guides/configuration/mount-volumes/#mount-a-configmap","title":"Mount a ConfigMap","text":"<p>Mount configuration files from a ConfigMap:</p> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: configmap-volume-test\nspec:\n  image: locustio/locust:2.20.0\n  testFiles:\n    configMapRef: my-test\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://api.example.com\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 3\n  volumes:\n    - name: config-files\n      configMap:\n        name: app-config  # ConfigMap containing config files\n  volumeMounts:\n    - name: config-files\n      mountPath: /config  # Files appear at /config/key1, /config/key2, etc.\n      target: both\n</code></pre> <p>Create the ConfigMap:</p> <pre><code>kubectl create configmap app-config \\\n  --from-file=config.json \\\n  --from-file=settings.yaml\n</code></pre> <p>Your test script can read files:</p> <pre><code>import json\n\n# Read config from mounted volume\nwith open('/config/config.json') as f:\n    config = json.load(f)\n</code></pre>","tags":["configuration","volumes","storage"]},{"location":"how-to-guides/configuration/mount-volumes/#mount-a-secret","title":"Mount a Secret","text":"<p>Mount TLS certificates or API keys as files:</p> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: secret-volume-test\nspec:\n  image: locustio/locust:2.20.0\n  testFiles:\n    configMapRef: my-test\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://api.example.com\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 3\n  volumes:\n    - name: tls-certs\n      secret:\n        secretName: tls-secret\n  volumeMounts:\n    - name: tls-certs\n      mountPath: /certs\n      readOnly: true  # Best practice for secrets\n      target: both\n</code></pre> <p>Create the secret:</p> <pre><code>kubectl create secret generic tls-secret \\\n  --from-file=tls.crt=cert.pem \\\n  --from-file=tls.key=key.pem\n</code></pre> <p>Use in test:</p> <pre><code>import requests\n\n# Use client certificates\nresponse = requests.get(\n    'https://api.example.com',\n    cert=('/certs/tls.crt', '/certs/tls.key')\n)\n</code></pre>","tags":["configuration","volumes","storage"]},{"location":"how-to-guides/configuration/mount-volumes/#use-emptydir-for-temporary-storage","title":"Use EmptyDir for temporary storage","text":"<p>Create temporary storage shared between containers:</p> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: emptydir-volume-test\nspec:\n  image: locustio/locust:2.20.0\n  testFiles:\n    configMapRef: my-test\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://api.example.com\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 3\n  volumes:\n    - name: cache\n      emptyDir: {}  # Created when pod starts, deleted when pod stops\n  volumeMounts:\n    - name: cache\n      mountPath: /tmp/cache\n      target: worker  # Only workers need cache\n</code></pre> <p>Use cases for EmptyDir:</p> <ul> <li>Temporary file processing</li> <li>Download cache</li> <li>Scratch space for generated data</li> </ul> <p>Note: EmptyDir is pod-specific. Each worker pod has its own EmptyDir, not shared across pods.</p>","tags":["configuration","volumes","storage"]},{"location":"how-to-guides/configuration/mount-volumes/#target-specific-pod-types","title":"Target specific pod types","text":"<p>Control which pods receive the volume mount:</p> <pre><code>volumeMounts:\n  - name: test-data\n    mountPath: /data\n    target: master  # Options: master, worker, both (default)\n</code></pre> Target Master Worker Use case <code>master</code> \u2713 \u2717 Master-only processing or UI data <code>worker</code> \u2717 \u2713 Worker-specific data or libraries <code>both</code> (default) \u2713 \u2713 Shared test data or configuration <p>Example with different targets:</p> <pre><code>volumes:\n  - name: shared-data\n    persistentVolumeClaim:\n      claimName: shared-pvc\n  - name: worker-cache\n    emptyDir: {}\n\nvolumeMounts:\n  - name: shared-data\n    mountPath: /data\n    target: both         # Both master and workers read test data\n  - name: worker-cache\n    mountPath: /cache\n    target: worker       # Only workers need cache space\n</code></pre>","tags":["configuration","volumes","storage"]},{"location":"how-to-guides/configuration/mount-volumes/#reserved-mount-paths","title":"Reserved mount paths","text":"<p>The following paths are reserved and cannot be used for volume mounts:</p> Path Purpose Customizable <code>/lotest/src/</code> Test script mount point Yes, via <code>testFiles.srcMountPath</code> <code>/opt/locust/lib</code> Library mount point Yes, via <code>testFiles.libMountPath</code> <p>If you customize these paths, the custom paths become reserved instead.</p>","tags":["configuration","volumes","storage"]},{"location":"how-to-guides/configuration/mount-volumes/#reserved-volume-names","title":"Reserved volume names","text":"<p>The following volume name patterns are reserved:</p> Pattern Purpose <code>&lt;crName&gt;-master</code> Master ConfigMap volume <code>&lt;crName&gt;-worker</code> Worker ConfigMap volume <code>locust-lib</code> Library ConfigMap volume <code>secret-*</code> Secret volumes from <code>env.secretMounts</code> <p>Choose different names for your volumes to avoid conflicts.</p>","tags":["configuration","volumes","storage"]},{"location":"how-to-guides/configuration/mount-volumes/#verify-volume-mount","title":"Verify volume mount","text":"<p>Check that volumes are mounted correctly:</p> <pre><code># Get a worker pod name\nWORKER_POD=$(kubectl get pod -l performance-test-pod-name=my-test-worker -o jsonpath='{.items[0].metadata.name}')\n\n# Check mount exists\nkubectl exec $WORKER_POD -- ls -la /data\n\n# Verify file contents\nkubectl exec $WORKER_POD -- cat /data/test-file.json\n</code></pre>","tags":["configuration","volumes","storage"]},{"location":"how-to-guides/configuration/mount-volumes/#full-example-with-multiple-volumes","title":"Full example with multiple volumes","text":"<pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: multi-volume-test\nspec:\n  image: locustio/locust:2.20.0\n  testFiles:\n    configMapRef: my-test\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://api.example.com\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 5\n  volumes:\n    # Large test data on PVC\n    - name: test-data\n      persistentVolumeClaim:\n        claimName: test-data-pvc\n    # TLS certificates from secret\n    - name: tls-certs\n      secret:\n        secretName: api-certs\n    # Configuration from ConfigMap\n    - name: app-config\n      configMap:\n        name: test-config\n    # Temporary cache per worker\n    - name: cache\n      emptyDir: {}\n  volumeMounts:\n    - name: test-data\n      mountPath: /data\n      target: both         # All pods read test data\n    - name: tls-certs\n      mountPath: /certs\n      readOnly: true\n      target: both         # All pods use same certs\n    - name: app-config\n      mountPath: /config\n      target: both         # All pods read config\n    - name: cache\n      mountPath: /tmp/cache\n      target: worker       # Only workers use cache\n</code></pre>","tags":["configuration","volumes","storage"]},{"location":"how-to-guides/configuration/mount-volumes/#whats-next","title":"What's next","text":"<ul> <li>Inject secrets \u2014 Pass credentials as environment variables instead of files</li> <li>Use private registry \u2014 Pull custom images with volume-specific tools</li> <li>Configure resources \u2014 Ensure pods have enough resources for I/O operations</li> </ul>","tags":["configuration","volumes","storage"]},{"location":"how-to-guides/configuration/use-private-registry/","title":"Use a private image registry","text":"<p>Pull custom Locust images from private container registries like Docker Hub, GitHub Container Registry, or AWS ECR.</p>","tags":["configuration","images","security"]},{"location":"how-to-guides/configuration/use-private-registry/#prerequisites","title":"Prerequisites","text":"<ul> <li>Registry credentials (username/password or access token)</li> <li>Custom Locust image pushed to private registry</li> </ul>","tags":["configuration","images","security"]},{"location":"how-to-guides/configuration/use-private-registry/#create-image-pull-secret","title":"Create image pull secret","text":"<p>Store registry credentials in a Kubernetes secret:</p> <pre><code>kubectl create secret docker-registry my-registry-secret \\\n  --docker-server=ghcr.io \\\n  --docker-username=myusername \\\n  --docker-password=ghp_myPersonalAccessToken \\\n  --docker-email=user@example.com\n</code></pre> <p>For specific registries:</p> GitHub Container RegistryDocker HubAWS ECRGoogle Container Registry <pre><code>kubectl create secret docker-registry ghcr-secret \\\n  --docker-server=ghcr.io \\\n  --docker-username=myusername \\\n  --docker-password=ghp_myPersonalAccessToken \\\n  --docker-email=user@example.com\n</code></pre> <pre><code>kubectl create secret docker-registry dockerhub-secret \\\n  --docker-server=docker.io \\\n  --docker-username=myusername \\\n  --docker-password=myAccessToken \\\n  --docker-email=user@example.com\n</code></pre> <pre><code># Get ECR login token (expires after 12 hours)\naws ecr get-login-password --region us-east-1 | \\\n  kubectl create secret docker-registry ecr-secret \\\n    --docker-server=123456789012.dkr.ecr.us-east-1.amazonaws.com \\\n    --docker-username=AWS \\\n    --docker-password-stdin \\\n    --docker-email=user@example.com\n</code></pre> <pre><code># Use JSON key file\nkubectl create secret docker-registry gcr-secret \\\n  --docker-server=gcr.io \\\n  --docker-username=_json_key \\\n  --docker-password=\"$(cat key.json)\" \\\n  --docker-email=user@example.com\n</code></pre> <p>Verify the secret exists:</p> <pre><code>kubectl get secret my-registry-secret\n</code></pre>","tags":["configuration","images","security"]},{"location":"how-to-guides/configuration/use-private-registry/#reference-secret-in-locusttest","title":"Reference secret in LocustTest","text":"<p>Add <code>imagePullSecrets</code> to your LocustTest CR:</p> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: private-registry-test\nspec:\n  image: ghcr.io/mycompany/locust-custom:v1.2.3  # Private image\n  imagePullSecrets:  # Reference the secret\n    - name: my-registry-secret\n  testFiles:\n    configMapRef: my-test\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://api.example.com\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 3\n</code></pre> <p>Apply the CR:</p> <pre><code>kubectl apply -f locusttest-private.yaml\n</code></pre>","tags":["configuration","images","security"]},{"location":"how-to-guides/configuration/use-private-registry/#configure-image-pull-policy","title":"Configure image pull policy","text":"<p>Control when Kubernetes pulls the image:</p> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: pull-policy-test\nspec:\n  image: ghcr.io/mycompany/locust-custom:latest\n  imagePullPolicy: Always  # Pull image every time\n  imagePullSecrets:\n    - name: my-registry-secret\n  testFiles:\n    configMapRef: my-test\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://api.example.com\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 3\n</code></pre> <p>Pull policy options:</p> Policy Behavior When to use <code>Always</code> Pull image on every pod creation Development with <code>:latest</code> tag or frequently updated images <code>IfNotPresent</code> Pull only if not cached locally Stable versioned images (default for non-<code>:latest</code> tags) <code>Never</code> Never pull, use cached image only Pre-loaded images or air-gapped environments <p>Recommendation: Use <code>Always</code> with <code>:latest</code> tags. Use <code>IfNotPresent</code> or omit (default) with version tags like <code>v1.2.3</code>.</p>","tags":["configuration","images","security"]},{"location":"how-to-guides/configuration/use-private-registry/#verify-image-pull","title":"Verify image pull","text":"<p>Check that pods successfully pulled the image:</p> <pre><code># Get pod status\nkubectl get pods -l performance-test-name=private-registry-test\n\n# Check image field\nkubectl get pod -l performance-test-pod-name=private-registry-test-master -o jsonpath='{.items[0].spec.containers[0].image}'\n</code></pre> <p>Expected output:</p> <pre><code>ghcr.io/mycompany/locust-custom:v1.2.3\n</code></pre> <p>Verify pull policy:</p> <pre><code>kubectl get pod -l performance-test-pod-name=private-registry-test-master -o jsonpath='{.items[0].spec.containers[0].imagePullPolicy}'\n</code></pre>","tags":["configuration","images","security"]},{"location":"how-to-guides/configuration/use-private-registry/#troubleshoot-imagepullbackoff","title":"Troubleshoot ImagePullBackOff","text":"<p>If pods fail with <code>ImagePullBackOff</code>:</p> <pre><code># Check pod events\nkubectl describe pod -l performance-test-name=private-registry-test | grep -A 10 \"Events:\"\n</code></pre> <p>Common issues:</p> <p>Authentication failed:</p> <pre><code>Failed to pull image: unauthorized: authentication required\n</code></pre> <p>Fix: Verify secret credentials are correct. Recreate the secret if needed.</p> <p>Image not found:</p> <pre><code>Failed to pull image: manifest unknown: manifest unknown\n</code></pre> <p>Fix: Verify image name, tag, and registry URL. Check the image exists:</p> <pre><code># For Docker Hub\ndocker pull ghcr.io/mycompany/locust-custom:v1.2.3\n\n# For AWS ECR\naws ecr describe-images --repository-name locust-custom --region us-east-1\n</code></pre> <p>Wrong secret referenced:</p> <pre><code>Couldn't find key .dockerconfigjson in Secret\n</code></pre> <p>Fix: Verify secret name in <code>imagePullSecrets</code> matches the created secret:</p> <pre><code>kubectl get secrets | grep registry\n</code></pre> <p>Network policy blocking registry:</p> <pre><code>Failed to pull image: dial tcp: i/o timeout\n</code></pre> <p>Fix: Check network policies allow egress to the registry:</p> <pre><code>kubectl get networkpolicies\n</code></pre>","tags":["configuration","images","security"]},{"location":"how-to-guides/configuration/use-private-registry/#whats-next","title":"What's next","text":"<ul> <li>Mount volumes \u2014 Add test data or certificates to pods</li> <li>Inject secrets \u2014 Pass API keys and credentials as environment variables</li> <li>Configure resources \u2014 Set CPU and memory limits for custom images</li> </ul>","tags":["configuration","images","security"]},{"location":"how-to-guides/observability/configure-opentelemetry/","title":"Configure OpenTelemetry integration","text":"<p>Native OpenTelemetry support in v2 eliminates the need for a metrics exporter sidecar. Your Locust tests can export metrics and traces directly to an OTel Collector.</p>","tags":["observability","opentelemetry","metrics","traces","monitoring"]},{"location":"how-to-guides/observability/configure-opentelemetry/#prerequisites","title":"Prerequisites","text":"<p>You need an OpenTelemetry Collector deployed in your cluster. The collector receives telemetry data from Locust and forwards it to your observability backend (Prometheus, Jaeger, Tempo, etc.).</p>","tags":["observability","opentelemetry","metrics","traces","monitoring"]},{"location":"how-to-guides/observability/configure-opentelemetry/#step-1-verify-otel-collector-endpoint-connectivity","title":"Step 1: Verify OTel Collector endpoint connectivity","text":"<p>Determine the correct endpoint for your OTel Collector:</p> Scenario Endpoint Format Example Same namespace <code>http://&lt;service-name&gt;:&lt;port&gt;</code> <code>http://otel-collector:4317</code> Different namespace <code>http://&lt;service-name&gt;.&lt;namespace&gt;:&lt;port&gt;</code> <code>http://otel-collector.monitoring:4317</code> External collector <code>https://&lt;hostname&gt;:&lt;port&gt;</code> <code>https://otel.example.com:4317</code> <p>Test connectivity from a debug pod:</p> <pre><code>kubectl run debug --image=busybox --rm -it -- nc -zv otel-collector.monitoring 4317\n</code></pre> <p>If the connection succeeds, you'll see: <code>otel-collector.monitoring (10.0.1.5:4317) open</code></p>","tags":["observability","opentelemetry","metrics","traces","monitoring"]},{"location":"how-to-guides/observability/configure-opentelemetry/#step-2-configure-in-locusttest-cr","title":"Step 2: Configure in LocustTest CR","text":"<p>Add the <code>observability.openTelemetry</code> block to your LocustTest CR:</p> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: otel-enabled-test\nspec:\n  image: locustio/locust:2.20.0\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://api.example.com\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 5\n  observability:\n    openTelemetry:\n      enabled: true                                    # Enable OTel integration\n      endpoint: \"http://otel-collector.monitoring:4317\"  # OTel Collector endpoint\n      protocol: \"grpc\"                                 # Use gRPC (or \"http/protobuf\")\n      insecure: false                                  # Use TLS (set true for dev/testing)\n      extraEnvVars:\n        OTEL_SERVICE_NAME: \"my-load-test\"              # Service name in traces\n        OTEL_RESOURCE_ATTRIBUTES: \"environment=staging,team=platform\"  # Resource attributes\n</code></pre> <p>Configuration fields explained:</p> <ul> <li><code>enabled</code>: Set to <code>true</code> to activate OpenTelemetry integration</li> <li><code>endpoint</code>: OTel Collector URL (scheme://hostname:port). Include the scheme (<code>http://</code> or <code>https://</code>) for compatibility across OTel SDK versions.</li> <li><code>protocol</code>: Transport protocol<ul> <li><code>grpc</code> (recommended, default): Use gRPC transport</li> <li><code>http/protobuf</code>: Use HTTP/protobuf transport</li> </ul> </li> <li> <p><code>insecure</code>: TLS configuration</p> <ul> <li><code>false</code> (default): Use TLS for secure communication</li> <li><code>true</code>: Skip TLS verification (development/testing only)</li> </ul> <p>Note</p> <p>TLS behavior primarily depends on the endpoint scheme (<code>http://</code> vs <code>https://</code>). The <code>OTEL_EXPORTER_OTLP_INSECURE</code> environment variable is set by the operator but may not be recognized by all OTel SDK implementations (e.g., Python). Use <code>http://</code> endpoints for non-TLS connections.</p> <ul> <li><code>extraEnvVars</code>: Additional OpenTelemetry environment variables</li> <li><code>OTEL_SERVICE_NAME</code>: Identifier for this test in traces</li> <li><code>OTEL_RESOURCE_ATTRIBUTES</code>: Metadata tags (key=value pairs, comma-separated)</li> </ul> </li> </ul>","tags":["observability","opentelemetry","metrics","traces","monitoring"]},{"location":"how-to-guides/observability/configure-opentelemetry/#step-3-deploy-and-verify","title":"Step 3: Deploy and verify","text":"<p>Apply your LocustTest CR:</p> <pre><code>kubectl apply -f locusttest.yaml\n</code></pre> <p>Check that OTel environment variables were injected:</p> <pre><code>kubectl get pod -l performance-test-name=otel-enabled-test -o yaml | grep OTEL_\n</code></pre> <p>Expected environment variables:</p> Variable Value Purpose <code>OTEL_TRACES_EXPORTER</code> <code>otlp</code> Enable OTLP trace export <code>OTEL_METRICS_EXPORTER</code> <code>otlp</code> Enable OTLP metrics export <code>OTEL_EXPORTER_OTLP_ENDPOINT</code> Your endpoint Collector address <code>OTEL_EXPORTER_OTLP_PROTOCOL</code> <code>grpc</code> or <code>http/protobuf</code> Transport protocol <code>OTEL_EXPORTER_OTLP_INSECURE</code> <code>true</code> (if set) Skip TLS verification <code>OTEL_SERVICE_NAME</code> Your service name Service identifier <code>OTEL_RESOURCE_ATTRIBUTES</code> Your attributes Resource metadata","tags":["observability","opentelemetry","metrics","traces","monitoring"]},{"location":"how-to-guides/observability/configure-opentelemetry/#step-4-query-traces-and-metrics","title":"Step 4: Query traces and metrics","text":"<p>Once your test is running, telemetry flows to your OTel Collector and downstream backends.</p> <p>Prometheus metrics (if OTel Collector exports to Prometheus):</p> <p>Note</p> <p>The exact metric names depend on your OTel Collector pipeline configuration and Locust's OTel instrumentation. The examples below assume the Collector exports to Prometheus with default naming.</p> <pre><code># Request rate by service\nrate(locust_requests_total{service_name=\"my-load-test\"}[1m])\n\n# Average response time\navg(locust_response_time_seconds{service_name=\"my-load-test\"})\n</code></pre> <p>Jaeger/Tempo traces (if OTel Collector exports to tracing backend):</p> <p>Filter by: - Service name: <code>my-load-test</code> - Resource attributes: <code>environment=staging</code>, <code>team=platform</code></p> <p>Look for: - Request spans showing HTTP calls - Duration metrics for performance analysis - Error traces for debugging failures</p>","tags":["observability","opentelemetry","metrics","traces","monitoring"]},{"location":"how-to-guides/observability/configure-opentelemetry/#troubleshooting","title":"Troubleshooting","text":"","tags":["observability","opentelemetry","metrics","traces","monitoring"]},{"location":"how-to-guides/observability/configure-opentelemetry/#no-traces-appearing-in-backend","title":"No traces appearing in backend","text":"<p>Check Locust logs for OTel errors:</p> <pre><code>kubectl logs job/otel-enabled-test-master | grep -i otel\n</code></pre> <p>Common issues:</p> Problem Symptom Solution Wrong endpoint Connection refused Verify endpoint with <code>nc -zv</code> test TLS mismatch TLS handshake errors Set <code>insecure: true</code> for testing, or fix TLS certificates Collector not receiving OTLP No error in logs but no data Check collector logs and verify protocol matches Network policy blocking Connection timeouts Ensure NetworkPolicy allows egress to collector <p>Check OTel Collector logs:</p> <pre><code>kubectl logs -n monitoring deployment/otel-collector | grep -i error\n</code></pre>","tags":["observability","opentelemetry","metrics","traces","monitoring"]},{"location":"how-to-guides/observability/configure-opentelemetry/#performance-impact","title":"Performance impact","text":"<p>OpenTelemetry adds overhead to your test execution:</p> <ul> <li>Typical overhead: 2-5% additional CPU/memory usage</li> <li>Network overhead: Depends on telemetry volume and sampling</li> </ul> <p>Recommendations:</p> <ul> <li>Use sampling for high-volume tests:   <pre><code>extraEnvVars:\n  OTEL_TRACES_SAMPLER: \"traceidratio\"\n  OTEL_TRACES_SAMPLER_ARG: \"0.1\"  # Sample 10% of traces\n</code></pre></li> <li>Adjust collector resources if experiencing backpressure</li> <li>Monitor test pods for resource saturation when OTel is enabled</li> </ul>","tags":["observability","opentelemetry","metrics","traces","monitoring"]},{"location":"how-to-guides/observability/configure-opentelemetry/#otel-vs-metrics-sidecar-comparison","title":"OTel vs Metrics Sidecar comparison","text":"Aspect OpenTelemetry Metrics Sidecar Traces Yes No Metrics Yes Yes Additional containers None 1 sidecar per master pod Setup complexity Requires OTel Collector Works with Prometheus directly Resource overhead 2-5% in test pods Additional sidecar container Recommended for New deployments, distributed tracing needs Legacy compatibility, Prometheus-only stacks v2 API Yes Yes (default when OTel disabled) v1 API No Yes <p>When OpenTelemetry is enabled:</p> <ul> <li>The <code>--otel</code> flag is automatically added to Locust commands</li> <li>The metrics exporter sidecar is NOT deployed</li> <li>The metrics port (9646) is excluded from the Service</li> </ul> <p>When to use each:</p> <ul> <li>Use OpenTelemetry if you need traces, want to reduce container count, or are building a new observability stack</li> <li>Use Metrics Sidecar if you only need Prometheus metrics, have existing Prometheus infrastructure, or need v1 API compatibility</li> </ul>","tags":["observability","opentelemetry","metrics","traces","monitoring"]},{"location":"how-to-guides/observability/configure-opentelemetry/#related-guides","title":"Related guides","text":"<ul> <li>Monitor test status and health \u2014 Check test phase, conditions, and pod health</li> <li>Configure resources \u2014 Adjust resource limits for OTel overhead</li> <li>Metrics &amp; Dashboards \u2014 Complete observability reference</li> </ul>","tags":["observability","opentelemetry","metrics","traces","monitoring"]},{"location":"how-to-guides/observability/monitor-test-status/","title":"Monitor test status and health","text":"<p>The operator reports test status through standard Kubernetes status fields and conditions. This guide shows you how to monitor test execution, detect failures, and integrate with CI/CD pipelines.</p>","tags":["observability","monitoring","status","health checks","ci/cd"]},{"location":"how-to-guides/observability/monitor-test-status/#how-the-operator-reports-test-status","title":"How the operator reports test status","text":"<p>The operator updates <code>.status</code> on your LocustTest CR throughout its lifecycle:</p> <ul> <li>Phase: Current state (Pending \u2192 Running \u2192 Succeeded/Failed)</li> <li>Conditions: Detailed health indicators (Ready, WorkersConnected, PodsHealthy, etc.)</li> <li>Worker counts: Expected vs connected workers</li> <li>Timestamps: Start time and completion time</li> </ul>","tags":["observability","monitoring","status","health checks","ci/cd"]},{"location":"how-to-guides/observability/monitor-test-status/#watch-test-progress","title":"Watch test progress","text":"<p>Monitor phase changes in real-time:</p> <pre><code>kubectl get locusttest my-test -w\n</code></pre> <p>Expected output:</p> <pre><code>NAME      PHASE      WORKERS   CONNECTED   AGE\nmy-test   Pending    5                     2s\nmy-test   Pending    5         0           5s\nmy-test   Running    5         3           12s\nmy-test   Running    5         5           18s\nmy-test   Succeeded  5         5           5m32s\n</code></pre> <p>Output columns explained:</p> Column Description NAME LocustTest resource name PHASE Current lifecycle phase WORKERS Requested worker count (from spec) CONNECTED Active worker pods (approximation from Job status) AGE Time since CR creation","tags":["observability","monitoring","status","health checks","ci/cd"]},{"location":"how-to-guides/observability/monitor-test-status/#phase-progression","title":"Phase progression","text":"<p>Tests move through these phases:</p> <pre><code>stateDiagram-v2\n    [*] --&gt; Pending: CR created\n    Pending --&gt; Running: Master pod active\n    Running --&gt; Succeeded: Test completed (exit 0)\n    Running --&gt; Failed: Master failed or pods unhealthy\n    Pending --&gt; Failed: Pod health check failed</code></pre> Phase Meaning What to do Pending Resources creating (Service, Jobs), pods scheduling Wait for resources to schedule. Check events if stuck &gt;2 minutes. Running Master pod active, test executing Monitor worker connections and test progress. Succeeded Master job completed successfully (exit code 0) Collect results. CR can be deleted or kept for records. Failed Master job failed or pods unhealthy (after 2-minute grace period) Check pod logs and events. Delete and recreate to retry. <p>Grace period</p> <p>The operator waits 2 minutes after pod creation before reporting pod health failures. This prevents false alarms during image pulls and startup.</p>","tags":["observability","monitoring","status","health checks","ci/cd"]},{"location":"how-to-guides/observability/monitor-test-status/#check-status-conditions","title":"Check status conditions","text":"<p>Conditions provide detailed health information beyond the phase.</p> <p>View all conditions:</p> <pre><code>kubectl get locusttest my-test -o jsonpath='{.status.conditions}' | jq .\n</code></pre> <p>Example output:</p> <pre><code>[\n  {\n    \"type\": \"Ready\",\n    \"status\": \"True\",\n    \"reason\": \"ResourcesCreated\",\n    \"message\": \"All resources created successfully\"\n  },\n  {\n    \"type\": \"WorkersConnected\",\n    \"status\": \"True\",\n    \"reason\": \"AllWorkersConnected\",\n    \"message\": \"5/5 workers connected\"\n  },\n  {\n    \"type\": \"PodsHealthy\",\n    \"status\": \"True\",\n    \"reason\": \"PodsHealthy\",\n    \"message\": \"All pods running normally\"\n  }\n]\n</code></pre>","tags":["observability","monitoring","status","health checks","ci/cd"]},{"location":"how-to-guides/observability/monitor-test-status/#key-condition-types","title":"Key condition types","text":"","tags":["observability","monitoring","status","health checks","ci/cd"]},{"location":"how-to-guides/observability/monitor-test-status/#ready","title":"Ready","text":"<p>Indicates whether test resources were created successfully.</p> Status Reason Meaning <code>True</code> <code>ResourcesCreated</code> All resources (Service, Jobs) created successfully <code>False</code> <code>ResourcesCreating</code> Resources are being created <code>False</code> <code>ResourcesFailed</code> Test failed, resources in error state","tags":["observability","monitoring","status","health checks","ci/cd"]},{"location":"how-to-guides/observability/monitor-test-status/#workersconnected","title":"WorkersConnected","text":"<p>Tracks worker connection progress.</p> Status Reason Meaning <code>True</code> <code>AllWorkersConnected</code> All expected workers have active pods <code>False</code> <code>WaitingForWorkers</code> Initial state, waiting for worker pods <code>False</code> <code>WorkersMissing</code> Some workers not yet active (message shows N/M count) <p>Note</p> <p><code>connectedWorkers</code> is an approximation from Job.Status.Active. It may briefly lag behind actual Locust master connections.</p>","tags":["observability","monitoring","status","health checks","ci/cd"]},{"location":"how-to-guides/observability/monitor-test-status/#podshealthy","title":"PodsHealthy","text":"<p>Detects pod-level failures (crashes, scheduling issues, image pull errors).</p> Status Reason Meaning <code>True</code> <code>PodsHealthy</code> All pods running normally <code>True</code> <code>PodsStarting</code> Within 2-minute grace period (not yet checking health) <code>False</code> <code>ImagePullError</code> One or more pods cannot pull container image <code>False</code> <code>ConfigurationError</code> ConfigMap or Secret referenced in CR not found <code>False</code> <code>SchedulingError</code> Pod cannot be scheduled (node affinity, insufficient resources) <code>False</code> <code>CrashLoopBackOff</code> Container repeatedly crashing <code>False</code> <code>InitializationError</code> Init container failed <p>Check a specific condition:</p> <pre><code>kubectl get locusttest my-test -o jsonpath='{.status.conditions[?(@.type==\"PodsHealthy\")]}'\n</code></pre>","tags":["observability","monitoring","status","health checks","ci/cd"]},{"location":"how-to-guides/observability/monitor-test-status/#specdrifted","title":"SpecDrifted","text":"<p>Appears only when the CR spec is edited after creation (tests are immutable).</p> Status Reason Meaning <code>True</code> <code>SpecChangeIgnored</code> Spec was modified after creation. Changes ignored. Delete and recreate to apply.","tags":["observability","monitoring","status","health checks","ci/cd"]},{"location":"how-to-guides/observability/monitor-test-status/#detect-pod-failures","title":"Detect pod failures","text":"<p>When <code>PodsHealthy=False</code>, the operator detected a problem with test pods.</p> <p>Get condition details:</p> <pre><code>kubectl describe locusttest my-test\n</code></pre> <p>Look for the <code>PodsHealthy</code> condition in the Status section. The message field explains what failed.</p> <p>Example failure messages:</p> <ul> <li><code>Master pod image pull error: ErrImagePull</code></li> <li><code>Worker pod configuration error: Secret \"api-creds\" not found</code></li> <li><code>Worker pod scheduling error: 0/3 nodes available: insufficient cpu</code></li> <li><code>Master pod crash loop: CrashLoopBackOff</code></li> </ul> <p>View pod states directly:</p> <pre><code>kubectl get pods -l performance-test-name=my-test\n</code></pre> <p>Check pod logs for errors:</p> <pre><code># Master logs\nkubectl logs job/my-test-master\n\n# Worker logs (first worker pod)\nkubectl logs job/my-test-worker --max-log-requests=1\n</code></pre>","tags":["observability","monitoring","status","health checks","ci/cd"]},{"location":"how-to-guides/observability/monitor-test-status/#common-failure-scenarios","title":"Common failure scenarios","text":"Symptom Likely cause How to investigate Phase stuck in <code>Pending</code> Pods not scheduling <code>kubectl describe pod</code> for scheduling errors <code>PodsHealthy=False</code> with <code>ImagePullError</code> Wrong image name or missing imagePullSecret Check image name in spec, verify secret exists <code>PodsHealthy=False</code> with <code>ConfigurationError</code> Missing ConfigMap or Secret Verify referenced resources exist: <code>kubectl get configmap,secret</code> Phase transitions to <code>Failed</code> immediately Master pod crashed on startup Check master logs for Python errors in locustfile Workers never connect Network policy or firewall Verify workers can reach master service on port 5557","tags":["observability","monitoring","status","health checks","ci/cd"]},{"location":"how-to-guides/observability/monitor-test-status/#cicd-integration","title":"CI/CD integration","text":"<p>Use <code>kubectl wait</code> to block until test completion. The operator follows Kubernetes condition conventions, making it compatible with standard CI/CD tools.</p>","tags":["observability","monitoring","status","health checks","ci/cd"]},{"location":"how-to-guides/observability/monitor-test-status/#github-actions-example","title":"GitHub Actions example","text":"<pre><code>name: Load Test\non:\n  workflow_dispatch:\n\njobs:\n  load-test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Apply test\n        run: kubectl apply -f locusttest.yaml\n\n      - name: Wait for test completion\n        run: |\n          kubectl wait locusttest/my-test \\\n            --for=jsonpath='{.status.phase}'=Succeeded \\\n            --timeout=30m\n\n      - name: Check result\n        if: failure()\n        run: |\n          echo \"Test failed or timed out\"\n          kubectl describe locusttest my-test\n          kubectl logs -l performance-test-name=my-test --tail=50\n\n      - name: Cleanup\n        if: always()\n        run: kubectl delete locusttest my-test --ignore-not-found\n</code></pre>","tags":["observability","monitoring","status","health checks","ci/cd"]},{"location":"how-to-guides/observability/monitor-test-status/#generic-shell-script","title":"Generic shell script","text":"<pre><code>#!/bin/bash\nset -e\n\n# Apply test\nkubectl apply -f locusttest.yaml\n\n# Wait for completion (Succeeded or Failed)\necho \"Waiting for test to complete...\"\nwhile true; do\n  PHASE=$(kubectl get locusttest my-test -o jsonpath='{.status.phase}' 2&gt;/dev/null)\n  case \"$PHASE\" in\n    Succeeded)\n      echo \"Test passed!\"\n      exit 0\n      ;;\n    Failed)\n      echo \"Test failed!\"\n      kubectl describe locusttest my-test\n      kubectl logs job/my-test-master --tail=50\n      exit 1\n      ;;\n    Pending|Running)\n      echo \"Phase: $PHASE - waiting...\"\n      sleep 10\n      ;;\n    *)\n      echo \"Unknown phase: $PHASE\"\n      sleep 10\n      ;;\n  esac\ndone\n</code></pre> <p>Wait patterns:</p> <pre><code># Wait for specific phase\nkubectl wait locusttest/my-test --for=jsonpath='{.status.phase}'=Succeeded --timeout=30m\n\n# Wait for condition\nkubectl wait locusttest/my-test --for=condition=Ready --timeout=5m\n\n# Check if test completed (success or failure)\nPHASE=$(kubectl get locusttest my-test -o jsonpath='{.status.phase}')\nif [ \"$PHASE\" = \"Succeeded\" ]; then\n  echo \"Test passed\"\nelif [ \"$PHASE\" = \"Failed\" ]; then\n  echo \"Test failed\"\n  exit 1\nfi\n</code></pre>","tags":["observability","monitoring","status","health checks","ci/cd"]},{"location":"how-to-guides/observability/monitor-test-status/#check-worker-connection-progress","title":"Check worker connection progress","text":"<p>Monitor how many workers have connected to the master:</p> <pre><code>kubectl get locusttest my-test -o jsonpath='{.status.connectedWorkers}/{.status.expectedWorkers}'\n</code></pre> <p>Example output: <code>5/5</code> (all workers connected)</p> <p>View WorkersConnected condition:</p> <pre><code>kubectl get locusttest my-test -o jsonpath='{.status.conditions[?(@.type==\"WorkersConnected\")]}'\n</code></pre> <p>If workers aren't connecting:</p> <ol> <li> <p>Check worker pod status: <pre><code>kubectl get pods -l performance-test-pod-name=my-test-worker\n</code></pre></p> </li> <li> <p>Verify master service exists: <pre><code>kubectl get service my-test-master\n</code></pre></p> </li> <li> <p>Check worker logs for connection errors: <pre><code>kubectl logs job/my-test-worker --max-log-requests=1 | grep -i connect\n</code></pre></p> </li> </ol>","tags":["observability","monitoring","status","health checks","ci/cd"]},{"location":"how-to-guides/observability/monitor-test-status/#related-guides","title":"Related guides","text":"<ul> <li>Configure OpenTelemetry integration \u2014 Export metrics and traces from tests</li> <li>API Reference - Status Fields \u2014 Complete status field documentation</li> <li>Metrics &amp; Dashboards \u2014 Monitor test metrics with Prometheus</li> </ul>","tags":["observability","monitoring","status","health checks","ci/cd"]},{"location":"how-to-guides/scaling/configure-tolerations/","title":"Configure tolerations for tainted nodes","text":"<p>Schedule Locust pods on tainted nodes using tolerations, enabling dedicated node pools and preventing other workloads from using test infrastructure.</p>","tags":["scaling","scheduling","tolerations","taints"]},{"location":"how-to-guides/scaling/configure-tolerations/#prerequisites","title":"Prerequisites","text":"<ul> <li>Locust Kubernetes Operator installed</li> <li>Access to taint cluster nodes</li> </ul>","tags":["scaling","scheduling","tolerations","taints"]},{"location":"how-to-guides/scaling/configure-tolerations/#when-to-use-tolerations","title":"When to use tolerations","text":"<p>Common use cases:</p> <ul> <li>Dedicated node pools: Reserve nodes exclusively for load testing</li> <li>High-performance nodes: Prevent regular workloads from consuming resources</li> <li>Spot instances: Allow tests on spot/preemptible nodes with taints</li> <li>Specialized hardware: Schedule on GPU or high-memory nodes</li> </ul> <p>How taints and tolerations work:</p> <ol> <li>Taint nodes: Mark nodes as special-purpose (e.g., \"dedicated=load-testing:NoSchedule\")</li> <li>Add tolerations: Pods with matching tolerations can be scheduled on tainted nodes</li> <li>Result: Only pods with tolerations use the tainted nodes</li> </ol>","tags":["scaling","scheduling","tolerations","taints"]},{"location":"how-to-guides/scaling/configure-tolerations/#taint-your-nodes","title":"Taint your nodes","text":"<p>Add taints to nodes you want to dedicate for testing:</p> <pre><code># Taint a node for load testing\nkubectl taint nodes node-1 dedicated=load-testing:NoSchedule\n\n# Taint multiple nodes\nkubectl taint nodes node-2 dedicated=load-testing:NoSchedule\nkubectl taint nodes node-3 dedicated=load-testing:NoSchedule\n\n# Verify taints\nkubectl describe node node-1 | grep Taints\n</code></pre> <p>Taint effects:</p> Effect Behavior <code>NoSchedule</code> New pods without toleration won't be scheduled <code>PreferNoSchedule</code> Scheduler tries to avoid placing pods here (soft) <code>NoExecute</code> Existing pods without toleration are evicted <p>Recommendation: Use <code>NoSchedule</code> for dedicated test nodes.</p>","tags":["scaling","scheduling","tolerations","taints"]},{"location":"how-to-guides/scaling/configure-tolerations/#configure-tolerations","title":"Configure tolerations","text":"<p>Add <code>scheduling.tolerations</code> to your LocustTest CR:</p> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: toleration-test\nspec:\n  image: locustio/locust:2.20.0\n  testFiles:\n    configMapRef: my-test\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://api.example.com\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 10\n  scheduling:\n    tolerations:\n      - key: dedicated\n        operator: Equal\n        value: load-testing\n        effect: NoSchedule\n</code></pre> <p>Apply the configuration:</p> <pre><code>kubectl apply -f locusttest-toleration.yaml\n</code></pre>","tags":["scaling","scheduling","tolerations","taints"]},{"location":"how-to-guides/scaling/configure-tolerations/#toleration-operators","title":"Toleration operators","text":"<p>Equal operator: Exact match required</p> <pre><code>tolerations:\n  - key: dedicated\n    operator: Equal\n    value: load-testing  # Must match exactly\n    effect: NoSchedule\n</code></pre> <p>Matches taint: <code>dedicated=load-testing:NoSchedule</code></p> <p>Exists operator: Key must exist, value doesn't matter</p> <pre><code>tolerations:\n  - key: dedicated\n    operator: Exists  # Any value for key \"dedicated\"\n    effect: NoSchedule\n</code></pre> <p>Matches taints: <code>dedicated=load-testing:NoSchedule</code>, <code>dedicated=anything:NoSchedule</code></p>","tags":["scaling","scheduling","tolerations","taints"]},{"location":"how-to-guides/scaling/configure-tolerations/#multiple-tolerations","title":"Multiple tolerations","text":"<p>Tolerate multiple taints:</p> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: multi-toleration-test\nspec:\n  image: locustio/locust:2.20.0\n  testFiles:\n    configMapRef: my-test\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://api.example.com\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 10\n  scheduling:\n    tolerations:\n      - key: dedicated\n        operator: Equal\n        value: load-testing\n        effect: NoSchedule\n      - key: spot-instance\n        operator: Exists\n        effect: NoSchedule\n      - key: high-performance\n        operator: Equal\n        value: \"true\"\n        effect: PreferNoSchedule\n</code></pre> <p>Pods can be scheduled on nodes with any of these taints.</p>","tags":["scaling","scheduling","tolerations","taints"]},{"location":"how-to-guides/scaling/configure-tolerations/#example-dedicated-node-pool","title":"Example: Dedicated node pool","text":"<p>Complete setup for dedicated load testing nodes:</p> <p>1. Taint the nodes:</p> <pre><code>kubectl taint nodes node-pool-load-1 workload=load-testing:NoSchedule\nkubectl taint nodes node-pool-load-2 workload=load-testing:NoSchedule\nkubectl taint nodes node-pool-load-3 workload=load-testing:NoSchedule\n</code></pre> <p>2. Label the nodes (for affinity):</p> <pre><code>kubectl label nodes node-pool-load-1 workload-type=load-testing\nkubectl label nodes node-pool-load-2 workload-type=load-testing\nkubectl label nodes node-pool-load-3 workload-type=load-testing\n</code></pre> <p>3. Create LocustTest with affinity + tolerations:</p> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: dedicated-pool-test\nspec:\n  image: locustio/locust:2.20.0\n  testFiles:\n    configMapRef: my-test\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://api.example.com\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 15\n  scheduling:\n    affinity:\n      nodeAffinity:\n        requiredDuringSchedulingIgnoredDuringExecution:\n          nodeSelectorTerms:\n            - matchExpressions:\n                - key: workload-type\n                  operator: In\n                  values:\n                    - load-testing  # Target labeled nodes\n    tolerations:\n      - key: workload\n        operator: Equal\n        value: load-testing\n        effect: NoSchedule  # Tolerate the taint\n</code></pre> <p>Result: Pods only run on dedicated nodes, and only these pods can use those nodes.</p>","tags":["scaling","scheduling","tolerations","taints"]},{"location":"how-to-guides/scaling/configure-tolerations/#example-spot-instances","title":"Example: Spot instances","text":"<p>Run tests on cost-optimized spot/preemptible instances:</p> <p>1. Taint spot nodes:</p> <pre><code># Cloud providers often add this taint automatically\nkubectl taint nodes spot-node-1 cloud.google.com/gke-preemptible=true:NoSchedule\n</code></pre> <p>2. Tolerate spot node taints:</p> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: spot-instance-test\nspec:\n  image: locustio/locust:2.20.0\n  testFiles:\n    configMapRef: my-test\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://api.example.com\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 20\n  scheduling:\n    tolerations:\n      - key: cloud.google.com/gke-preemptible  # GKE spot instances\n        operator: Exists\n        effect: NoSchedule\n      - key: eks.amazonaws.com/capacityType    # AWS spot instances\n        operator: Equal\n        value: SPOT\n        effect: NoSchedule\n</code></pre>","tags":["scaling","scheduling","tolerations","taints"]},{"location":"how-to-guides/scaling/configure-tolerations/#noexecute-effect","title":"NoExecute effect","text":"<p><code>NoExecute</code> evicts running pods without toleration:</p> <pre><code># Taint with NoExecute\nkubectl taint nodes node-1 maintenance=scheduled:NoExecute\n</code></pre> <p>Pods without toleration are immediately evicted. Use for:</p> <ul> <li>Scheduled node maintenance</li> <li>Emergency capacity reclaim</li> <li>Node pool draining</li> </ul> <p>Toleration with grace period:</p> <pre><code>tolerations:\n  - key: maintenance\n    operator: Equal\n    value: scheduled\n    effect: NoExecute\n    tolerationSeconds: 300  # Pod survives 5 minutes, then evicted\n</code></pre>","tags":["scaling","scheduling","tolerations","taints"]},{"location":"how-to-guides/scaling/configure-tolerations/#verify-tolerations-and-node-placement","title":"Verify tolerations and node placement","text":"<p>Check that pods are scheduled on tainted nodes:</p> <pre><code># Show pod placement\nkubectl get pods -l performance-test-name=toleration-test -o wide\n\n# Check node taints\nkubectl describe node &lt;node-name&gt; | grep Taints\n\n# Verify pod tolerations\nkubectl get pod &lt;pod-name&gt; -o jsonpath='{.spec.tolerations}'\n</code></pre>","tags":["scaling","scheduling","tolerations","taints"]},{"location":"how-to-guides/scaling/configure-tolerations/#troubleshoot-scheduling-failures","title":"Troubleshoot scheduling failures","text":"<p>If pods remain <code>Pending</code>:</p> <pre><code>kubectl describe pod &lt;pod-name&gt; | grep -A 10 \"Events:\"\n</code></pre> <p>Common issues:</p> <p>Missing toleration:</p> <pre><code>Warning  FailedScheduling  0/3 nodes are available: 3 node(s) had taint {dedicated: load-testing}\n</code></pre> <p>Fix: Add matching toleration to the CR.</p> <p>Typo in taint key or value:</p> <pre><code># Check actual taint\nkubectl describe node node-1 | grep Taints\n\n# Output: Taints: dedicated=load-testing:NoSchedule\n</code></pre> <p>Ensure toleration matches exactly (case-sensitive).</p> <p>Wrong effect:</p> <p>Taint: <code>dedicated=load-testing:NoSchedule</code> Toleration: <code>effect: PreferNoSchedule</code> (mismatch)</p> <p>Fix: Match the effect in toleration to the taint.</p>","tags":["scaling","scheduling","tolerations","taints"]},{"location":"how-to-guides/scaling/configure-tolerations/#remove-taints","title":"Remove taints","text":"<p>When no longer needed:</p> <pre><code># Remove specific taint\nkubectl taint nodes node-1 dedicated=load-testing:NoSchedule-\n\n# Note the trailing minus (-) to remove\n</code></pre>","tags":["scaling","scheduling","tolerations","taints"]},{"location":"how-to-guides/scaling/configure-tolerations/#whats-next","title":"What's next","text":"<ul> <li>Use node affinity \u2014 Target specific nodes (often used together)</li> <li>Use node selector \u2014 Simpler alternative without taints</li> <li>Scale worker replicas \u2014 Calculate capacity for dedicated pools</li> </ul>","tags":["scaling","scheduling","tolerations","taints"]},{"location":"how-to-guides/scaling/scale-workers/","title":"Scale worker replicas for high load","text":"<p>Calculate and configure worker replicas to handle your target user count and request throughput.</p>","tags":["scaling","workers","performance"]},{"location":"how-to-guides/scaling/scale-workers/#prerequisites","title":"Prerequisites","text":"<ul> <li>Locust Kubernetes Operator installed</li> <li>Basic understanding of distributed load testing</li> </ul>","tags":["scaling","workers","performance"]},{"location":"how-to-guides/scaling/scale-workers/#how-worker-replicas-affect-throughput","title":"How worker replicas affect throughput","text":"<p>Each worker pod generates load independently. More workers = more throughput capacity.</p> <p>Key factors:</p> <ul> <li>User count: Each worker can efficiently handle 50-100 simulated users (depends on test complexity)</li> <li>Request rate: CPU-intensive tests (complex parsing, encryption) need more workers</li> <li>Memory usage: Tests with large payloads or state need more memory per worker</li> </ul>","tags":["scaling","workers","performance"]},{"location":"how-to-guides/scaling/scale-workers/#calculate-worker-count","title":"Calculate worker count","text":"<p>Formula:</p> <pre><code>workers = ceil(total_users / users_per_worker)\n</code></pre> <p>Default rule of thumb: 50 users per worker</p> <p>Examples:</p> Target Users Users/Worker Workers Needed 100 50 2 500 50 10 1000 50 20 5000 50 100 <p>Adjust users per worker based on test complexity:</p> <ul> <li>Simple tests (basic HTTP GET): 100 users/worker</li> <li>Standard tests (REST API with JSON): 50 users/worker</li> <li>Complex tests (heavy parsing, encryption, large payloads): 25 users/worker</li> </ul>","tags":["scaling","workers","performance"]},{"location":"how-to-guides/scaling/scale-workers/#configure-worker-replicas","title":"Configure worker replicas","text":"<p>Set worker count in your LocustTest CR:</p> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: scaled-test\nspec:\n  image: locustio/locust:2.20.0\n  testFiles:\n    configMapRef: my-test\n  master:\n    command: |\n      --locustfile /lotest/src/test.py\n      --host https://api.example.com\n      --users 1000          # Total simulated users\n      --spawn-rate 50       # Users to add per second\n      --run-time 10m\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 20          # 1000 users / 50 users per worker = 20 workers\n</code></pre> <p>Apply the configuration:</p> <pre><code>kubectl apply -f locusttest-scaled.yaml\n</code></pre>","tags":["scaling","workers","performance"]},{"location":"how-to-guides/scaling/scale-workers/#example-1000-users-test","title":"Example: 1000 users test","text":"<p>Complete configuration for 1000 concurrent users:</p> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: high-load-test\nspec:\n  image: locustio/locust:2.20.0\n  testFiles:\n    configMapRef: high-load-test-script\n  master:\n    command: |\n      --locustfile /lotest/src/test.py\n      --host https://api.example.com\n      --users 1000\n      --spawn-rate 50\n      --run-time 15m\n    resources:\n      requests:\n        memory: \"256Mi\"\n        cpu: \"200m\"\n      limits:\n        memory: \"512Mi\"\n        cpu: \"500m\"\n  worker:\n    replicas: 20  # 1000 users at 50 users/worker\n    command: \"--locustfile /lotest/src/test.py\"\n    resources:\n      requests:\n        memory: \"512Mi\"   # More memory for load generation\n        cpu: \"500m\"\n      limits:\n        memory: \"1Gi\"\n        # CPU limit omitted for maximum performance\n</code></pre>","tags":["scaling","workers","performance"]},{"location":"how-to-guides/scaling/scale-workers/#resource-implications","title":"Resource implications","text":"<p>Each worker consumes cluster resources:</p> <p>Per-worker resource baseline:</p> <ul> <li>CPU: 500m request, no limit (for performance)</li> <li>Memory: 512Mi-1Gi (depends on test data)</li> </ul> <p>Total resources for 20 workers:</p> <ul> <li>CPU: 10 cores requested (20 workers \u00d7 500m)</li> <li>Memory: 10-20Gi (20 workers \u00d7 512Mi-1Gi)</li> </ul> <p>Planning checklist:</p> <ul> <li> Cluster has enough capacity for all workers</li> <li> Consider using node affinity to target specific node pools</li> <li> Configure resource limits appropriately</li> <li> Use tolerations if running on dedicated nodes</li> </ul>","tags":["scaling","workers","performance"]},{"location":"how-to-guides/scaling/scale-workers/#monitor-connected-workers","title":"Monitor connected workers","text":"<p>Verify that all workers connect successfully:</p> <pre><code># Watch test status\nkubectl get locusttest high-load-test -w\n\n# Check status field\nkubectl get locusttest high-load-test -o jsonpath='{.status}'\n</code></pre> <p>Look for:</p> <pre><code>{\n  \"phase\": \"Running\",\n  \"expectedWorkers\": 20,\n  \"connectedWorkers\": 20\n}\n</code></pre> <p>If <code>connectedWorkers</code> &lt; <code>expectedWorkers</code>:</p> <pre><code># List worker pods\nkubectl get pods -l performance-test-pod-name=high-load-test-worker\n\n# Check for pending or failed pods\nkubectl get pods -l performance-test-pod-name=high-load-test-worker | grep -v Running\n\n# Describe problematic pods\nkubectl describe pod &lt;worker-pod-name&gt;\n</code></pre> <p>Common issues:</p> <ul> <li>Insufficient cluster capacity (pending pods)</li> <li>Image pull failures</li> <li>Resource quota exceeded</li> <li>Node selector or affinity constraints not satisfied</li> </ul>","tags":["scaling","workers","performance"]},{"location":"how-to-guides/scaling/scale-workers/#view-worker-pod-distribution","title":"View worker pod distribution","text":"<p>Check which nodes are running workers:</p> <pre><code>kubectl get pods -l performance-test-pod-name=high-load-test-worker -o wide\n</code></pre> <p>Output shows pod-to-node distribution:</p> <pre><code>NAME                        NODE            STATUS\nhigh-load-test-worker-0     node-pool-1-a   Running\nhigh-load-test-worker-1     node-pool-1-b   Running\nhigh-load-test-worker-2     node-pool-1-c   Running\n...\n</code></pre> <p>Best practice: Distribute workers across multiple nodes for resilience and better resource utilization.</p>","tags":["scaling","workers","performance"]},{"location":"how-to-guides/scaling/scale-workers/#scaling-considerations","title":"Scaling considerations","text":"<p>Spawn rate:</p> <p>Match spawn rate to worker count and network capacity:</p> <pre><code>recommended_spawn_rate = workers \u00d7 5-10 users/second\n</code></pre> <p>For 20 workers: 100-200 users/second spawn rate is reasonable.</p> <p>Example:</p> <pre><code>master:\n  command: |\n    --users 1000\n    --spawn-rate 100   # 20 workers \u00d7 5 users/sec/worker\n</code></pre> <p>Too high spawn rate overwhelms workers during ramp-up. Too low takes too long to reach target.</p> <p>Network bandwidth:</p> <p>High worker counts can saturate network:</p> <ul> <li>20 workers \u00d7 100 RPS = 2000 total RPS</li> <li>At 10KB per request = 20MB/s bandwidth</li> </ul> <p>Ensure cluster networking can handle aggregate throughput.</p> <p>Master capacity:</p> <p>Master coordinates all workers. Very high worker counts (&gt;50) may require increased master resources:</p> <pre><code>master:\n  resources:\n    requests:\n      memory: \"512Mi\"  # Increased from 256Mi\n      cpu: \"500m\"      # Increased from 200m\n    limits:\n      memory: \"1Gi\"\n      cpu: \"1000m\"\n</code></pre>","tags":["scaling","workers","performance"]},{"location":"how-to-guides/scaling/scale-workers/#adjusting-worker-count","title":"Adjusting worker count","text":"<p>Due to the operator's immutability model, worker count changes require updating the CR specification:</p> <pre><code># Edit the worker replica count in your LocustTest manifest\n# Change spec.worker.replicas to desired count, then apply:\nkubectl apply -f my-locust-test.yaml\n</code></pre> <p>New test execution</p> <p>Changing the worker count and reapplying creates a new test execution \u2014 this is not live scaling of running workers. The operator will tear down the existing test and create a new one with the updated worker count.</p>","tags":["scaling","workers","performance"]},{"location":"how-to-guides/scaling/scale-workers/#whats-next","title":"What's next","text":"<ul> <li>Configure resources \u2014 Set appropriate CPU and memory for workers</li> <li>Use node affinity \u2014 Target high-performance nodes for workers</li> <li>Configure tolerations \u2014 Run workers on dedicated node pools</li> </ul>","tags":["scaling","workers","performance"]},{"location":"how-to-guides/scaling/use-node-affinity/","title":"Use node affinity for dedicated test nodes","text":"<p>Schedule Locust pods on specific nodes using node affinity, enabling dedicated test infrastructure or zone isolation.</p>","tags":["scaling","scheduling","node affinity"]},{"location":"how-to-guides/scaling/use-node-affinity/#prerequisites","title":"Prerequisites","text":"<ul> <li>Locust Kubernetes Operator installed</li> <li>Access to label cluster nodes</li> </ul>","tags":["scaling","scheduling","node affinity"]},{"location":"how-to-guides/scaling/use-node-affinity/#when-to-use-node-affinity","title":"When to use node affinity","text":"<p>Common use cases:</p> <ul> <li>Dedicated nodes: Run load tests on nodes reserved for testing</li> <li>High-performance nodes: Target nodes with faster CPUs or more memory</li> <li>Zone isolation: Keep tests in specific availability zones</li> <li>Cost optimization: Use spot instances or lower-cost node pools for testing</li> </ul> <p>Node affinity vs node selector:</p> <ul> <li>Node selector: Simple label matching (use this for basic needs)</li> <li>Node affinity: Complex rules with OR logic, soft preferences, multiple conditions</li> </ul> <p>Use node affinity when you need the flexibility. Use node selector for simplicity.</p>","tags":["scaling","scheduling","node affinity"]},{"location":"how-to-guides/scaling/use-node-affinity/#label-your-nodes","title":"Label your nodes","text":"<p>Add labels to nodes where you want to run tests:</p> <pre><code># Label nodes for load testing\nkubectl label nodes node-1 workload-type=load-testing\nkubectl label nodes node-2 workload-type=load-testing\nkubectl label nodes node-3 workload-type=load-testing\n\n# Verify labels\nkubectl get nodes --show-labels | grep workload-type\n</code></pre> <p>Example labels:</p> <pre><code># By workload type\nkubectl label nodes node-1 workload-type=load-testing\n\n# By performance tier\nkubectl label nodes node-2 performance-tier=high\n\n# By environment\nkubectl label nodes node-3 environment=testing\n\n# By instance type (AWS)\nkubectl label nodes node-4 node.kubernetes.io/instance-type=c5.2xlarge\n</code></pre>","tags":["scaling","scheduling","node affinity"]},{"location":"how-to-guides/scaling/use-node-affinity/#configure-node-affinity","title":"Configure node affinity","text":"<p>Add <code>scheduling.affinity.nodeAffinity</code> to your LocustTest CR:</p> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: affinity-test\nspec:\n  image: locustio/locust:2.20.0\n  testFiles:\n    configMapRef: my-test\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://api.example.com\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 10\n  scheduling:\n    affinity:\n      nodeAffinity:\n        requiredDuringSchedulingIgnoredDuringExecution:  # Hard requirement\n          nodeSelectorTerms:\n            - matchExpressions:\n                - key: workload-type\n                  operator: In\n                  values:\n                    - load-testing  # Only schedule on nodes with this label\n</code></pre> <p>Apply the configuration:</p> <pre><code>kubectl apply -f locusttest-affinity.yaml\n</code></pre>","tags":["scaling","scheduling","node affinity"]},{"location":"how-to-guides/scaling/use-node-affinity/#multiple-label-requirements","title":"Multiple label requirements","text":"<p>Require multiple labels on nodes (AND logic):</p> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: multi-label-affinity\nspec:\n  image: locustio/locust:2.20.0\n  testFiles:\n    configMapRef: my-test\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://api.example.com\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 10\n  scheduling:\n    affinity:\n      nodeAffinity:\n        requiredDuringSchedulingIgnoredDuringExecution:\n          nodeSelectorTerms:\n            - matchExpressions:\n                - key: workload-type           # Must be load-testing\n                  operator: In\n                  values:\n                    - load-testing\n                - key: performance-tier        # AND must be high-performance\n                  operator: In\n                  values:\n                    - high\n                - key: environment             # AND must be in testing env\n                  operator: In\n                  values:\n                    - testing\n</code></pre> <p>All conditions must be true for a node to be selected.</p>","tags":["scaling","scheduling","node affinity"]},{"location":"how-to-guides/scaling/use-node-affinity/#example-aws-instance-type-targeting","title":"Example: AWS instance type targeting","text":"<p>Target specific EC2 instance types:</p> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: aws-instance-affinity\nspec:\n  image: locustio/locust:2.20.0\n  testFiles:\n    configMapRef: my-test\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://api.example.com\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 20\n  scheduling:\n    affinity:\n      nodeAffinity:\n        requiredDuringSchedulingIgnoredDuringExecution:\n          nodeSelectorTerms:\n            - matchExpressions:\n                - key: node.kubernetes.io/instance-type\n                  operator: In\n                  values:\n                    - c5.2xlarge   # Compute-optimized instances\n                    - c5.4xlarge\n</code></pre>","tags":["scaling","scheduling","node affinity"]},{"location":"how-to-guides/scaling/use-node-affinity/#example-zone-isolation","title":"Example: Zone isolation","text":"<p>Keep tests in specific availability zones:</p> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: zone-affinity-test\nspec:\n  image: locustio/locust:2.20.0\n  testFiles:\n    configMapRef: my-test\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://api.example.com\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 10\n  scheduling:\n    affinity:\n      nodeAffinity:\n        requiredDuringSchedulingIgnoredDuringExecution:\n          nodeSelectorTerms:\n            - matchExpressions:\n                - key: topology.kubernetes.io/zone\n                  operator: In\n                  values:\n                    - us-east-1a  # Only use nodes in zone 1a\n</code></pre>","tags":["scaling","scheduling","node affinity"]},{"location":"how-to-guides/scaling/use-node-affinity/#verify-node-placement","title":"Verify node placement","text":"<p>Check that pods are scheduled on the correct nodes:</p> <pre><code># Show pod-to-node mapping\nkubectl get pods -l performance-test-name=affinity-test -o wide\n\n# Check specific labels on nodes where pods are running\nkubectl get nodes -l workload-type=load-testing\n</code></pre> <p>Expected output showing pods only on labeled nodes:</p> <pre><code>NAME                           NODE                          STATUS\naffinity-test-master-abc123    node-1 (workload=load-test)   Running\naffinity-test-worker-0         node-1 (workload=load-test)   Running\naffinity-test-worker-1         node-2 (workload=load-test)   Running\naffinity-test-worker-2         node-3 (workload=load-test)   Running\n</code></pre>","tags":["scaling","scheduling","node affinity"]},{"location":"how-to-guides/scaling/use-node-affinity/#troubleshoot-scheduling-failures","title":"Troubleshoot scheduling failures","text":"<p>If pods remain in <code>Pending</code> state:</p> <pre><code># Check pod events\nkubectl describe pod &lt;pod-name&gt; | grep -A 10 \"Events:\"\n</code></pre> <p>Common issue:</p> <pre><code>Warning  FailedScheduling  No nodes are available that match all of the following predicates: NodeAffinity (3)\n</code></pre> <p>Causes:</p> <ol> <li>No nodes with matching labels:</li> </ol> <pre><code># Check labeled nodes exist\nkubectl get nodes -l workload-type=load-testing\n</code></pre> <p>Fix: Label at least one node.</p> <ol> <li>Insufficient capacity on labeled nodes:</li> </ol> <pre><code># Check node resources\nkubectl describe nodes -l workload-type=load-testing | grep -A 5 \"Allocated resources\"\n</code></pre> <p>Fix: Add more nodes with the label or reduce resource requests.</p> <ol> <li>Typo in label key or value:</li> </ol> <p>Verify label spelling matches exactly:</p> <pre><code>kubectl get nodes --show-labels | grep workload\n</code></pre>","tags":["scaling","scheduling","node affinity"]},{"location":"how-to-guides/scaling/use-node-affinity/#combine-with-tolerations","title":"Combine with tolerations","text":"<p>Often used together for dedicated node pools:</p> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: affinity-toleration-test\nspec:\n  image: locustio/locust:2.20.0\n  testFiles:\n    configMapRef: my-test\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://api.example.com\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 10\n  scheduling:\n    affinity:\n      nodeAffinity:\n        requiredDuringSchedulingIgnoredDuringExecution:\n          nodeSelectorTerms:\n            - matchExpressions:\n                - key: workload-type\n                  operator: In\n                  values:\n                    - load-testing\n    tolerations:\n      - key: dedicated\n        operator: Equal\n        value: load-testing\n        effect: NoSchedule\n</code></pre> <p>See Configure tolerations for details.</p>","tags":["scaling","scheduling","node affinity"]},{"location":"how-to-guides/scaling/use-node-affinity/#whats-next","title":"What's next","text":"<ul> <li>Configure tolerations \u2014 Schedule on tainted nodes (often used together)</li> <li>Use node selector \u2014 Simpler alternative for basic label matching</li> <li>Scale worker replicas \u2014 Calculate worker count for dedicated nodes</li> </ul>","tags":["scaling","scheduling","node affinity"]},{"location":"how-to-guides/scaling/use-node-selector/","title":"Use node selector for simple node targeting","text":"<p>Target specific nodes using simple label matching with node selector, the easiest way to control pod placement.</p>","tags":["scaling","scheduling","node selector"]},{"location":"how-to-guides/scaling/use-node-selector/#prerequisites","title":"Prerequisites","text":"<ul> <li>Locust Kubernetes Operator installed</li> <li>Access to label cluster nodes</li> </ul>","tags":["scaling","scheduling","node selector"]},{"location":"how-to-guides/scaling/use-node-selector/#when-to-use-node-selector","title":"When to use node selector","text":"<p>Use node selector when:</p> <ul> <li>You need simple label matching (key=value)</li> <li>All conditions are AND (all labels must match)</li> <li>You want the simplest configuration</li> </ul> <p>Use node affinity when:</p> <ul> <li>You need OR logic (match any of multiple labels)</li> <li>You need soft preferences (preferred but not required)</li> <li>You need complex expressions (In, NotIn, Exists, DoesNotExist)</li> </ul> <p>See Use node affinity for advanced scenarios.</p>","tags":["scaling","scheduling","node selector"]},{"location":"how-to-guides/scaling/use-node-selector/#label-your-nodes","title":"Label your nodes","text":"<p>Add labels to nodes:</p> <pre><code># Label for SSD storage\nkubectl label nodes node-1 disktype=ssd\n\n# Label for performance environment\nkubectl label nodes node-1 environment=performance\n\n# Label multiple nodes\nkubectl label nodes node-2 disktype=ssd environment=performance\nkubectl label nodes node-3 disktype=ssd environment=performance\n\n# Verify labels\nkubectl get nodes --show-labels | grep disktype\n</code></pre>","tags":["scaling","scheduling","node selector"]},{"location":"how-to-guides/scaling/use-node-selector/#configure-node-selector","title":"Configure node selector","text":"<p>Add <code>scheduling.nodeSelector</code> to your LocustTest CR:</p> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: nodeselector-test\nspec:\n  image: locustio/locust:2.20.0\n  testFiles:\n    configMapRef: my-test\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://api.example.com\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 5\n  scheduling:\n    nodeSelector:\n      disktype: ssd  # Only schedule on nodes with this label\n</code></pre> <p>Apply the configuration:</p> <pre><code>kubectl apply -f locusttest-nodeselector.yaml\n</code></pre>","tags":["scaling","scheduling","node selector"]},{"location":"how-to-guides/scaling/use-node-selector/#multiple-labels-and-logic","title":"Multiple labels (AND logic)","text":"<p>Require multiple labels on nodes:</p> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: multi-label-selector\nspec:\n  image: locustio/locust:2.20.0\n  testFiles:\n    configMapRef: my-test\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://api.example.com\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 10\n  scheduling:\n    nodeSelector:\n      disktype: ssd              # Must have SSD\n      environment: performance   # AND must be performance environment\n</code></pre> <p>Nodes must have both labels to be selected.</p>","tags":["scaling","scheduling","node selector"]},{"location":"how-to-guides/scaling/use-node-selector/#example-high-performance-nodes","title":"Example: High-performance nodes","text":"<p>Target high-performance node pool:</p> <p>1. Label your high-performance nodes:</p> <pre><code>kubectl label nodes perf-node-1 performance-tier=high\nkubectl label nodes perf-node-2 performance-tier=high\nkubectl label nodes perf-node-3 performance-tier=high\n</code></pre> <p>2. Configure test to use labeled nodes:</p> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: high-perf-test\nspec:\n  image: locustio/locust:2.20.0\n  testFiles:\n    configMapRef: performance-test\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://api.example.com\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 20\n  scheduling:\n    nodeSelector:\n      performance-tier: high  # Only high-performance nodes\n</code></pre>","tags":["scaling","scheduling","node selector"]},{"location":"how-to-guides/scaling/use-node-selector/#example-aws-instance-type-targeting","title":"Example: AWS instance type targeting","text":"<p>Target specific EC2 instance types:</p> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: aws-instance-test\nspec:\n  image: locustio/locust:2.20.0\n  testFiles:\n    configMapRef: my-test\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://api.example.com\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 10\n  scheduling:\n    nodeSelector:\n      node.kubernetes.io/instance-type: c5.2xlarge  # Compute-optimized\n</code></pre> <p>Note: This only matches one instance type. For multiple types, use node affinity with <code>In</code> operator.</p>","tags":["scaling","scheduling","node selector"]},{"location":"how-to-guides/scaling/use-node-selector/#example-zone-specific-deployment","title":"Example: Zone-specific deployment","text":"<p>Keep tests in a specific availability zone:</p> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: zone-specific-test\nspec:\n  image: locustio/locust:2.20.0\n  testFiles:\n    configMapRef: my-test\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://api.example.com\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 10\n  scheduling:\n    nodeSelector:\n      topology.kubernetes.io/zone: us-east-1a  # Only zone 1a\n</code></pre>","tags":["scaling","scheduling","node selector"]},{"location":"how-to-guides/scaling/use-node-selector/#verify-node-placement","title":"Verify node placement","text":"<p>Check that pods are scheduled on the correct nodes:</p> <pre><code># Show pod-to-node mapping\nkubectl get pods -l performance-test-name=nodeselector-test -o wide\n\n# Check labels on nodes where pods are running\nNODE=$(kubectl get pod -l performance-test-pod-name=nodeselector-test-master -o jsonpath='{.items[0].spec.nodeName}')\nkubectl get node $NODE --show-labels | grep disktype\n</code></pre> <p>Expected: All pods running on nodes with matching labels.</p>","tags":["scaling","scheduling","node selector"]},{"location":"how-to-guides/scaling/use-node-selector/#troubleshoot-scheduling-failures","title":"Troubleshoot scheduling failures","text":"<p>If pods remain <code>Pending</code>:</p> <pre><code>kubectl describe pod &lt;pod-name&gt; | grep -A 10 \"Events:\"\n</code></pre> <p>Common issue:</p> <pre><code>Warning  FailedScheduling  0/5 nodes are available: 5 node(s) didn't match Pod's node affinity/selector\n</code></pre> <p>Causes:</p> <ol> <li>No nodes with matching labels:</li> </ol> <pre><code># Check if any nodes have the label\nkubectl get nodes -l disktype=ssd\n</code></pre> <p>Fix: Label at least one node.</p> <ol> <li>Typo in label key or value:</li> </ol> <pre><code># Check actual labels\nkubectl get nodes --show-labels | grep disktype\n</code></pre> <p>Ensure spelling and case match exactly.</p> <ol> <li>Insufficient capacity on labeled nodes:</li> </ol> <pre><code># Check node resources\nkubectl describe node -l disktype=ssd | grep -A 5 \"Allocated resources\"\n</code></pre> <p>Fix: Add more labeled nodes or reduce resource requests.</p>","tags":["scaling","scheduling","node selector"]},{"location":"how-to-guides/scaling/use-node-selector/#compare-with-node-affinity","title":"Compare with node affinity","text":"<p>Node selector:</p> <pre><code>scheduling:\n  nodeSelector:\n    disktype: ssd\n</code></pre> <p>Equivalent node affinity:</p> <pre><code>scheduling:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n          - matchExpressions:\n              - key: disktype\n                operator: In\n                values:\n                  - ssd\n</code></pre> <p>Node selector is simpler. Node affinity is more powerful.</p>","tags":["scaling","scheduling","node selector"]},{"location":"how-to-guides/scaling/use-node-selector/#combine-with-other-scheduling","title":"Combine with other scheduling","text":"<p>Node selector works with tolerations:</p> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: selector-toleration-test\nspec:\n  image: locustio/locust:2.20.0\n  testFiles:\n    configMapRef: my-test\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://api.example.com\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 10\n  scheduling:\n    nodeSelector:\n      disktype: ssd  # Simple label matching\n    tolerations:\n      - key: dedicated\n        operator: Equal\n        value: load-testing\n        effect: NoSchedule  # Tolerate taint on SSD nodes\n</code></pre> <p>See Configure tolerations for details.</p>","tags":["scaling","scheduling","node selector"]},{"location":"how-to-guides/scaling/use-node-selector/#whats-next","title":"What's next","text":"<ul> <li>Use node affinity \u2014 Complex scheduling with OR logic and preferences</li> <li>Configure tolerations \u2014 Schedule on tainted nodes</li> <li>Scale worker replicas \u2014 Calculate capacity for labeled nodes</li> </ul>","tags":["scaling","scheduling","node selector"]},{"location":"how-to-guides/security/configure-pod-security/","title":"Configure pod security settings","text":"<p>The operator applies security settings to all test pods by default. This guide explains the default security posture, RBAC requirements, and network isolation options.</p>","tags":["security","pod security","rbac","network policy","hardening"]},{"location":"how-to-guides/security/configure-pod-security/#default-security-context","title":"Default security context","text":"<p>The operator automatically applies a security context to all Locust test pods (master and worker). This security context meets Kubernetes Pod Security Standards \"baseline\" profile requirements.</p>","tags":["security","pod security","rbac","network policy","hardening"]},{"location":"how-to-guides/security/configure-pod-security/#security-settings-applied","title":"Security settings applied","text":"<pre><code># Applied to all test pods by default\nsecurityContext:\n  seccompProfile:\n    type: RuntimeDefault                # Use runtime's default seccomp profile\n</code></pre> <p>Why this default:</p> <ul> <li>seccompProfile: RuntimeDefault \u2014 Uses the container runtime's default seccomp profile to restrict system calls.</li> </ul> <p>Non-root execution</p> <p>The official Locust image (<code>locustio/locust</code>) runs as a non-root user by default (UID 1000), but the operator does not explicitly set <code>runAsNonRoot: true</code> on the pod security context. If you require enforced non-root execution, see the restricted profile section below.</p>","tags":["security","pod security","rbac","network policy","hardening"]},{"location":"how-to-guides/security/configure-pod-security/#why-not-readonlyrootfilesystem","title":"Why NOT readOnlyRootFilesystem","text":"<p>The operator does NOT set <code>readOnlyRootFilesystem: true</code> because:</p> <ul> <li>Locust needs to write to <code>/tmp</code> for temporary files</li> <li>Python pip may need cache directories for plugin installation</li> <li>The locustfile may write temporary data during test execution</li> </ul> <p>If your test doesn't require write access, you can customize the security context (see below).</p>","tags":["security","pod security","rbac","network policy","hardening"]},{"location":"how-to-guides/security/configure-pod-security/#customizing-security-context","title":"Customizing security context","text":"<p>Override the default security context per test if needed:</p> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: hardened-test\nspec:\n  image: locustio/locust:2.20.0\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://example.com\"\n    # Custom security context for master pod (if needed)\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 3\n    # Custom security context for worker pod (if needed)\n</code></pre> <p>Note</p> <p>The v2 API does not currently expose securityContext at the CR level. The operator uses defaults from Helm values. To customize per-test, you would need to modify the operator's Helm chart values.</p>","tags":["security","pod security","rbac","network policy","hardening"]},{"location":"how-to-guides/security/configure-pod-security/#rbac-best-practices","title":"RBAC best practices","text":"","tags":["security","pod security","rbac","network policy","hardening"]},{"location":"how-to-guides/security/configure-pod-security/#operator-rbac","title":"Operator RBAC","text":"<p>The operator's service account needs permissions to manage LocustTest resources and create test infrastructure.</p> <p>What the operator needs:</p> Resource Verbs Purpose <code>locusttests</code> get, list, watch, update, patch Watch CRs and reconcile state <code>locusttests/status</code> get, update, patch Report test status <code>locusttests/finalizers</code> update Manage deletion lifecycle <code>configmaps</code> get, list, watch Read test files and library code <code>secrets</code> get, list, watch Read credentials for env injection <code>services</code> get, list, watch, create, delete Master service for worker communication <code>pods</code> get, list, watch Monitor pod health for status reporting <code>events</code> create, patch Report status changes and errors <code>jobs</code> get, list, watch, create, delete Master and worker pods (immutable pattern) <p>Read-only Secret access</p> <p>The operator never creates or modifies ConfigMaps or Secrets. It only reads them to populate environment variables and volume mounts in test pods.</p> <p>ClusterRole vs Role:</p> <p>The operator supports two RBAC modes (configured via Helm):</p> Mode Scope Use case ClusterRole (default) All namespaces Multi-tenant platform, centralized operator Role Single namespace Security-sensitive environments, namespace isolation <p>Configure in Helm values:</p> <pre><code># values.yaml\nk8s:\n  clusterRole:\n    enabled: false  # Restrict to operator namespace only\n</code></pre>","tags":["security","pod security","rbac","network policy","hardening"]},{"location":"how-to-guides/security/configure-pod-security/#test-pod-rbac","title":"Test pod RBAC","text":"<p>Test pods run as non-root and do not get elevated privileges. By default:</p> <ul> <li>No service account token is mounted (unless you explicitly set <code>serviceAccountName</code>)</li> <li>No Kubernetes API access</li> <li>No elevated Linux capabilities</li> </ul> <p>If your test needs Kubernetes API access:</p> <ol> <li> <p>Create a service account with minimal permissions:    <pre><code>kubectl create serviceaccount locust-test-runner\n</code></pre></p> </li> <li> <p>Grant only required permissions:    <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: locust-pod-reader\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\"]\n    verbs: [\"get\", \"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: locust-pod-reader-binding\nsubjects:\n  - kind: ServiceAccount\n    name: locust-test-runner\nroleRef:\n  kind: Role\n  name: locust-pod-reader\n  apiGroup: rbac.authorization.k8s.io\n</code></pre></p> </li> <li> <p>Reference in LocustTest CR (not yet supported in v2 API).</p> </li> </ol> <p>Least privilege</p> <p>Only grant the minimum permissions your test needs. Avoid <code>cluster-admin</code> or broad wildcard permissions.</p>","tags":["security","pod security","rbac","network policy","hardening"]},{"location":"how-to-guides/security/configure-pod-security/#user-rbac-for-test-creators","title":"User RBAC for test creators","text":"<p>Users who create and manage LocustTest CRs need different permissions than the operator.</p> <p>Minimal test creator role:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: locusttest-creator\n  namespace: performance-testing\nrules:\n  # Create and manage LocustTest CRs\n  - apiGroups: [\"locust.io\"]\n    resources: [\"locusttests\"]\n    verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"]\n\n  # Create ConfigMaps for test files\n  - apiGroups: [\"\"]\n    resources: [\"configmaps\"]\n    verbs: [\"get\", \"list\", \"create\", \"update\", \"delete\"]\n\n  # View pods for debugging\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"list\"]\n\n  # View events for status monitoring\n  - apiGroups: [\"\"]\n    resources: [\"events\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n</code></pre> <p>Verify user permissions:</p> <pre><code># Check if user can create LocustTest\nkubectl auth can-i create locusttests --as jane.doe\n\n# Check if user can read secrets (should be \"no\")\nkubectl auth can-i get secrets --as jane.doe\n</code></pre>","tags":["security","pod security","rbac","network policy","hardening"]},{"location":"how-to-guides/security/configure-pod-security/#network-isolation","title":"Network isolation","text":"<p>Use NetworkPolicies to restrict traffic to/from test pods.</p>","tags":["security","pod security","rbac","network policy","hardening"]},{"location":"how-to-guides/security/configure-pod-security/#allow-only-necessary-traffic","title":"Allow only necessary traffic","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: locust-test-isolation\n  namespace: performance-testing\nspec:\n  podSelector:\n    matchLabels:\n      performance-test-name: my-test    # Apply to specific test\n  policyTypes:\n    - Ingress\n    - Egress\n\n  ingress:\n    # Allow communication between pods in the same test\n    - from:\n        - podSelector:\n            matchLabels:\n              performance-test-name: my-test\n      ports:\n        - port: 5557                    # Worker -&gt; Master\n          protocol: TCP\n\n  egress:\n    # Allow DNS resolution\n    - to:\n        - namespaceSelector:\n            matchLabels:\n              kubernetes.io/metadata.name: kube-system\n      ports:\n        - port: 53\n          protocol: UDP\n\n    # Allow traffic to target system under test\n    - to:\n        - podSelector: {}               # All pods (adjust as needed)\n      ports:\n        - port: 80\n          protocol: TCP\n        - port: 443\n          protocol: TCP\n\n    # Allow traffic to OTel Collector (if using OpenTelemetry)\n    - to:\n        - namespaceSelector:\n            matchLabels:\n              kubernetes.io/metadata.name: monitoring\n        - podSelector:\n            matchLabels:\n              app: otel-collector\n      ports:\n        - port: 4317                    # OTLP gRPC\n          protocol: TCP\n</code></pre> <p>What this policy allows:</p> <ul> <li>Ingress: Only communication between pods in the same test (master \u2194 workers)</li> <li>Egress: DNS, target system (HTTP/HTTPS), OTel Collector</li> </ul> <p>What this policy blocks:</p> <ul> <li>Cross-test communication</li> <li>External egress except explicitly allowed</li> <li>Ingress from outside the test</li> </ul>","tags":["security","pod security","rbac","network policy","hardening"]},{"location":"how-to-guides/security/configure-pod-security/#verification","title":"Verification","text":"<p>Check if NetworkPolicy is active:</p> <pre><code>kubectl get networkpolicy -n performance-testing\n</code></pre> <p>Test connectivity from a worker pod:</p> <pre><code># Get a worker pod\nPOD=$(kubectl get pods -l performance-test-pod-name=my-test-worker -o jsonpath='{.items[0].metadata.name}')\n\n# Test target system connectivity\nkubectl exec $POD -- curl -I https://api.example.com\n\n# Test master connectivity\nkubectl exec $POD -- nc -zv my-test-master 5557\n\n# Test blocked traffic (should timeout or fail)\nkubectl exec $POD -- curl -I https://blocked-host.com --max-time 5\n</code></pre>","tags":["security","pod security","rbac","network policy","hardening"]},{"location":"how-to-guides/security/configure-pod-security/#networkpolicy-best-practices","title":"NetworkPolicy best practices","text":"<ol> <li> <p>Start with allow-all, then restrict: Test your application first, then add NetworkPolicies gradually.</p> </li> <li> <p>Allow DNS: Always allow egress to <code>kube-system</code> namespace port 53 for DNS resolution.</p> </li> <li> <p>Test-specific policies: Use <code>performance-test-name</code> label to isolate individual tests.</p> </li> <li> <p>Monitor denied traffic: Use a CNI that logs dropped packets (Calico, Cilium) to identify blocked traffic.</p> </li> <li> <p>Document exceptions: If you must allow broad egress, document why in the NetworkPolicy annotations.</p> </li> </ol>","tags":["security","pod security","rbac","network policy","hardening"]},{"location":"how-to-guides/security/configure-pod-security/#verification_1","title":"Verification","text":"","tags":["security","pod security","rbac","network policy","hardening"]},{"location":"how-to-guides/security/configure-pod-security/#check-pod-security-context","title":"Check pod security context","text":"<pre><code># Get pod security context\nkubectl get pod -l performance-test-name=my-test -o jsonpath='{.items[0].spec.securityContext}' | jq .\n</code></pre> <p>Expected output:</p> <pre><code>{\n  \"seccompProfile\": {\n    \"type\": \"RuntimeDefault\"\n  }\n}\n</code></pre>","tags":["security","pod security","rbac","network policy","hardening"]},{"location":"how-to-guides/security/configure-pod-security/#verify-non-root-execution","title":"Verify non-root execution","text":"<pre><code># Check which user the pod runs as\nPOD=$(kubectl get pods -l performance-test-pod-name=my-test-master -o jsonpath='{.items[0].metadata.name}')\nkubectl exec $POD -- id\n</code></pre> <p>Expected output:</p> <pre><code>uid=1000(locust) gid=1000(locust) groups=1000(locust)\n</code></pre> <p>If you see <code>uid=0(root)</code>, the pod is running as root (violation of security policy).</p>","tags":["security","pod security","rbac","network policy","hardening"]},{"location":"how-to-guides/security/configure-pod-security/#verify-rbac-permissions","title":"Verify RBAC permissions","text":"<pre><code># Check operator service account permissions\nkubectl auth can-i --list --as=system:serviceaccount:locust-operator-system:locust-k8s-operator\n\n# Check if test pod has Kubernetes API access (should be \"no\" by default)\nkubectl exec $POD -- curl -k https://kubernetes.default.svc\n</code></pre> <p>Expected: Connection refused or authentication error (test pods should NOT have API access by default).</p>","tags":["security","pod security","rbac","network policy","hardening"]},{"location":"how-to-guides/security/configure-pod-security/#pod-security-standards-compliance","title":"Pod Security Standards compliance","text":"<p>The operator's default security settings meet these Pod Security Standards profiles:</p> Profile Compliant Notes Baseline \u2705 Yes Seccomp profile satisfies baseline requirements Restricted \u26a0\ufe0f Partial Missing: <code>runAsNonRoot</code>, <code>allowPrivilegeEscalation=false</code>, <code>capabilities drop ALL</code> Privileged \u2705 Yes No restrictions <p>To meet \"restricted\" profile:</p> <p>Users would need to add the following settings (via Helm values customization):</p> <pre><code>securityContext:\n  runAsNonRoot: true                     # Add this\n  allowPrivilegeEscalation: false        # Add this\n  seccompProfile:\n    type: RuntimeDefault\n  capabilities:                          # Add this\n    drop:\n      - ALL\n</code></pre> <p>Currently, the operator meets \"baseline\" by default. For \"restricted\" compliance, customize the operator's Helm values.</p>","tags":["security","pod security","rbac","network policy","hardening"]},{"location":"how-to-guides/security/configure-pod-security/#related-guides","title":"Related guides","text":"<ul> <li>Inject secrets and configuration \u2014 Manage credentials for test pods</li> <li>Security Best Practices \u2014 Complete security guide (RBAC, secrets, external integrations)</li> <li>API Reference \u2014 LocustTest CR specification</li> </ul>","tags":["security","pod security","rbac","network policy","hardening"]},{"location":"how-to-guides/security/inject-secrets/","title":"Inject secrets and configuration into test pods","text":"<p>Inject credentials and configuration into Locust test pods without hardcoding them in test files. The operator provides four methods for injecting data.</p>","tags":["security","secrets","environment variables","configuration"]},{"location":"how-to-guides/security/inject-secrets/#method-1-configmap-environment-variables","title":"Method 1: ConfigMap environment variables","text":"<p>Inject all keys from a ConfigMap as environment variables with an optional prefix.</p> <p>Create a ConfigMap:</p> <pre><code>kubectl create configmap app-config \\\n  --from-literal=TARGET_HOST=https://api.example.com \\\n  --from-literal=LOG_LEVEL=INFO \\\n  --from-literal=TIMEOUT=30\n</code></pre> <p>Reference in LocustTest CR:</p> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: configmap-test\nspec:\n  image: locustio/locust:2.20.0\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://example.com\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 3\n  env:\n    configMapRefs:\n      - name: app-config          # ConfigMap name\n        prefix: \"APP_\"            # Prefix for all keys (optional)\n</code></pre> <p>Result: ConfigMap keys become environment variables with the prefix: - <code>TARGET_HOST</code> \u2192 <code>APP_TARGET_HOST</code> - <code>LOG_LEVEL</code> \u2192 <code>APP_LOG_LEVEL</code> - <code>TIMEOUT</code> \u2192 <code>APP_TIMEOUT</code></p> <p>Access in your locustfile:</p> <pre><code>import os\n\ntarget_host = os.getenv('APP_TARGET_HOST')\nlog_level = os.getenv('APP_LOG_LEVEL', 'INFO')\ntimeout = int(os.getenv('APP_TIMEOUT', '30'))\n</code></pre>","tags":["security","secrets","environment variables","configuration"]},{"location":"how-to-guides/security/inject-secrets/#method-2-secret-environment-variables","title":"Method 2: Secret environment variables","text":"<p>Inject all keys from a Secret as environment variables with an optional prefix.</p> <p>Create a Secret:</p> <pre><code>kubectl create secret generic api-credentials \\\n  --from-literal=API_TOKEN=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9... \\\n  --from-literal=API_KEY=sk_live_51H8... \\\n  --from-literal=DB_PASSWORD=secure-password-here\n</code></pre> <p>Reference in LocustTest CR:</p> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: secret-test\nspec:\n  image: locustio/locust:2.20.0\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://api.example.com\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 5\n  env:\n    secretRefs:\n      - name: api-credentials     # Secret name\n        prefix: \"\"                # No prefix (use key names directly)\n</code></pre> <p>Result: Secret keys become environment variables: - <code>API_TOKEN</code> \u2192 <code>API_TOKEN</code> - <code>API_KEY</code> \u2192 <code>API_KEY</code> - <code>DB_PASSWORD</code> \u2192 <code>DB_PASSWORD</code></p> <p>Access in your locustfile:</p> <pre><code>import os\n\napi_token = os.getenv('API_TOKEN')\napi_key = os.getenv('API_KEY')\ndb_password = os.getenv('DB_PASSWORD')\n</code></pre> <p>Secret values in pod specs</p> <p>Kubernetes injects Secret values as environment variables. They're visible in pod specs. Use RBAC to restrict access to pod definitions.</p>","tags":["security","secrets","environment variables","configuration"]},{"location":"how-to-guides/security/inject-secrets/#method-3-individual-variables","title":"Method 3: Individual variables","text":"<p>Define individual environment variables with literal values or references to ConfigMap/Secret keys. This gives you fine-grained control over which keys to inject.</p> <p>Create sources:</p> <pre><code>kubectl create configmap app-settings --from-literal=api-url=https://api.example.com\nkubectl create secret generic auth --from-literal=token=secret-token-here\n</code></pre> <p>Reference in LocustTest CR:</p> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: individual-vars-test\nspec:\n  image: locustio/locust:2.20.0\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://example.com\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 3\n  env:\n    variables:\n      # Literal value\n      - name: ENVIRONMENT\n        value: \"staging\"\n\n      # Reference to Secret key\n      - name: API_TOKEN\n        valueFrom:\n          secretKeyRef:\n            name: auth                    # Secret name\n            key: token                    # Key within Secret\n\n      # Reference to ConfigMap key\n      - name: API_URL\n        valueFrom:\n          configMapKeyRef:\n            name: app-settings            # ConfigMap name\n            key: api-url                  # Key within ConfigMap\n</code></pre> <p>Result: Three environment variables are injected: - <code>ENVIRONMENT=staging</code> (literal value) - <code>API_TOKEN=secret-token-here</code> (from Secret) - <code>API_URL=https://api.example.com</code> (from ConfigMap)</p> <p>Use cases: - Mix literal values with secrets/configs - Select specific keys from ConfigMaps/Secrets - Set defaults with fallback to secrets for sensitive values</p>","tags":["security","secrets","environment variables","configuration"]},{"location":"how-to-guides/security/inject-secrets/#method-4-secret-file-mounts","title":"Method 4: Secret file mounts","text":"<p>Mount secrets as files in the container filesystem. This is useful for: - TLS certificates - Credential files (JSON key files, kubeconfig, etc.) - Configuration files that must be read from disk</p> <p>Create a Secret from files:</p> <pre><code>kubectl create secret generic tls-certs \\\n  --from-file=ca.crt=path/to/ca.crt \\\n  --from-file=client.crt=path/to/client.crt \\\n  --from-file=client.key=path/to/client.key\n</code></pre> <p>Reference in LocustTest CR:</p> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: file-mount-test\nspec:\n  image: locustio/locust:2.20.0\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://api.example.com\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 3\n  env:\n    secretMounts:\n      - name: tls-certs                   # Secret name\n        mountPath: /etc/locust/certs      # Mount path in container\n        readOnly: true                    # Mount as read-only (recommended)\n</code></pre> <p>Result: Secret keys become files at mount path: - <code>/etc/locust/certs/ca.crt</code> - <code>/etc/locust/certs/client.crt</code> - <code>/etc/locust/certs/client.key</code></p> <p>Access in your locustfile:</p> <pre><code>import ssl\n\n# Create SSL context with mounted certificates\nssl_context = ssl.create_default_context(cafile='/etc/locust/certs/ca.crt')\nssl_context.load_cert_chain(\n    certfile='/etc/locust/certs/client.crt',\n    keyfile='/etc/locust/certs/client.key'\n)\n\n# Use in HTTP client\n# (implementation depends on your HTTP library)\n</code></pre>","tags":["security","secrets","environment variables","configuration"]},{"location":"how-to-guides/security/inject-secrets/#reserved-paths","title":"Reserved paths","text":"<p>The following paths are reserved and cannot be used for secret mounts:</p> Path Purpose Customizable via <code>/lotest/src/</code> Test script mount point <code>testFiles.srcMountPath</code> <code>/opt/locust/lib</code> Library mount point <code>testFiles.libMountPath</code> <p>If you customize <code>srcMountPath</code> or <code>libMountPath</code>, those custom paths become reserved instead.</p>","tags":["security","secrets","environment variables","configuration"]},{"location":"how-to-guides/security/inject-secrets/#combined-example","title":"Combined example","text":"<p>Use multiple injection methods together:</p> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: combined-injection\nspec:\n  image: locustio/locust:2.20.0\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://api.example.com\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 5\n  env:\n    # Method 1: ConfigMap environment variables\n    configMapRefs:\n      - name: app-config\n        prefix: \"APP_\"\n\n    # Method 2: Secret environment variables\n    secretRefs:\n      - name: api-credentials\n        prefix: \"\"\n\n    # Method 3: Individual variables\n    variables:\n      - name: ENVIRONMENT\n        value: \"production\"\n      - name: REGION\n        value: \"us-west-2\"\n      - name: SPECIAL_TOKEN\n        valueFrom:\n          secretKeyRef:\n            name: special-auth\n            key: token\n\n    # Method 4: Secret file mounts\n    secretMounts:\n      - name: tls-certs\n        mountPath: /etc/locust/certs\n        readOnly: true\n      - name: service-account-key\n        mountPath: /etc/locust/keys\n        readOnly: true\n</code></pre> <p>Result: - All keys from <code>app-config</code> ConfigMap with <code>APP_</code> prefix - All keys from <code>api-credentials</code> Secret (no prefix) - Literal values: <code>ENVIRONMENT</code>, <code>REGION</code> - Individual secret reference: <code>SPECIAL_TOKEN</code> - Files mounted at <code>/etc/locust/certs/</code> and <code>/etc/locust/keys/</code></p>","tags":["security","secrets","environment variables","configuration"]},{"location":"how-to-guides/security/inject-secrets/#verification","title":"Verification","text":"","tags":["security","secrets","environment variables","configuration"]},{"location":"how-to-guides/security/inject-secrets/#check-environment-variables","title":"Check environment variables","text":"<p>Verify that environment variables were injected into test pods:</p> <pre><code># Get a pod name\nPOD=$(kubectl get pods -l performance-test-name=combined-injection -o jsonpath='{.items[0].metadata.name}')\n\n# Check all environment variables\nkubectl exec $POD -- printenv | sort\n\n# Check specific prefix\nkubectl exec $POD -- printenv | grep \"APP_\"\n\n# Check specific variable\nkubectl exec $POD -- printenv API_TOKEN\n</code></pre>","tags":["security","secrets","environment variables","configuration"]},{"location":"how-to-guides/security/inject-secrets/#check-file-mounts","title":"Check file mounts","text":"<p>Verify that secret files were mounted:</p> <pre><code># List files in mount path\nkubectl exec $POD -- ls -la /etc/locust/certs/\n\n# Read file content (use with caution for sensitive data)\nkubectl exec $POD -- cat /etc/locust/certs/ca.crt\n</code></pre>","tags":["security","secrets","environment variables","configuration"]},{"location":"how-to-guides/security/inject-secrets/#troubleshooting","title":"Troubleshooting","text":"Problem Symptom Solution Pod stuck in <code>Pending</code> ConfigurationError condition Verify ConfigMap/Secret exists: <code>kubectl get configmap,secret</code> Environment variable missing Variable not in <code>printenv</code> output Check spelling of ConfigMap/Secret name and key File mount empty Directory exists but no files Verify Secret exists and has data: <code>kubectl get secret &lt;name&gt; -o yaml</code> Permission denied reading file <code>cat</code> fails with permission error Check <code>readOnly: true</code> and Secret file permissions <p>Check PodsHealthy condition:</p> <pre><code>kubectl get locusttest combined-injection -o jsonpath='{.status.conditions[?(@.type==\"PodsHealthy\")]}'\n</code></pre> <p>If <code>status=False</code> with reason <code>ConfigurationError</code>, the error message shows which ConfigMap or Secret is missing.</p>","tags":["security","secrets","environment variables","configuration"]},{"location":"how-to-guides/security/inject-secrets/#security-best-practices","title":"Security best practices","text":"<ol> <li> <p>Use Secrets for sensitive data: Never use ConfigMaps for passwords, tokens, or keys.</p> </li> <li> <p>Use RBAC to restrict Secret access: Limit who can read Secrets in your namespace:    <pre><code># Users should NOT have direct Secret access\n# Only the operator's service account needs it\n</code></pre></p> </li> <li> <p>Rotate secrets regularly: See Security Best Practices - Secret Rotation for the rotation process.</p> </li> <li> <p>Use External Secrets Operator: For production, sync secrets from external vaults (AWS Secrets Manager, HashiCorp Vault, etc.). See Security Best Practices - External Secrets.</p> </li> <li> <p>Prefer file mounts for certificates: Mount TLS certificates as files instead of environment variables (harder to accidentally log).</p> </li> <li> <p>Use read-only mounts: Always set <code>readOnly: true</code> for secret mounts to prevent accidental modification.</p> </li> </ol>","tags":["security","secrets","environment variables","configuration"]},{"location":"how-to-guides/security/inject-secrets/#related-guides","title":"Related guides","text":"<ul> <li>Mount volumes \u2014 Mount non-secret volumes (PVCs, ConfigMaps, emptyDir)</li> <li>Security Best Practices \u2014 RBAC, secret rotation, external secrets integration</li> <li>API Reference - EnvConfig \u2014 Complete env configuration reference</li> </ul>","tags":["security","secrets","environment variables","configuration"]},{"location":"tutorials/","title":"Tutorials","text":"<p>Learn to use the Locust Kubernetes Operator through progressive, hands-on tutorials. Each tutorial builds on the previous one, taking you from basic load testing to production-ready deployments.</p>","tags":["tutorials","learning","guides"]},{"location":"tutorials/#learning-path","title":"Learning Path","text":"<p>Follow these tutorials in order to build your expertise:</p>","tags":["tutorials","learning","guides"]},{"location":"tutorials/#1-your-first-load-test-10-minutes","title":"1. Your First Load Test (10 minutes)","text":"<p>Learn distributed testing fundamentals by building a realistic e-commerce test from scratch.</p> <p>What you'll learn: - How Locust master and worker pods communicate - Writing multi-task test scripts with realistic user behavior - Configuring test parameters (users, spawn rate, duration) - Monitoring test progress and interpreting results</p>","tags":["tutorials","learning","guides"]},{"location":"tutorials/#2-cicd-integration-15-minutes","title":"2. CI/CD Integration (15 minutes)","text":"<p>Automate load tests in your deployment pipeline.</p> <p>What you'll learn: - Running tests as part of CI/CD workflows - Validating performance before production deployments - Extracting test results for automated decisions - Handling test failures and rollback scenarios</p>","tags":["tutorials","learning","guides"]},{"location":"tutorials/#3-production-deployment-20-minutes","title":"3. Production Deployment (20 minutes)","text":"<p>Configure production-grade load tests with resource limits, monitoring, and security.</p> <p>What you'll learn: - Setting resource requests and limits for stable tests - Integrating with Prometheus for metrics - Securing test workloads - Scaling to thousands of users</p>","tags":["tutorials","learning","guides"]},{"location":"tutorials/#prerequisites","title":"Prerequisites","text":"<p>Before starting the tutorials, complete the Quick Start guide to ensure you have:</p> <ul> <li>A working Kubernetes cluster</li> <li>The Locust Kubernetes Operator installed</li> <li>kubectl and Helm configured</li> <li>Basic understanding of Kubernetes and HTTP</li> </ul>","tags":["tutorials","learning","guides"]},{"location":"tutorials/#need-help","title":"Need Help?","text":"<ul> <li>Troubleshooting: See common issues and solutions in the FAQ</li> <li>API Reference: Complete field documentation in the API Reference</li> </ul>","tags":["tutorials","learning","guides"]},{"location":"tutorials/ci-cd-integration/","title":"CI/CD Integration (15 minutes)","text":"<p>Automate your load tests to run on every deployment or on a schedule.</p>","tags":["tutorial","ci-cd","automation","github-actions"]},{"location":"tutorials/ci-cd-integration/#what-youll-learn","title":"What you'll learn","text":"<ul> <li>How to run load tests in CI/CD pipelines</li> <li>How to create unique test runs per pipeline execution</li> <li>How to collect and store test results</li> <li>How to fail a pipeline on performance regression</li> </ul>","tags":["tutorial","ci-cd","automation","github-actions"]},{"location":"tutorials/ci-cd-integration/#prerequisites","title":"Prerequisites","text":"<ul> <li>Completed the Your First Load Test tutorial</li> <li>A Kubernetes cluster accessible from CI (kubeconfig or service account)</li> <li>GitHub repository (for GitHub Actions example)</li> </ul>","tags":["tutorial","ci-cd","automation","github-actions"]},{"location":"tutorials/ci-cd-integration/#the-scenario","title":"The scenario","text":"<p>You want nightly load tests against your staging environment, plus on-demand tests before releases. Tests should fail the pipeline if error rate exceeds 1%.</p>","tags":["tutorial","ci-cd","automation","github-actions"]},{"location":"tutorials/ci-cd-integration/#step-1-prepare-the-test-script","title":"Step 1: Prepare the test script","text":"<p>We'll reuse the <code>ecommerce_test.py</code> from Tutorial 1. Store it in your repository:</p> <pre><code>your-repo/\n\u251c\u2500\u2500 .github/\n\u2502   \u2514\u2500\u2500 workflows/\n\u2502       \u2514\u2500\u2500 load-test.yaml\n\u2514\u2500\u2500 tests/\n    \u2514\u2500\u2500 locust/\n        \u2514\u2500\u2500 ecommerce_test.py\n</code></pre> <p>The test script should be checked into your repository at <code>tests/locust/ecommerce_test.py</code> (same content from Tutorial 1). This ensures version control and consistency across pipeline runs.</p>","tags":["tutorial","ci-cd","automation","github-actions"]},{"location":"tutorials/ci-cd-integration/#step-2-create-the-github-actions-workflow","title":"Step 2: Create the GitHub Actions workflow","text":"<p>Create <code>.github/workflows/load-test.yaml</code>:</p> <pre><code>name: Nightly Load Test\n\non:\n  schedule:\n    - cron: '0 2 * * 1'  # Every Monday at 2 AM UTC\n  workflow_dispatch:  # Allow manual trigger from GitHub UI\n\njobs:\n  load-test:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@v4\n\n      - name: Set up kubectl\n        uses: azure/setup-kubectl@v3\n        with:\n          version: 'v1.28.0'\n\n      - name: Configure kubeconfig\n        run: |\n          # Create .kube directory\n          mkdir -p $HOME/.kube\n          # Write kubeconfig from GitHub secret\n          echo \"${{ secrets.KUBECONFIG }}\" &gt; $HOME/.kube/config\n          # Verify connectivity\n          kubectl cluster-info\n\n      - name: Create/update test ConfigMap\n        run: |\n          # Use --dry-run + kubectl apply for idempotency\n          kubectl create configmap ecommerce-test \\\n            --from-file=tests/locust/ecommerce_test.py \\\n            --dry-run=client -o yaml | kubectl apply -f -\n\n      - name: Deploy LocustTest with unique name\n        run: |\n          # Generate unique test name with timestamp\n          TEST_NAME=\"ecommerce-ci-$(date +%Y%m%d-%H%M%S)\"\n\n          kubectl apply -f - &lt;&lt;EOF\n          apiVersion: locust.io/v2\n          kind: LocustTest\n          metadata:\n            name: ${TEST_NAME}\n          spec:\n            image: locustio/locust:2.20.0\n            testFiles:\n              configMapRef: ecommerce-test\n            master:\n              command: |\n                --locustfile /lotest/src/ecommerce_test.py\n                --host https://api.staging.example.com\n                --users 100\n                --spawn-rate 10\n                --run-time 5m\n            worker:\n              command: \"--locustfile /lotest/src/ecommerce_test.py\"\n              replicas: 5\n          EOF\n\n          # Store test name for later steps\n          echo \"TEST_NAME=${TEST_NAME}\" &gt;&gt; $GITHUB_ENV\n\n      - name: Wait for test completion\n        run: |\n          # Wait up to 10 minutes for test to succeed\n          kubectl wait --for=jsonpath='{.status.phase}'=Succeeded \\\n            locusttest/${TEST_NAME} --timeout=10m\n\n      - name: Collect test results\n        if: always()  # Run even if test fails\n        run: |\n          # Get master pod logs\n          kubectl logs job/${TEST_NAME}-master &gt; results.log\n\n          # Get test status YAML\n          kubectl get locusttest ${TEST_NAME} -o yaml &gt; test-status.yaml\n\n          # Display summary\n          echo \"=== Test Summary ===\"\n          kubectl get locusttest ${TEST_NAME}\n\n      - name: Check for performance regression\n        run: |\n          # Extract final statistics from master logs\n          FAILURE_RATE=$(kubectl logs job/${TEST_NAME}-master | \\\n            grep -oP 'Total.*Failures.*\\K[\\d.]+%' | tail -1 | sed 's/%//')\n\n          echo \"Failure rate: ${FAILURE_RATE}%\"\n\n          # Fail pipeline if error rate &gt; 1%\n          if (( $(echo \"$FAILURE_RATE &gt; 1.0\" | bc -l) )); then\n            echo \"ERROR: Failure rate ${FAILURE_RATE}% exceeds threshold of 1%\"\n            exit 1\n          fi\n\n          echo \"\u2713 Performance acceptable: ${FAILURE_RATE}% failures\"\n\n      - name: Upload test artifacts\n        if: always()\n        uses: actions/upload-artifact@v4\n        with:\n          name: load-test-results-${{ env.TEST_NAME }}\n          path: |\n            results.log\n            test-status.yaml\n          retention-days: 30\n</code></pre> <p>Key workflow features:</p> <ul> <li>Scheduled execution: <code>cron: '0 2 * * 1'</code> runs every Monday at 2 AM</li> <li>Manual trigger: <code>workflow_dispatch</code> allows on-demand runs from GitHub UI</li> <li>Unique test names: <code>$(date +%Y%m%d-%H%M%S)</code> prevents name conflicts</li> <li>Idempotent ConfigMap: <code>--dry-run=client -o yaml | kubectl apply</code> updates existing ConfigMap</li> <li>Result collection: Logs and YAML saved as GitHub artifacts</li> <li>Regression detection: Pipeline fails if error rate exceeds 1%</li> </ul>","tags":["tutorial","ci-cd","automation","github-actions"]},{"location":"tutorials/ci-cd-integration/#step-3-configure-github-secrets","title":"Step 3: Configure GitHub secrets","text":"<p>Your workflow needs a kubeconfig to access the cluster. Add it as a GitHub secret:</p> <ol> <li> <p>Get your kubeconfig:    <pre><code>cat ~/.kube/config | base64\n</code></pre></p> </li> <li> <p>In GitHub: Go to Settings \u2192 Secrets and variables \u2192 Actions \u2192 New repository secret</p> </li> <li> <p>Create secret <code>KUBECONFIG</code> with the base64-encoded content</p> </li> </ol> <p>Security note: For production, use a service account with minimal permissions instead of full admin kubeconfig.</p>","tags":["tutorial","ci-cd","automation","github-actions"]},{"location":"tutorials/ci-cd-integration/#step-4-run-and-verify","title":"Step 4: Run and verify","text":"","tags":["tutorial","ci-cd","automation","github-actions"]},{"location":"tutorials/ci-cd-integration/#trigger-the-workflow-manually","title":"Trigger the workflow manually","text":"<ol> <li>Go to Actions tab in GitHub</li> <li>Select Nightly Load Test workflow</li> <li>Click Run workflow \u2192 Run workflow</li> </ol>","tags":["tutorial","ci-cd","automation","github-actions"]},{"location":"tutorials/ci-cd-integration/#monitor-execution","title":"Monitor execution","text":"<p>Watch the workflow run in real-time. Check each step's output:</p> <ul> <li>\u2713 ConfigMap created/updated</li> <li>\u2713 LocustTest deployed with unique name</li> <li>\u2713 Test completed successfully</li> <li>\u2713 Performance within acceptable limits</li> </ul>","tags":["tutorial","ci-cd","automation","github-actions"]},{"location":"tutorials/ci-cd-integration/#check-artifacts","title":"Check artifacts","text":"<p>After completion (or failure), download artifacts:</p> <ol> <li>Click on the workflow run</li> <li>Scroll to Artifacts section</li> <li>Download <code>load-test-results-ecommerce-ci-YYYYMMDD-HHMMSS.zip</code></li> </ol> <p>The artifact contains: - <code>results.log</code> \u2014 Full Locust master output with statistics - <code>test-status.yaml</code> \u2014 Complete LocustTest CR status</p>","tags":["tutorial","ci-cd","automation","github-actions"]},{"location":"tutorials/ci-cd-integration/#step-5-make-tests-fail-on-regression","title":"Step 5: Make tests fail on regression","text":"<p>The workflow already includes regression detection in the \"Check for performance regression\" step. It:</p> <ol> <li>Extracts error rate from master logs using <code>grep</code></li> <li>Compares to threshold (1% in this example)</li> <li>Fails pipeline with <code>exit 1</code> if threshold exceeded</li> </ol> <p>Customizing thresholds:</p> <pre><code># Fail on error rate &gt; 1%\nif (( $(echo \"$FAILURE_RATE &gt; 1.0\" | bc -l) )); then\n  exit 1\nfi\n\n# Or fail on response time &gt; 500ms\nAVG_RESPONSE=$(kubectl logs job/${TEST_NAME}-master | \\\n  grep -oP 'Average response time.*\\K[\\d.]+' | tail -1)\nif (( $(echo \"$AVG_RESPONSE &gt; 500\" | bc -l) )); then\n  exit 1\nfi\n</code></pre>","tags":["tutorial","ci-cd","automation","github-actions"]},{"location":"tutorials/ci-cd-integration/#what-you-learned","title":"What you learned","text":"<p>\u2713 How to run Kubernetes-based load tests in CI/CD pipelines \u2713 How to create unique test names for traceability \u2713 How to collect and store test results as artifacts \u2713 How to fail pipelines on performance regression \u2713 How to configure scheduled and manual test execution</p>","tags":["tutorial","ci-cd","automation","github-actions"]},{"location":"tutorials/ci-cd-integration/#next-steps","title":"Next steps","text":"<ul> <li>Production Deployment \u2014 Configure production-grade load tests</li> <li>Configure resources \u2014 Optimize pod resource allocation</li> <li>Set up OpenTelemetry \u2014 Export metrics for long-term analysis</li> </ul>","tags":["tutorial","ci-cd","automation","github-actions"]},{"location":"tutorials/first-load-test/","title":"Your First Load Test (10 minutes)","text":"<p>Learn how distributed load testing works by building a realistic test from scratch. You'll create an e-commerce scenario that simulates 100 users browsing products and viewing details.</p>","tags":["tutorial","distributed testing","load testing","beginners"]},{"location":"tutorials/first-load-test/#what-youll-learn","title":"What you'll learn","text":"<ul> <li>How Locust master and worker pods communicate and distribute load</li> <li>How to write realistic test scripts with multiple tasks and weighted behavior</li> <li>How to configure test parameters for meaningful results</li> <li>How to monitor test progress and interpret statistics</li> </ul>","tags":["tutorial","distributed testing","load testing","beginners"]},{"location":"tutorials/first-load-test/#prerequisites","title":"Prerequisites","text":"<ul> <li>Completed the Quick Start guide</li> <li>Basic understanding of HTTP and REST APIs</li> <li>Kubernetes cluster with the operator installed</li> </ul>","tags":["tutorial","distributed testing","load testing","beginners"]},{"location":"tutorials/first-load-test/#the-scenario","title":"The scenario","text":"<p>You're testing an e-commerce API before a big sale. You need to verify it can handle 100 simultaneous users over 5 minutes, with users primarily browsing products (75% of traffic) and occasionally viewing product details (25% of traffic).</p> <p>This simulates realistic user behavior - most users browse, fewer drill into specific items.</p>","tags":["tutorial","distributed testing","load testing","beginners"]},{"location":"tutorials/first-load-test/#step-1-write-the-test-script","title":"Step 1: Write the test script","text":"<p>Create a test script that simulates realistic shopping behavior:</p> <pre><code>cat &gt; ecommerce_test.py &lt;&lt; 'EOF'\nfrom locust import HttpUser, task, between\n\nclass ShopperUser(HttpUser):\n    # Wait 1-3 seconds between tasks to simulate realistic user pacing\n    wait_time = between(1, 3)\n\n    @task(3)  # This task runs 3x more often (75% of requests)\n    def browse_products(self):\n        \"\"\"Browse the product catalog.\"\"\"\n        # The name parameter helps identify requests in statistics\n        self.client.get(\n            \"/anything/products\",\n            name=\"GET /products\"\n        )\n\n    @task(1)  # This task runs 1x as often (25% of requests)\n    def view_product_detail(self):\n        \"\"\"View details for a specific product.\"\"\"\n        # Simulate viewing product ID 42\n        self.client.get(\n            \"/anything/products/42\",\n            name=\"GET /products/:id\"\n        )\nEOF\n</code></pre>","tags":["tutorial","distributed testing","load testing","beginners"]},{"location":"tutorials/first-load-test/#whats-happening-here","title":"What's happening here","text":"<ul> <li><code>HttpUser</code>: Base class for simulating HTTP clients. Each instance represents one user.</li> <li><code>wait_time = between(1, 3)</code>: Adds realistic pauses between requests. Real users don't hammer APIs continuously.</li> <li><code>@task(3)</code> and <code>@task(1)</code>: Task weights control distribution. Weight 3 means \"run 3x as often as weight 1\", giving us 75%/25% split.</li> <li><code>name</code> parameter: Groups similar URLs (like <code>/products/42</code>, <code>/products/99</code>) into one statistic row. Without this, you'd see hundreds of separate rows.</li> </ul> <p>We're using <code>https://httpbin.org/anything</code> as a mock API - it accepts any request and returns 200, perfect for learning without deploying a real e-commerce backend.</p>","tags":["tutorial","distributed testing","load testing","beginners"]},{"location":"tutorials/first-load-test/#step-2-deploy-the-test","title":"Step 2: Deploy the test","text":"<p>First, create the ConfigMap:</p> <pre><code>kubectl create configmap ecommerce-test --from-file=ecommerce_test.py\n</code></pre> <p>Now create the LocustTest resource:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: ecommerce-load\nspec:\n  image: locustio/locust:2.20.0\n  testFiles:\n    configMapRef: ecommerce-test\n  master:\n    # 100 users, spawn 10 per second, run for 5 minutes\n    command: \"--locustfile /lotest/src/ecommerce_test.py --host https://httpbin.org --users 100 --spawn-rate 10 --run-time 5m\"\n  worker:\n    command: \"--locustfile /lotest/src/ecommerce_test.py\"\n    replicas: 5  # Distribute 100 users across 5 workers (~20 users each)\nEOF\n</code></pre>","tags":["tutorial","distributed testing","load testing","beginners"]},{"location":"tutorials/first-load-test/#understanding-the-configuration","title":"Understanding the configuration","text":"<ul> <li><code>--users 100</code>: Total simulated users across all workers</li> <li><code>--spawn-rate 10</code>: Add 10 users per second until reaching 100 (takes 10 seconds to ramp up)</li> <li><code>--run-time 5m</code>: Stop after 5 minutes</li> <li><code>replicas: 5</code>: Five worker pods distribute the load. Each worker handles ~20 users. This is a good ratio for moderate load.</li> </ul>","tags":["tutorial","distributed testing","load testing","beginners"]},{"location":"tutorials/first-load-test/#step-3-monitor-the-test","title":"Step 3: Monitor the test","text":"<p>Watch the test progress through its lifecycle:</p> <pre><code>kubectl get locusttest ecommerce-load -w\n</code></pre> <p>You'll see output like:</p> <pre><code>NAME             PHASE      EXPECTED   CONNECTED   AGE\necommerce-load   Pending    5          0           2s\necommerce-load   Running    5          1           8s\necommerce-load   Running    5          3           12s\necommerce-load   Running    5          5           18s\necommerce-load   Succeeded  5          5           5m22s\n</code></pre>","tags":["tutorial","distributed testing","load testing","beginners"]},{"location":"tutorials/first-load-test/#what-the-phases-mean","title":"What the phases mean","text":"<ul> <li>Pending: Kubernetes is scheduling the pods</li> <li>Running: Test is active, workers are connected and generating load</li> <li>Succeeded: Test completed successfully (ran for full 5 minutes)</li> </ul> <p>You can also check individual pods:</p> <pre><code>kubectl get pods -l performance-test-name=ecommerce-load\n</code></pre> <p>Expected output:</p> <pre><code>NAME                          READY   STATUS    RESTARTS   AGE\necommerce-load-master-xxxxx   1/1     Running   0          25s\necommerce-load-worker-xxxxx   1/1     Running   0          25s\necommerce-load-worker-yyyyy   1/1     Running   0          25s\necommerce-load-worker-zzzzz   1/1     Running   0          25s\necommerce-load-worker-aaaaa   1/1     Running   0          25s\necommerce-load-worker-bbbbb   1/1     Running   0          25s\n</code></pre> <p>You should see 6 pods total: 1 master + 5 workers.</p>","tags":["tutorial","distributed testing","load testing","beginners"]},{"location":"tutorials/first-load-test/#step-4-access-real-time-statistics","title":"Step 4: Access real-time statistics","text":"<p>View the Locust web UI to see live statistics:</p> <pre><code>kubectl port-forward job/ecommerce-load-master 8089:8089\n</code></pre> <p>Open http://localhost:8089 and look for:</p>","tags":["tutorial","distributed testing","load testing","beginners"]},{"location":"tutorials/first-load-test/#key-metrics-to-watch","title":"Key metrics to watch","text":"<ul> <li> <p>Request statistics table:</p> <ul> <li>RPS (requests per second): Should be steady once all users spawn</li> <li>Response times: Median, 95<sup>th</sup> percentile, and 99<sup>th</sup> percentile</li> <li>Failures: Should be 0 for this test (httpbin is reliable)</li> </ul> </li> <li> <p>Charts tab:</p> <ul> <li>Total Requests per Second: Shows load distribution over time</li> <li>Response Times: Visualizes latency trends</li> <li>Number of Users: Shows the 10-second ramp-up to 100 users</li> </ul> </li> <li> <p>Workers tab:</p> <ul> <li>Verify all 5 workers are connected</li> <li>Check that users are distributed across workers</li> </ul> </li> </ul> <p>For a 100-user test with 2-second average wait time and 2 tasks per user cycle, expect roughly 50-75 requests per second total.</p>","tags":["tutorial","distributed testing","load testing","beginners"]},{"location":"tutorials/first-load-test/#step-5-clean-up","title":"Step 5: Clean up","text":"<p>After the test completes (or if you stop it early):</p> <pre><code>kubectl delete locusttest ecommerce-load\nkubectl delete configmap ecommerce-test\n</code></pre> <p>The operator automatically cleans up all related pods, services, and jobs when you delete the LocustTest resource.</p>","tags":["tutorial","distributed testing","load testing","beginners"]},{"location":"tutorials/first-load-test/#what-you-learned","title":"What you learned","text":"<ul> <li>Distributed architecture: Master pod coordinates, worker pods generate load</li> <li>Realistic user simulation: Task weights and wait times model real behavior</li> <li>Test lifecycle: Pending \u2192 Running \u2192 Succeeded phases</li> <li>Resource scaling: 5 workers for 100 users is a good baseline (~20 users per worker)</li> <li>Monitoring: LocustTest status shows health, web UI shows performance metrics</li> </ul>","tags":["tutorial","distributed testing","load testing","beginners"]},{"location":"tutorials/first-load-test/#next-steps","title":"Next steps","text":"<ul> <li>CI/CD Integration: Run tests automatically in your pipeline (15 minutes)</li> <li>How-To: Configure resources: Set CPU/memory for stable tests</li> <li>How-To: Set up Prometheus monitoring: Export metrics for long-term analysis</li> <li>API Reference: Explore all LocustTest configuration options</li> </ul>","tags":["tutorial","distributed testing","load testing","beginners"]},{"location":"tutorials/production-deployment/","title":"Production Deployment (20 minutes)","text":"<p>Configure your load tests for production-grade reliability, observability, and performance.</p>","tags":["tutorial","production","opentelemetry","resources","affinity","scaling"]},{"location":"tutorials/production-deployment/#what-youll-learn","title":"What you'll learn","text":"<ul> <li>How to size resources for master and worker pods</li> <li>How to isolate load test workloads on dedicated nodes</li> <li>How to export metrics to OpenTelemetry</li> <li>How to scale workers for high user counts</li> <li>How to monitor test health via status conditions</li> </ul>","tags":["tutorial","production","opentelemetry","resources","affinity","scaling"]},{"location":"tutorials/production-deployment/#prerequisites","title":"Prerequisites","text":"<ul> <li>Completed the CI/CD Integration tutorial</li> <li>Understanding of Kubernetes resource management</li> <li>OpenTelemetry Collector deployed (optional, for Step 4)</li> </ul>","tags":["tutorial","production","opentelemetry","resources","affinity","scaling"]},{"location":"tutorials/production-deployment/#the-scenario","title":"The scenario","text":"<p>You're running a 1000-user production load test against your staging environment. The test needs:</p> <ul> <li>Dedicated nodes (no interference with production workloads)</li> <li>Resource limits (predictable cluster usage)</li> <li>Metrics export to your observability stack</li> <li>30-minute sustained test with monitoring</li> </ul>","tags":["tutorial","production","opentelemetry","resources","affinity","scaling"]},{"location":"tutorials/production-deployment/#step-1-enhanced-test-script","title":"Step 1: Enhanced test script","text":"<p>Building on the <code>ecommerce_test.py</code> from Tutorial 1, here's a production-ready version with realistic complexity:</p> <pre><code># production_test.py\nfrom locust import HttpUser, task, between\nimport logging\n\nclass ProductionShopperUser(HttpUser):\n    \"\"\"Production-grade e-commerce load test with authentication and realistic behavior.\"\"\"\n\n    wait_time = between(1, 3)  # Realistic user pacing: 1-3 seconds between actions\n\n    def on_start(self):\n        \"\"\"Called once when user starts - simulate authentication.\"\"\"\n        # Authenticate with the API\n        response = self.client.post(\"/api/v1/auth/login\", json={\n            \"username\": \"loadtest@example.com\",\n            \"password\": \"test-password\"\n        }, name=\"Login\")\n\n        if response.status_code == 200:\n            # Store auth token for subsequent requests\n            self.auth_token = response.json().get(\"token\")\n            self.client.headers.update({\"Authorization\": f\"Bearer {self.auth_token}\"})\n            logging.info(\"User authenticated successfully\")\n        else:\n            logging.error(f\"Authentication failed: {response.status_code}\")\n\n    @task(5)  # 50% of requests - most common action\n    def browse_products(self):\n        \"\"\"Browse product catalog - main landing page.\"\"\"\n        self.client.get(\"/api/v1/products\", name=\"Browse Products\")\n\n    @task(2)  # 20% of requests\n    def search_products(self):\n        \"\"\"Search for specific products.\"\"\"\n        search_terms = [\"laptop\", \"phone\", \"tablet\", \"monitor\"]\n        term = search_terms[hash(str(self.user_id)) % len(search_terms)]\n        self.client.get(f\"/api/v1/products?q={term}\", name=\"Search Products\")\n\n    @task(2)  # 20% of requests\n    def view_product_detail(self):\n        \"\"\"View specific product details.\"\"\"\n        # Simulate viewing different products\n        product_id = 1000 + (hash(str(self.user_id)) % 100)\n        self.client.get(f\"/api/v1/products/{product_id}\", name=\"View Product Detail\")\n\n    @task(1)  # 10% of requests\n    def add_to_cart(self):\n        \"\"\"Add product to shopping cart.\"\"\"\n        product_id = 1000 + (hash(str(self.user_id)) % 100)\n        self.client.post(\"/api/v1/cart\", json={\n            \"product_id\": product_id,\n            \"quantity\": 1\n        }, name=\"Add to Cart\")\n\n    def on_stop(self):\n        \"\"\"Called when user stops - cleanup if needed.\"\"\"\n        logging.info(\"User session ended\")\n</code></pre> <p>Key enhancements:</p> <ul> <li><code>on_start</code> hook \u2014 Authenticates once per user (realistic behavior)</li> <li>Weighted tasks \u2014 5:2:2:1 ratio matches real user behavior (browsing &gt; searching &gt; viewing &gt; purchasing)</li> <li>Dynamic data \u2014 Product IDs vary per user (prevents cache hits)</li> <li>Wait time \u2014 <code>between(1, 3)</code> simulates realistic user pacing</li> <li>Logging \u2014 Helps debug test issues in production</li> </ul>","tags":["tutorial","production","opentelemetry","resources","affinity","scaling"]},{"location":"tutorials/production-deployment/#step-2-size-resources-appropriately","title":"Step 2: Size resources appropriately","text":"","tags":["tutorial","production","opentelemetry","resources","affinity","scaling"]},{"location":"tutorials/production-deployment/#why-resource-sizing-matters","title":"Why resource sizing matters","text":"<p>Load tests need consistent performance to generate reliable results. Without resource limits:</p> <ul> <li>Worker pods compete for CPU with other workloads \u2192 inconsistent request rates</li> <li>Memory exhaustion can crash pods mid-test \u2192 incomplete results</li> <li>Cluster instability affects production services</li> </ul>","tags":["tutorial","production","opentelemetry","resources","affinity","scaling"]},{"location":"tutorials/production-deployment/#sizing-guidelines","title":"Sizing guidelines","text":"<p>Master pod (coordinator): - Memory: 512Mi request, 1Gi limit \u2014 handles test coordination and statistics - CPU: 500m request, 1000m limit \u2014 moderate processing needs</p> <p>Worker pods (load generators): - Memory: 256Mi request, 512Mi limit \u2014 per-worker estimate: ~50 users - CPU: 250m request, no limit \u2014 maximizes request generation throughput - Replica count: Total users \u00f7 50 = worker count (e.g., 1000 users = 20 workers)</p>","tags":["tutorial","production","opentelemetry","resources","affinity","scaling"]},{"location":"tutorials/production-deployment/#resource-configuration-example","title":"Resource configuration example","text":"<pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: resource-sized-test\nspec:\n  image: locustio/locust:2.20.0\n  testFiles:\n    configMapRef: production-test\n  master:\n    command: |\n      --locustfile /lotest/src/production_test.py\n      --host https://api.staging.example.com\n      --users 1000\n      --spawn-rate 50\n      --run-time 30m\n    resources:\n      requests:\n        memory: \"512Mi\"\n        cpu: \"500m\"\n      limits:\n        memory: \"1Gi\"\n        cpu: \"1000m\"\n  worker:\n    command: \"--locustfile /lotest/src/production_test.py\"\n    replicas: 20  # 1000 users \u00f7 50 users/worker = 20 workers\n    resources:\n      requests:\n        memory: \"256Mi\"\n        cpu: \"250m\"\n      limits:\n        memory: \"512Mi\"\n        # CPU limit intentionally omitted for maximum performance\n</code></pre> <p>Why omit CPU limit on workers? CPU limits can throttle request generation, reducing test accuracy. Workers with only CPU requests get maximum available CPU while still being schedulable.</p>","tags":["tutorial","production","opentelemetry","resources","affinity","scaling"]},{"location":"tutorials/production-deployment/#step-3-isolate-on-dedicated-nodes","title":"Step 3: Isolate on dedicated nodes","text":"","tags":["tutorial","production","opentelemetry","resources","affinity","scaling"]},{"location":"tutorials/production-deployment/#why-dedicated-nodes-prevent-interference","title":"Why dedicated nodes prevent interference","text":"<p>Running load tests on shared nodes can:</p> <ul> <li>Throttle production workloads \u2014 high CPU usage from workers affects critical services</li> <li>Skew test results \u2014 resource contention from other pods creates inconsistent performance</li> <li>Violate policies \u2014 some clusters prohibit non-production workloads on production nodes</li> </ul>","tags":["tutorial","production","opentelemetry","resources","affinity","scaling"]},{"location":"tutorials/production-deployment/#label-nodes-for-load-testing","title":"Label nodes for load testing","text":"<pre><code># Identify nodes for load testing (e.g., separate node pool)\nkubectl get nodes\n\n# Label dedicated node(s)\nkubectl label nodes worker-node-1 workload-type=load-testing\nkubectl label nodes worker-node-2 workload-type=load-testing\nkubectl label nodes worker-node-3 workload-type=load-testing\n</code></pre>","tags":["tutorial","production","opentelemetry","resources","affinity","scaling"]},{"location":"tutorials/production-deployment/#configure-node-affinity","title":"Configure node affinity","text":"<p>Add node affinity to ensure pods only run on labeled nodes:</p> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: isolated-test\nspec:\n  image: locustio/locust:2.20.0\n  testFiles:\n    configMapRef: production-test\n  master:\n    command: |\n      --locustfile /lotest/src/production_test.py\n      --host https://api.staging.example.com\n      --users 1000\n      --spawn-rate 50\n      --run-time 30m\n  worker:\n    command: \"--locustfile /lotest/src/production_test.py\"\n    replicas: 20\n  scheduling:\n    affinity:\n      nodeAffinity:\n        requiredDuringSchedulingIgnoredDuringExecution:\n          nodeSelectorTerms:\n            - matchExpressions:\n                - key: workload-type\n                  operator: In\n                  values:\n                    - load-testing  # Only schedule on labeled nodes\n</code></pre>","tags":["tutorial","production","opentelemetry","resources","affinity","scaling"]},{"location":"tutorials/production-deployment/#add-tolerations-for-tainted-nodes","title":"Add tolerations for tainted nodes","text":"<p>If your dedicated nodes have taints (prevents accidental scheduling), add tolerations:</p> <pre><code>spec:\n  scheduling:\n    affinity:\n      # ... (node affinity from above)\n    tolerations:\n      - key: \"workload-type\"\n        operator: \"Equal\"\n        value: \"load-testing\"\n        effect: \"NoSchedule\"\n</code></pre> <p>Verification:</p> <pre><code># Check where master and worker pods are scheduled\nkubectl get pods -l performance-test-name=isolated-test -o wide\n\n# You should see NODE column showing only your labeled nodes\n</code></pre>","tags":["tutorial","production","opentelemetry","resources","affinity","scaling"]},{"location":"tutorials/production-deployment/#step-4-enable-opentelemetry","title":"Step 4: Enable OpenTelemetry","text":"","tags":["tutorial","production","opentelemetry","resources","affinity","scaling"]},{"location":"tutorials/production-deployment/#why-native-opentelemetry-beats-sidecars","title":"Why native OpenTelemetry beats sidecars","text":"<p>The v2 operator includes native OpenTelemetry support, eliminating the need for sidecar containers:</p> <ul> <li>Lower overhead \u2014 no extra containers per pod</li> <li>Simpler configuration \u2014 environment variables injected automatically</li> <li>Better performance \u2014 direct export from Locust to collector</li> </ul>","tags":["tutorial","production","opentelemetry","resources","affinity","scaling"]},{"location":"tutorials/production-deployment/#configure-opentelemetry-export","title":"Configure OpenTelemetry export","text":"<pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: otel-enabled-test\nspec:\n  image: locustio/locust:2.20.0\n  testFiles:\n    configMapRef: production-test\n  master:\n    command: |\n      --locustfile /lotest/src/production_test.py\n      --host https://api.staging.example.com\n      --users 1000\n      --spawn-rate 50\n      --run-time 30m\n  worker:\n    command: \"--locustfile /lotest/src/production_test.py\"\n    replicas: 20\n  observability:\n    openTelemetry:\n      enabled: true\n      endpoint: \"otel-collector.monitoring:4317\"  # Your OTel Collector endpoint\n      protocol: \"grpc\"  # or \"http/protobuf\"\n      insecure: false  # Set true for development without TLS\n      extraEnvVars:\n        OTEL_SERVICE_NAME: \"production-load-test\"\n        OTEL_RESOURCE_ATTRIBUTES: \"environment=staging,team=platform,test.type=load\"\n</code></pre> <p>Configuration details:</p> <ul> <li><code>endpoint</code> \u2014 OpenTelemetry Collector gRPC endpoint (format: <code>host:port</code>)</li> <li><code>protocol</code> \u2014 <code>grpc</code> (default) or <code>http/protobuf</code></li> <li><code>insecure</code> \u2014 <code>false</code> for TLS (production), <code>true</code> for development</li> <li><code>extraEnvVars</code> \u2014 Custom attributes for trace/metric filtering</li> </ul>","tags":["tutorial","production","opentelemetry","resources","affinity","scaling"]},{"location":"tutorials/production-deployment/#verify-opentelemetry-injection","title":"Verify OpenTelemetry injection","text":"<pre><code># Check environment variables in master pod\nkubectl get pod -l performance-test-pod-name=otel-enabled-test-master \\\n  -o yaml | grep OTEL_\n\n# Expected output:\n# OTEL_TRACES_EXPORTER: otlp\n# OTEL_METRICS_EXPORTER: otlp\n# OTEL_EXPORTER_OTLP_ENDPOINT: otel-collector.monitoring:4317\n# OTEL_EXPORTER_OTLP_PROTOCOL: grpc\n# OTEL_SERVICE_NAME: production-load-test\n</code></pre>","tags":["tutorial","production","opentelemetry","resources","affinity","scaling"]},{"location":"tutorials/production-deployment/#step-5-deploy-the-complete-production-test","title":"Step 5: Deploy the complete production test","text":"<p>Combining all previous steps, here's the full production-ready LocustTest CR:</p> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: production-load-test\n  namespace: load-testing\nspec:\n  image: locustio/locust:2.20.0\n  testFiles:\n    configMapRef: production-test\n  master:\n    command: |\n      --locustfile /lotest/src/production_test.py\n      --host https://api.staging.example.com\n      --users 1000\n      --spawn-rate 50\n      --run-time 30m\n    resources:\n      requests:\n        memory: \"512Mi\"\n        cpu: \"500m\"\n      limits:\n        memory: \"1Gi\"\n        cpu: \"1000m\"\n  worker:\n    command: \"--locustfile /lotest/src/production_test.py\"\n    replicas: 20  # 1000 users \u00f7 50 users per worker\n    resources:\n      requests:\n        memory: \"256Mi\"\n        cpu: \"250m\"\n      limits:\n        memory: \"512Mi\"\n        # CPU limit omitted for maximum worker performance\n  scheduling:\n    affinity:\n      nodeAffinity:\n        requiredDuringSchedulingIgnoredDuringExecution:\n          nodeSelectorTerms:\n            - matchExpressions:\n                - key: workload-type\n                  operator: In\n                  values:\n                    - load-testing\n    tolerations:\n      - key: \"workload-type\"\n        operator: \"Equal\"\n        value: \"load-testing\"\n        effect: \"NoSchedule\"\n  observability:\n    openTelemetry:\n      enabled: true\n      endpoint: \"otel-collector.monitoring:4317\"\n      protocol: \"grpc\"\n      extraEnvVars:\n        OTEL_SERVICE_NAME: \"production-load-test\"\n        OTEL_RESOURCE_ATTRIBUTES: \"environment=staging,team=platform\"\n</code></pre>","tags":["tutorial","production","opentelemetry","resources","affinity","scaling"]},{"location":"tutorials/production-deployment/#deploy-the-test","title":"Deploy the test","text":"<pre><code># Create ConfigMap from enhanced test script\nkubectl create configmap production-test \\\n  --from-file=production_test.py \\\n  --namespace load-testing\n\n# Apply the LocustTest CR\nkubectl apply -f production-load-test.yaml\n</code></pre>","tags":["tutorial","production","opentelemetry","resources","affinity","scaling"]},{"location":"tutorials/production-deployment/#step-6-monitor-and-verify","title":"Step 6: Monitor and verify","text":"","tags":["tutorial","production","opentelemetry","resources","affinity","scaling"]},{"location":"tutorials/production-deployment/#watch-test-progression","title":"Watch test progression","text":"<pre><code># Monitor test status (watch mode)\nkubectl get locusttest production-load-test -n load-testing -w\n\n# Expected progression:\n# NAME                     PHASE       EXPECTED   CONNECTED   AGE\n# production-load-test     Pending     20         0           5s\n# production-load-test     Running     20         20          45s\n# production-load-test     Succeeded   20         20          31m\n</code></pre>","tags":["tutorial","production","opentelemetry","resources","affinity","scaling"]},{"location":"tutorials/production-deployment/#check-status-conditions","title":"Check status conditions","text":"<pre><code># View detailed status conditions\nkubectl get locusttest production-load-test -n load-testing \\\n  -o jsonpath='{.status.conditions[*]}' | jq\n\n# Expected conditions:\n# {\n#   \"type\": \"PodsHealthy\",\n#   \"status\": \"True\",\n#   \"reason\": \"AllPodsReady\",\n#   \"message\": \"Master and 20/20 workers are ready\"\n# }\n</code></pre>","tags":["tutorial","production","opentelemetry","resources","affinity","scaling"]},{"location":"tutorials/production-deployment/#verify-worker-health","title":"Verify worker health","text":"<pre><code># Check all worker pods are running\nkubectl get pods -l performance-test-pod-name=production-load-test-worker \\\n  -n load-testing\n\n# Expected: 20 pods in Running state\n</code></pre>","tags":["tutorial","production","opentelemetry","resources","affinity","scaling"]},{"location":"tutorials/production-deployment/#verify-opentelemetry-traces","title":"Verify OpenTelemetry traces","text":"<p>If OpenTelemetry is configured, check your observability backend:</p> <p>Prometheus (metrics): <pre><code># Query Locust request metrics\nlocust_requests_total{service_name=\"production-load-test\"}\n\n# Query response time metrics\nlocust_request_duration_seconds{service_name=\"production-load-test\"}\n</code></pre></p> <p>Jaeger/Tempo (traces):</p> <p>Filter by <code>service.name=production-load-test</code> to see:</p> <ul> <li>Individual request spans</li> <li>Request duration distribution</li> <li>Error traces</li> </ul>","tags":["tutorial","production","opentelemetry","resources","affinity","scaling"]},{"location":"tutorials/production-deployment/#access-real-time-locust-ui","title":"Access real-time Locust UI","text":"<pre><code># Port-forward to master pod\nkubectl port-forward -n load-testing \\\n  job/production-load-test-master 8089:8089\n\n# Open http://localhost:8089 in browser\n</code></pre> <p>The UI shows:</p> <ul> <li>Live request statistics (RPS, response times, failures)</li> <li>Charts showing performance trends over time</li> <li>Worker connection status</li> <li>Test phase and remaining duration</li> </ul>","tags":["tutorial","production","opentelemetry","resources","affinity","scaling"]},{"location":"tutorials/production-deployment/#what-you-learned","title":"What you learned","text":"<p>\u2713 How to size master and worker resources for production workloads \u2713 How to isolate load tests on dedicated nodes using affinity and tolerations \u2713 How to export traces and metrics to OpenTelemetry collectors \u2713 How to scale worker replicas based on simulated user count \u2713 How to monitor test health through status conditions \u2713 How to deploy complete production-grade load tests</p>","tags":["tutorial","production","opentelemetry","resources","affinity","scaling"]},{"location":"tutorials/production-deployment/#next-steps","title":"Next steps","text":"<ul> <li>Configure resources \u2014 Deep dive into resource optimization</li> <li>Configure OpenTelemetry \u2014 Advanced observability setup</li> <li>Use node affinity \u2014 More scheduling strategies</li> <li>API Reference \u2014 Explore all configuration options</li> <li>Security best practices \u2014 Secure your load testing infrastructure</li> </ul>","tags":["tutorial","production","opentelemetry","resources","affinity","scaling"]}]}