{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Performance testing that simply works","text":"<p>Utilize the full power of Locust in the cloud with a fully automated, cloud-native approach, creating professional and reliable performance tests in minutes.</p>      Get started         Learn more"},{"location":"#experience-the-power-of-v20","title":"\ud83d\ude80 Experience the Power of v2.0","text":"<ul> <li> <p> Rebuilt in Go</p> <p>Experience 60x faster startup times and a 4x smaller memory footprint. The entire operator has been rewritten in Go for maximum efficiency and reliability.</p> <p> Read the migration guide</p> </li> <li> <p> Native OpenTelemetry</p> <p>Gain deep visibility with built-in tracing and metrics. No sidecars required\u2014just pure, cloud-native observability.</p> <p> Learn more</p> </li> <li> <p> Secret Injection</p> <p>Securely manage your test credentials with native Kubernetes Secret and ConfigMap injection directly into your test pods.</p> <p> Learn more</p> </li> <li> <p> Volume Mounting</p> <p>Mount any storage volume to your master and worker pods for flexible test data and configuration management.</p> <p> Learn more</p> </li> </ul>"},{"location":"#build-for-cloud-native-performance-testing","title":"Build for cloud-native performance testing","text":"<p>The Operator is designed to unlock seamless &amp; effortless distributed performance testing in the cloud and enable continuous integration for CI/CD pipelines. By design, the entire system is cloud native and focuses on automation and CI practices. One strong feature about the system is its ability to horizontally scale to meet any required performance demands.</p>"},{"location":"#key-capabilities","title":"Key capabilities","text":"<ul> <li> <p> Cloud Native</p> <p>Leverage the full power of Kubernetes and cloud-native technologies for distributed performance testing.</p> <p> Learn more</p> </li> <li> <p> Automation &amp; CI</p> <p>Integrate performance testing directly into your CI/CD pipelines for continuous validation.</p> <p> Learn more</p> </li> <li> <p> Governance</p> <p>Maintain control over how resources are deployed and used in the cloud.</p> <p> Learn more</p> </li> <li> <p> Observability</p> <p>Gain insights into test results and infrastructure usage with built-in observability features.</p> <p> Learn more</p> </li> </ul> <p>Check out the full list of features!</p> <p></p>"},{"location":"#designed-for-teams-and-organizations","title":"Designed for teams and organizations","text":""},{"location":"#who-is-it-for","title":"Who is it for","text":"<p>It is built for performance engineers, DevOps teams, and organizations looking to integrate performance testing into their CI/CD pipelines.</p> <p></p>"},{"location":"#universal-deployment","title":"Universal deployment","text":"<p>Due to its design, the Operator can be deployed on any Kubernetes cluster. This means you can have a full cloud-native performance testing system anywhere in a matter of seconds.</p>"},{"location":"#scalable-resources","title":"Scalable resources","text":"<p>The only real limit to this approach is the amount of cluster resources a team or organization is willing to dedicate to performance testing. Scale up or down based on your needs.</p>"},{"location":"advanced_topics/","title":"Advanced topics","text":"<p>Basic configuration is not always enough to satisfy the performance test needs, for example when needing to work with Kafka and MSK. Below is a collection of topics of an advanced nature. This list will be keep growing as the tool matures more and more.</p>","tags":["advanced","configuration","kafka","aws msk","technical"]},{"location":"advanced_topics/#kafka-integration","title":"Kafka &amp; AWS MSK configuration","text":"<p>Generally speaking, the usage of Kafka in a locustfile is identical to how it would be used anywhere else within the cloud context. Thus, no special setup is needed for the purposes of performance testing with the Operator. That being said, if an organization is using kafka in production, chances are that authenticated kafka is being used. One of the main providers of such managed service is AWS in the form of MSK. For that end, the Operator provides an out-of-the-box support for MSK through a two-level configuration model.</p>","tags":["advanced","configuration","kafka","aws msk","technical"]},{"location":"advanced_topics/#two-level-configuration-model","title":"Two-Level Configuration Model","text":"<p>The operator supports two approaches to Kafka configuration, which can be used independently or combined:</p> <p>1. Operator-Level Configuration (Centralized Management)</p> <p>The \"cloud admin\" or team responsible for the Operator deployment can configure Kafka credentials once during operator installation via Helm values. The operator automatically injects these credentials as environment variables into all generated Locust pods. This centralized approach means CR creators don't need to know or manage Kafka credentials - they're handled transparently by the operator.</p> <p>2. Per-Test Configuration (Test-Specific Override)</p> <p>Individual tests can override the operator-level configuration by specifying <code>spec.env.variables</code> in their LocustTest CR. This allows specific tests to use different Kafka clusters, credentials, or configurations when needed.</p> <p>Configuration Priority: Per-test variables specified in the CR override operator-level configuration. This allows centralized management as the default while maintaining flexibility for special cases.</p>","tags":["advanced","configuration","kafka","aws msk","technical"]},{"location":"advanced_topics/#available-environment-variables","title":"Available Environment Variables","text":"<p>When Kafka configuration is enabled (either at operator-level or per-test), the following environment variables are available to the test:</p> Variable Name Description <code>KAFKA_BOOTSTRAP_SERVERS</code> Kafka bootstrap servers <code>KAFKA_SECURITY_ENABLED</code> - <code>KAFKA_SECURITY_PROTOCOL_CONFIG</code> Security protocol. Options: <code>PLAINTEXT</code>, <code>SASL_PLAINTEXT</code>, <code>SASL_SSL</code>, <code>SSL</code> <code>KAFKA_SASL_MECHANISM</code> Authentication mechanism. Options: <code>PLAINTEXT</code>, <code>SCRAM-SHA-256</code>, <code>SCRAM-SHA-512</code> <code>KAFKA_USERNAME</code> The username used to authenticate Kafka clients with the Kafka server <code>KAFKA_PASSWORD</code> The password used to authenticate Kafka clients with the Kafka server","tags":["advanced","configuration","kafka","aws msk","technical"]},{"location":"advanced_topics/#dedicated-kubernetes-nodes","title":"Dedicated Kubernetes Nodes","text":"<p>To run test resources on dedicated Kubernetes node(s), the Operator support deploying resources with Affinity and Taint Tolerations.</p>","tags":["advanced","configuration","kafka","aws msk","technical"]},{"location":"advanced_topics/#affinity","title":"Affinity","text":"<p>This allows generated resources to have specific Affinity options.</p> <p>Note</p> <p>The Custom Resource Definition Spec is designed with modularity and expandability in mind. This means that although a specific set of Kubernetes Affinity options are supported today, extending this support based on need is a streamlined and easy processes. If additonal support is needed, don't hesitate to open a feature request.</p>","tags":["advanced","configuration","kafka","aws msk","technical"]},{"location":"advanced_topics/#affinity-options","title":"Affinity Options","text":"<p>The specification for affinity is defined as follows</p> v2 APIv1 API (Deprecated) <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: my-test\nspec:\n  image: locustio/locust:2.20.0\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://example.com\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 3\n  scheduling:\n    affinity:\n      nodeAffinity:\n        requiredDuringSchedulingIgnoredDuringExecution:\n          nodeSelectorTerms:\n            - matchExpressions:\n                - key: &lt;label-key&gt;\n                  operator: In\n                  values:\n                    - &lt;label-value&gt;\n</code></pre> <pre><code>apiVersion: locust.io/v1\n...\nspec:\n...\naffinity:\n    nodeAffinity:\n        requiredDuringSchedulingIgnoredDuringExecution\n            &lt;label-key&gt;: &lt;label-value&gt;\n            ...\n...\n</code></pre>","tags":["advanced","configuration","kafka","aws msk","technical"]},{"location":"advanced_topics/#node-affinity","title":"Node Affinity","text":"<p>This optional section causes generated pods to declare specific Node Affinity so Kubernetes scheduler becomes aware of this requirement.</p> <p>The implementation from the Custom Resource perspective is strongly influenced by Kubernetes native definition of node affinity. However, the implementation is on purpose slightly simplified in order to allow users to have easier time working with affinity.</p> <p>The <code>nodeAffinity</code> section supports declaring node affinity under <code>requiredDuringSchedulingIgnoredDuringExecution</code>. Meaning that any declared affinity labels must be present on nodes in order for resources to be deployed on them.</p> <p>Example:</p> <p>In the below example, generated pods will declare 3 required labels (keys and values) to be present on nodes before they are scheduled.</p> v2 APIv1 API (Deprecated) <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: affinity-example\nspec:\n  image: locustio/locust:2.20.0\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://example.com\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 5\n  scheduling:\n    affinity:\n      nodeAffinity:\n        requiredDuringSchedulingIgnoredDuringExecution:\n          nodeSelectorTerms:\n            - matchExpressions:\n                - key: nodeAffinityLabel1\n                  operator: In\n                  values:\n                    - locust-cloud-tests\n                - key: nodeAffinityLabel2\n                  operator: In\n                  values:\n                    - performance-nodes\n                - key: nodeAffinityLabel3\n                  operator: In\n                  values:\n                    - high-memory\n</code></pre> <pre><code>apiVersion: locust.io/v1\n...\nspec:\n    ...\n    affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeAffinityLabel1: locust-cloud-tests\n            nodeAffinityLabel2: performance-nodes\n            nodeAffinityLabel3: high-memory\n            ...\n    ...\n</code></pre>","tags":["advanced","configuration","kafka","aws msk","technical"]},{"location":"advanced_topics/#taint-tolerations","title":"Taint Tolerations","text":"<p>This allows generated resources to have specific Taint Tolerations options.</p>","tags":["advanced","configuration","kafka","aws msk","technical"]},{"location":"advanced_topics/#toleration-options","title":"Toleration Options","text":"<p>The specification for tolerations is defined as follows</p> v2 APIv2 API Examplev1 API (Deprecated) <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: my-test\nspec:\n  image: locustio/locust:2.20.0\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://example.com\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 3\n  scheduling:\n    tolerations:\n      - key: &lt;string value&gt;\n        operator: &lt;\"Exists\", \"Equal\"&gt;\n        value: &lt;string value&gt; #(1)!\n        effect: &lt;\"NoSchedule\", \"PreferNoSchedule\", \"NoExecute\"&gt;\n</code></pre> <ol> <li>Optional when <code>operator</code> value is set to <code>Exists</code>.</li> </ol> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: toleration-example\nspec:\n  image: locustio/locust:2.20.0\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://example.com\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 3\n  scheduling:\n    tolerations:\n      - key: taint-A\n        operator: Equal\n        value: ssd\n        effect: NoSchedule\n      - key: taint-B\n        operator: Exists\n        effect: NoExecute\n</code></pre> <pre><code>apiVersion: locust.io/v1\n...\nspec:\n  ...\n  tolerations:\n    - key: taint-A\n      operator: Equal\n      value: ssd\n      effect: NoSchedule\n    - key: taint-B\n      operator: Exists\n      effect: NoExecute\n</code></pre>","tags":["advanced","configuration","kafka","aws msk","technical"]},{"location":"advanced_topics/#node-selector","title":"Node Selector","text":"<p>This allows generated resources to be scheduled on nodes with specific labels using Kubernetes node selector.</p>","tags":["advanced","configuration","kafka","aws msk","technical"]},{"location":"advanced_topics/#node-selector-options","title":"Node Selector Options","text":"<p>The specification for node selector is defined as follows:</p> v2 APIv2 API Example <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: my-test\nspec:\n  image: locustio/locust:2.20.0\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://example.com\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 3\n  scheduling:\n    nodeSelector:\n      &lt;key&gt;: &lt;value&gt;\n      &lt;key&gt;: &lt;value&gt;\n</code></pre> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: nodeselector-example\nspec:\n  image: locustio/locust:2.20.0\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://example.com\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 3\n  scheduling:\n    nodeSelector:\n      disktype: ssd\n      environment: performance\n</code></pre> <p>Node selector is the simplest way to constrain pods to nodes with specific labels. For more complex scheduling requirements, use affinity rules or tolerations.</p> <p>Node Selector vs Affinity</p> <p>Node selector is a simpler, label-based approach to node selection. Use affinity rules when you need more complex scheduling logic such as:</p> <ul> <li>Multiple node selection criteria with OR logic</li> <li>Soft preferences (preferred rather than required)</li> <li>Pod affinity/anti-affinity rules</li> </ul>","tags":["advanced","configuration","kafka","aws msk","technical"]},{"location":"advanced_topics/#resource-management","title":"Resource Management","text":"<p>The operator allows for fine-grained control over the resource requests and limits for the Locust master and worker pods. This is useful for ensuring that your load tests have the resources they need, and for preventing them from consuming too many resources on your cluster.</p>","tags":["advanced","configuration","kafka","aws msk","technical"]},{"location":"advanced_topics/#global-defaults-via-helm","title":"Global Defaults via Helm","text":"<p>Configuration is done through Helm values. The following properties are available:</p> <ul> <li><code>locustPods.resources.requests.cpu</code></li> <li><code>locustPods.resources.requests.memory</code></li> <li><code>locustPods.resources.limits.cpu</code></li> <li><code>locustPods.resources.limits.memory</code></li> </ul> <p>These defaults apply to all Locust pods unless overridden in individual CRs.</p>","tags":["advanced","configuration","kafka","aws msk","technical"]},{"location":"advanced_topics/#per-cr-resource-configuration-v2-api","title":"Per-CR Resource Configuration (v2 API)","text":"<p>New in v2.0</p> <p>The v2 API allows you to configure resources independently for master and worker pods.</p> <p>You can specify resources directly in your LocustTest CR:</p> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: resource-example\nspec:\n  image: locustio/locust:2.20.0\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://example.com\"\n    resources:\n      requests:\n        memory: \"256Mi\"\n        cpu: \"100m\"\n      limits:\n        memory: \"512Mi\"\n        cpu: \"500m\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 10\n    resources:\n      requests:\n        memory: \"512Mi\"\n        cpu: \"500m\"\n      limits:\n        memory: \"1Gi\"\n        cpu: \"1000m\"\n</code></pre> <p> Learn more about separate resource specs</p>","tags":["advanced","configuration","kafka","aws msk","technical"]},{"location":"advanced_topics/#disabling-cpu-limits","title":"Disabling CPU Limits","text":"<p>In some scenarios, particularly during performance-sensitive tests, you may want to disable CPU limits to prevent throttling.</p> Global (Helm Values)Per-CR (v2 API) <pre><code>locustPods:\n  resources:\n    limits:\n      cpu: \"\" # (1)!\n</code></pre> <ol> <li>Setting the CPU limit to an empty string disables it globally.</li> </ol> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: no-cpu-limit-test\nspec:\n  image: locustio/locust:2.20.0\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://example.com\"\n    resources:\n      requests:\n        memory: \"256Mi\"\n        cpu: \"100m\"\n      limits:\n        memory: \"512Mi\"\n        # No CPU limit specified\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 10\n    resources:\n      requests:\n        memory: \"512Mi\"\n        cpu: \"500m\"\n      limits:\n        memory: \"1Gi\"\n        # No CPU limit specified\n</code></pre> <p>Note</p> <p>When the CPU limit is disabled, the pod is allowed to use as much CPU as is available on the node. This can be useful for maximizing performance, but it can also lead to resource contention if not managed carefully.</p>","tags":["advanced","configuration","kafka","aws msk","technical"]},{"location":"advanced_topics/#private-image-registry","title":"Usage of a private image registry","text":"<p>Images from a private image registry can be used through various methods as described in the kubernetes documentation, one of those methods depends on setting <code>imagePullSecrets</code> for pods. This is supported in the operator by simply setting the <code>imagePullSecrets</code> option in the deployed custom resource. For example:</p> v2 APIv1 API (Deprecated) locusttest-pull-secret-cr.yaml<pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: private-registry-test\nspec:\n  image: ghcr.io/mycompany/locust:latest # (1)!\n  imagePullSecrets: # (2)!\n    - name: gcr-secret\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://example.com\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 3\n</code></pre> <ol> <li>Specify which Locust image to use for both master and worker containers.</li> <li>[Optional] Specify an existing pull secret to use for master and worker pods.</li> </ol> <pre><code>apiVersion: locust.io/v1\n...\nspec:\n  image: ghcr.io/mycompany/locust:latest\n  imagePullSecrets:\n    - name: gcr-secret\n  ...\n</code></pre>","tags":["advanced","configuration","kafka","aws msk","technical"]},{"location":"advanced_topics/#image-pull-policy","title":"Image pull policy","text":"<p>Kubernetes uses the image tag and pull policy to control when kubelet attempts to download (pull) a container image. The image pull policy can be defined through the <code>imagePullPolicy</code> option, as explained in the kubernetes documentation. When using the operator, the <code>imagePullPolicy</code> option can be directly configured in the custom resource. For example:</p> v2 APIv1 API (Deprecated) locusttest-pull-policy-cr.yaml<pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: pull-policy-test\nspec:\n  image: ghcr.io/mycompany/locust:latest # (1)!\n  imagePullPolicy: Always # (2)!\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://example.com\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 3\n</code></pre> <ol> <li>Specify which Locust image to use for both master and worker containers.</li> <li>[Optional] Specify the pull policy to use for containers. Supported options: <code>Always</code>, <code>IfNotPresent</code>, <code>Never</code>.</li> </ol> <pre><code>apiVersion: locust.io/v1\n...\nspec:\n  image: ghcr.io/mycompany/locust:latest\n  imagePullPolicy: Always\n  ...\n</code></pre>","tags":["advanced","configuration","kafka","aws msk","technical"]},{"location":"advanced_topics/#automatic-cleanup","title":"Automatic Cleanup for Finished Master and Worker Jobs","text":"<p>Once load tests finish, master and worker jobs remain available in Kubernetes. You can set up a time-to-live (TTL) value in the operator's Helm chart, so that kubernetes jobs are eligible for cascading removal once the TTL expires. This means that Master and Worker jobs and their dependent objects (e.g., pods) will be deleted.</p> <p>Note that setting up a TTL will not delete <code>LocustTest</code> or <code>ConfigMap</code> resources.</p> <p>To set a TTL value, override the key <code>ttlSecondsAfterFinished</code> in <code>values.yaml</code>:</p> <code>values.yaml</code> <pre><code>locustPods:\n  # Either leave empty or use an empty string to avoid setting this option\n  ttlSecondsAfterFinished: 3600 # (1)!\n</code></pre> <ol> <li>Time in seconds to keep the job after it finishes.</li> </ol> <p>You can also use Helm's CLI arguments: <code>helm install ... --set locustPods.ttlSecondsAfterFinished=0</code>.</p> <p>Backward Compatibility</p> <p>The old path <code>config.loadGenerationJobs.ttlSecondsAfterFinished</code> is still supported via helper functions in the Helm chart.</p> <p>Read more about the <code>ttlSecondsAfterFinished</code> parameter in Kubernetes's official documentation.</p>","tags":["advanced","configuration","kafka","aws msk","technical"]},{"location":"advanced_topics/#kubernetes-support-for-ttlsecondsafterfinished","title":"Kubernetes Support for <code>ttlSecondsAfterFinished</code>","text":"<p>Support for parameter <code>ttlSecondsAfterFinished</code> was added in Kubernetes v1.12. In case you're deploying the locust operator to a Kubernetes cluster that does not support <code>ttlSecondsAfterFinished</code>, you may leave the Helm key empty or use an empty string. In this case, job definitions will not include the parameter.</p>","tags":["advanced","configuration","kafka","aws msk","technical"]},{"location":"advanced_topics/#opentelemetry-integration","title":"OpenTelemetry Integration","text":"<p>New in v2.0</p> <p>This feature is only available in the v2 API.</p> <p>Locust 2.x supports native OpenTelemetry for exporting traces and metrics. The operator can configure this automatically, eliminating the need for the metrics exporter sidecar.</p>","tags":["advanced","configuration","kafka","aws msk","technical"]},{"location":"advanced_topics/#enabling-opentelemetry","title":"Enabling OpenTelemetry","text":"<pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: otel-enabled-test\nspec:\n  image: locustio/locust:2.20.0\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://example.com\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 5\n  observability:\n    openTelemetry:\n      enabled: true\n      endpoint: \"otel-collector.monitoring:4317\"\n      protocol: \"grpc\"  # or \"http/protobuf\"\n      insecure: false\n      extraEnvVars:\n        OTEL_SERVICE_NAME: \"my-load-test\"\n        OTEL_RESOURCE_ATTRIBUTES: \"environment=staging,team=platform\"\n</code></pre>","tags":["advanced","configuration","kafka","aws msk","technical"]},{"location":"advanced_topics/#otel-environment-variables","title":"OTel Environment Variables","text":"<p>When OpenTelemetry is enabled, the operator injects the following environment variables:</p> Variable Description <code>OTEL_TRACES_EXPORTER</code> Set to <code>otlp</code> <code>OTEL_METRICS_EXPORTER</code> Set to <code>otlp</code> <code>OTEL_EXPORTER_OTLP_ENDPOINT</code> Your configured endpoint <code>OTEL_EXPORTER_OTLP_PROTOCOL</code> <code>grpc</code> or <code>http/protobuf</code> <code>OTEL_EXPORTER_OTLP_INSECURE</code> Only set if <code>insecure: true</code>","tags":["advanced","configuration","kafka","aws msk","technical"]},{"location":"advanced_topics/#otel-vs-metrics-sidecar","title":"OTel vs Metrics Sidecar","text":"Aspect OpenTelemetry Metrics Sidecar Setup complexity Low Low Traces  Yes  No Metrics  Yes  Yes Additional containers None 1 sidecar Recommended for New deployments Legacy compatibility <p>When OpenTelemetry is enabled:</p> <ul> <li>The <code>--otel</code> flag is added to Locust commands</li> <li>The metrics exporter sidecar is not deployed</li> <li>The metrics port is excluded from the Service</li> </ul>","tags":["advanced","configuration","kafka","aws msk","technical"]},{"location":"advanced_topics/#environment-secret-injection","title":"Environment &amp; Secret Injection","text":"<p>New in v2.0</p> <p>This feature is only available in the v2 API.</p> <p>Inject configuration and credentials into Locust pods without hardcoding them in test files.</p>","tags":["advanced","configuration","kafka","aws msk","technical"]},{"location":"advanced_topics/#configmap-environment-variables","title":"ConfigMap Environment Variables","text":"<p>Inject all keys from a ConfigMap as environment variables:</p> <pre><code>spec:\n  env:\n    configMapRefs:\n      - name: app-config\n        prefix: \"APP_\"  # Results in APP_KEY1, APP_KEY2, etc.\n</code></pre>","tags":["advanced","configuration","kafka","aws msk","technical"]},{"location":"advanced_topics/#secret-environment-variables","title":"Secret Environment Variables","text":"<p>Inject all keys from a Secret as environment variables:</p> <pre><code>spec:\n  env:\n    secretRefs:\n      - name: api-credentials\n        prefix: \"\"  # No prefix, use key names directly\n</code></pre>","tags":["advanced","configuration","kafka","aws msk","technical"]},{"location":"advanced_topics/#individual-variables","title":"Individual Variables","text":"<p>Define individual environment variables with values or references:</p> <pre><code>spec:\n  env:\n    variables:\n      - name: TARGET_HOST\n        value: \"https://api.example.com\"\n      - name: API_TOKEN\n        valueFrom:\n          secretKeyRef:\n            name: api-secret\n            key: token\n      - name: CONFIG_VALUE\n        valueFrom:\n          configMapKeyRef:\n            name: app-config\n            key: some-key\n</code></pre>","tags":["advanced","configuration","kafka","aws msk","technical"]},{"location":"advanced_topics/#secret-file-mounts","title":"Secret File Mounts","text":"<p>Mount secrets as files in the container:</p> <pre><code>spec:\n  env:\n    secretMounts:\n      - name: tls-certs\n        mountPath: /etc/locust/certs\n        readOnly: true\n</code></pre>","tags":["advanced","configuration","kafka","aws msk","technical"]},{"location":"advanced_topics/#reserved-paths","title":"Reserved Paths","text":"<p>The following paths are reserved and cannot be used for secret mounts:</p> Path Purpose <code>/lotest/src/</code> Test script mount point (default) <code>/opt/locust/lib</code> Library mount point (default) <p>Note</p> <p>If you customize <code>testFiles.srcMountPath</code> or <code>testFiles.libMountPath</code>, those custom paths become reserved instead.</p>","tags":["advanced","configuration","kafka","aws msk","technical"]},{"location":"advanced_topics/#volume-mounting","title":"Volume Mounting","text":"<p>New in v2.0</p> <p>This feature is only available in the v2 API.</p> <p>Mount arbitrary volumes to Locust pods for test data, certificates, or configuration files.</p>","tags":["advanced","configuration","kafka","aws msk","technical"]},{"location":"advanced_topics/#basic-volume-mount","title":"Basic Volume Mount","text":"<pre><code>spec:\n  volumes:\n    - name: test-data\n      persistentVolumeClaim:\n        claimName: test-data-pvc\n  volumeMounts:\n    - name: test-data\n      mountPath: /data\n      target: both  # master, worker, or both\n</code></pre>","tags":["advanced","configuration","kafka","aws msk","technical"]},{"location":"advanced_topics/#target-filtering","title":"Target Filtering","text":"<p>Control which pods receive the volume mount:</p> Target Master Worker <code>master</code> <code>worker</code> <code>both</code> (default)","tags":["advanced","configuration","kafka","aws msk","technical"]},{"location":"advanced_topics/#supported-volume-types","title":"Supported Volume Types","text":"<p>You can use any Kubernetes volume type:</p> PersistentVolumeClaimConfigMapSecretEmptyDir <pre><code>volumes:\n  - name: test-data\n    persistentVolumeClaim:\n      claimName: my-pvc\n</code></pre> <pre><code>volumes:\n  - name: config-files\n    configMap:\n      name: my-configmap\n</code></pre> <pre><code>volumes:\n  - name: certs\n    secret:\n      secretName: tls-secret\n</code></pre> <pre><code>volumes:\n  - name: cache\n    emptyDir: {}\n</code></pre>","tags":["advanced","configuration","kafka","aws msk","technical"]},{"location":"advanced_topics/#reserved-volume-names","title":"Reserved Volume Names","text":"<p>The following volume names are reserved:</p> Pattern Purpose <code>&lt;crName&gt;-master</code> Master ConfigMap volume <code>&lt;crName&gt;-worker</code> Worker ConfigMap volume <code>locust-lib</code> Library ConfigMap volume <code>secret-*</code> Secret volumes from <code>env.secretMounts</code>","tags":["advanced","configuration","kafka","aws msk","technical"]},{"location":"advanced_topics/#separate-resource-specs","title":"Separate Resource Specs","text":"<p>New in v2.0</p> <p>This feature is only available in the v2 API.</p> <p>Configure resources independently for master and worker pods, allowing you to optimize each component based on its specific needs.</p>","tags":["advanced","configuration","kafka","aws msk","technical"]},{"location":"advanced_topics/#independent-resource-configuration","title":"Independent Resource Configuration","text":"<pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: optimized-test\nspec:\n  image: locustio/locust:2.20.0\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://example.com\"\n    resources:\n      requests:\n        memory: \"256Mi\"\n        cpu: \"100m\"\n      limits:\n        memory: \"512Mi\"\n        cpu: \"500m\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 10\n    resources:\n      requests:\n        memory: \"512Mi\"\n        cpu: \"500m\"\n      limits:\n        memory: \"1Gi\"\n        cpu: \"1000m\"\n</code></pre>","tags":["advanced","configuration","kafka","aws msk","technical"]},{"location":"advanced_topics/#use-cases","title":"Use Cases","text":"<ul> <li>Master pod: Lower resources since it primarily coordinates workers</li> <li>Worker pods: Higher resources for actual load generation</li> <li>Memory-intensive tests: Increase memory limits for workers</li> <li>CPU-intensive tests: Increase CPU limits or remove limits entirely</li> </ul>","tags":["advanced","configuration","kafka","aws msk","technical"]},{"location":"advanced_topics/#fallback-to-operator-defaults","title":"Fallback to Operator Defaults","text":"<p>If <code>resources</code> is not specified in the CR, the operator uses default values from its configuration (set via Helm values).</p>","tags":["advanced","configuration","kafka","aws msk","technical"]},{"location":"api_reference/","title":"API Reference","text":"<p>This document provides a complete reference for the LocustTest Custom Resource Definition (CRD).</p>","tags":["api","reference","crd","specification"]},{"location":"api_reference/#overview","title":"Overview","text":"Property Value Group <code>locust.io</code> Kind <code>LocustTest</code> Versions v2 (recommended), v1 (deprecated) Short Name <code>lotest</code> Scope Namespaced","tags":["api","reference","crd","specification"]},{"location":"api_reference/#locusttest-v2-recommended","title":"LocustTest v2 (Recommended)","text":"<p>The v2 API provides a cleaner, grouped configuration structure with new features.</p>","tags":["api","reference","crd","specification"]},{"location":"api_reference/#spec-fields","title":"Spec Fields","text":"","tags":["api","reference","crd","specification"]},{"location":"api_reference/#root-fields","title":"Root Fields","text":"Field Type Required Default Description <code>image</code> string Yes - Container image for Locust pods (e.g., <code>locustio/locust:2.20.0</code>) <code>imagePullPolicy</code> string No <code>IfNotPresent</code> Image pull policy: <code>Always</code>, <code>IfNotPresent</code>, <code>Never</code> <code>imagePullSecrets</code> []LocalObjectReference No - Secrets for pulling from private registries (specify as <code>- name: secret-name</code>) <code>master</code> MasterSpec Yes - Master pod configuration <code>worker</code> WorkerSpec Yes - Worker pod configuration <code>testFiles</code> TestFilesConfig No - ConfigMap references for test files <code>scheduling</code> SchedulingConfig No - Affinity, tolerations, nodeSelector <code>env</code> EnvConfig No - Environment variable injection <code>volumes</code> []corev1.Volume No - Additional volumes to mount <code>volumeMounts</code> []TargetedVolumeMount No - Volume mounts with target filtering <code>observability</code> ObservabilityConfig No - OpenTelemetry configuration","tags":["api","reference","crd","specification"]},{"location":"api_reference/#masterspec","title":"MasterSpec","text":"Field Type Required Default Description <code>command</code> string Yes - Locust command seed (e.g., <code>--locustfile /lotest/src/test.py --host https://example.com</code>) <code>resources</code> corev1.ResourceRequirements No From operator config CPU/memory requests and limits <code>labels</code> map[string]string No - Additional labels for master pod <code>annotations</code> map[string]string No - Additional annotations for master pod <code>autostart</code> bool No <code>true</code> Start test automatically when workers connect <code>autoquit</code> AutoquitConfig No <code>{enabled: true, timeout: 60}</code> Auto-quit behavior after test completion <code>extraArgs</code> []string No - Additional command-line arguments","tags":["api","reference","crd","specification"]},{"location":"api_reference/#workerspec","title":"WorkerSpec","text":"Field Type Required Default Description <code>command</code> string Yes - Locust command seed (e.g., <code>--locustfile /lotest/src/test.py</code>) <code>replicas</code> int32 Yes - Number of worker replicas (1-500) <code>resources</code> corev1.ResourceRequirements No From operator config CPU/memory requests and limits <code>labels</code> map[string]string No - Additional labels for worker pods <code>annotations</code> map[string]string No - Additional annotations for worker pods <code>extraArgs</code> []string No - Additional command-line arguments","tags":["api","reference","crd","specification"]},{"location":"api_reference/#autoquitconfig","title":"AutoquitConfig","text":"Field Type Required Default Description <code>enabled</code> bool No <code>true</code> Enable auto-quit after test completion <code>timeout</code> int32 No <code>60</code> Seconds to wait before quitting after test ends","tags":["api","reference","crd","specification"]},{"location":"api_reference/#testfilesconfig","title":"TestFilesConfig","text":"Field Type Required Default Description <code>configMapRef</code> string No - ConfigMap containing test files <code>libConfigMapRef</code> string No - ConfigMap containing library files <code>srcMountPath</code> string No <code>/lotest/src</code> Mount path for test files <code>libMountPath</code> string No <code>/opt/locust/lib</code> Mount path for library files","tags":["api","reference","crd","specification"]},{"location":"api_reference/#schedulingconfig","title":"SchedulingConfig","text":"Field Type Required Default Description <code>affinity</code> corev1.Affinity No - Standard Kubernetes affinity rules <code>tolerations</code> []corev1.Toleration No - Standard Kubernetes tolerations <code>nodeSelector</code> map[string]string No - Node selector labels","tags":["api","reference","crd","specification"]},{"location":"api_reference/#envconfig","title":"EnvConfig","text":"Field Type Required Default Description <code>configMapRefs</code> []ConfigMapEnvSource No - ConfigMaps to inject as environment variables <code>secretRefs</code> []SecretEnvSource No - Secrets to inject as environment variables <code>variables</code> []corev1.EnvVar No - Individual environment variables <code>secretMounts</code> []SecretMount No - Secrets to mount as files","tags":["api","reference","crd","specification"]},{"location":"api_reference/#configmapenvsource","title":"ConfigMapEnvSource","text":"Field Type Required Default Description <code>name</code> string Yes - ConfigMap name <code>prefix</code> string No - Prefix to add to all keys (e.g., <code>APP_</code>)","tags":["api","reference","crd","specification"]},{"location":"api_reference/#secretenvsource","title":"SecretEnvSource","text":"Field Type Required Default Description <code>name</code> string Yes - Secret name <code>prefix</code> string No - Prefix to add to all keys","tags":["api","reference","crd","specification"]},{"location":"api_reference/#secretmount","title":"SecretMount","text":"Field Type Required Default Description <code>name</code> string Yes - Secret name <code>mountPath</code> string Yes - Path to mount the secret <code>readOnly</code> bool No <code>true</code> Mount as read-only","tags":["api","reference","crd","specification"]},{"location":"api_reference/#targetedvolumemount","title":"TargetedVolumeMount","text":"Field Type Required Default Description <code>name</code> string Yes - Volume name (must match a volume in <code>volumes</code>) <code>mountPath</code> string Yes - Path to mount the volume <code>subPath</code> string No - Sub-path within the volume <code>readOnly</code> bool No <code>false</code> Mount as read-only <code>target</code> string No <code>both</code> Target pods: <code>master</code>, <code>worker</code>, or <code>both</code>","tags":["api","reference","crd","specification"]},{"location":"api_reference/#observabilityconfig","title":"ObservabilityConfig","text":"Field Type Required Default Description <code>openTelemetry</code> OpenTelemetryConfig No - OpenTelemetry configuration","tags":["api","reference","crd","specification"]},{"location":"api_reference/#opentelemetryconfig","title":"OpenTelemetryConfig","text":"Field Type Required Default Description <code>enabled</code> bool No <code>false</code> Enable OpenTelemetry integration <code>endpoint</code> string Required if enabled - OTel collector endpoint (e.g., <code>otel-collector:4317</code>) <code>protocol</code> string No <code>grpc</code> Protocol: <code>grpc</code> or <code>http/protobuf</code> <code>insecure</code> bool No <code>false</code> Use insecure connection <code>extraEnvVars</code> map[string]string No - Additional OTel environment variables","tags":["api","reference","crd","specification"]},{"location":"api_reference/#status-fields","title":"Status Fields","text":"Field Type Description <code>phase</code> string Current phase: <code>Pending</code>, <code>Running</code>, <code>Succeeded</code>, <code>Failed</code> <code>expectedWorkers</code> int32 Number of expected worker replicas <code>connectedWorkers</code> int32 Number of connected workers <code>startTime</code> metav1.Time When the test started <code>completionTime</code> metav1.Time When the test completed <code>conditions</code> []metav1.Condition Standard Kubernetes conditions","tags":["api","reference","crd","specification"]},{"location":"api_reference/#condition-types","title":"Condition Types","text":"Type Description <code>Ready</code> True when all resources are created <code>WorkersConnected</code> True when all workers are connected <code>TestCompleted</code> True when the test has finished","tags":["api","reference","crd","specification"]},{"location":"api_reference/#complete-v2-example","title":"Complete v2 Example","text":"<pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: comprehensive-test\nspec:\n  image: locustio/locust:2.20.0\n  imagePullPolicy: IfNotPresent\n\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://api.example.com --users 1000 --spawn-rate 50 --run-time 10m\"\n    resources:\n      requests:\n        memory: \"256Mi\"\n        cpu: \"100m\"\n      limits:\n        memory: \"512Mi\"\n        cpu: \"500m\"\n    labels:\n      role: master\n    autostart: true\n    autoquit:\n      enabled: true\n      timeout: 120\n\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 10\n    resources:\n      requests:\n        memory: \"512Mi\"\n        cpu: \"500m\"\n      limits:\n        memory: \"1Gi\"\n        cpu: \"1000m\"\n    labels:\n      role: worker\n\n  testFiles:\n    configMapRef: my-test-scripts\n    libConfigMapRef: my-lib-files\n\n  scheduling:\n    nodeSelector:\n      node-type: performance\n    tolerations:\n      - key: \"dedicated\"\n        operator: \"Equal\"\n        value: \"performance\"\n        effect: \"NoSchedule\"\n\n  env:\n    secretRefs:\n      - name: api-credentials\n        prefix: \"API_\"\n    configMapRefs:\n      - name: app-config\n    variables:\n      - name: LOG_LEVEL\n        value: \"INFO\"\n\n  volumes:\n    - name: test-data\n      persistentVolumeClaim:\n        claimName: test-data-pvc\n\n  volumeMounts:\n    - name: test-data\n      mountPath: /data\n      target: both\n\n  observability:\n    openTelemetry:\n      enabled: true\n      endpoint: \"otel-collector.monitoring:4317\"\n      protocol: \"grpc\"\n      extraEnvVars:\n        OTEL_SERVICE_NAME: \"load-test\"\n        OTEL_RESOURCE_ATTRIBUTES: \"environment=staging,team=platform\"\n</code></pre>","tags":["api","reference","crd","specification"]},{"location":"api_reference/#locusttest-v1-deprecated","title":"LocustTest v1 (Deprecated)","text":"<p>Deprecated</p> <p>The v1 API is deprecated and will be removed in v3.0. Use v2 for new deployments. See the Migration Guide for upgrade instructions.</p>","tags":["api","reference","crd","specification"]},{"location":"api_reference/#spec-fields-v1","title":"Spec Fields (v1)","text":"Field Type Required Default Description <code>masterCommandSeed</code> string Yes - Command seed for master pod <code>workerCommandSeed</code> string Yes - Command seed for worker pods <code>workerReplicas</code> int32 Yes - Number of worker replicas (1-500) <code>image</code> string Yes - Container image <code>imagePullPolicy</code> string No <code>IfNotPresent</code> Image pull policy <code>imagePullSecrets</code> []string No - Pull secrets <code>configMap</code> string No - ConfigMap for test files <code>libConfigMap</code> string No - ConfigMap for library files <code>labels</code> PodLabels No - Labels with <code>master</code> and <code>worker</code> maps <code>annotations</code> PodAnnotations No - Annotations with <code>master</code> and <code>worker</code> maps <code>affinity</code> LocustTestAffinity No - Custom affinity structure <code>tolerations</code> []LocustTestToleration No - Custom toleration structure","tags":["api","reference","crd","specification"]},{"location":"api_reference/#v1-example","title":"v1 Example","text":"<pre><code>apiVersion: locust.io/v1\nkind: LocustTest\nmetadata:\n  name: basic-test\nspec:\n  image: locustio/locust:2.20.0\n  masterCommandSeed: \"--locustfile /lotest/src/test.py --host https://example.com\"\n  workerCommandSeed: \"--locustfile /lotest/src/test.py\"\n  workerReplicas: 3\n  configMap: test-scripts\n</code></pre>","tags":["api","reference","crd","specification"]},{"location":"api_reference/#kubectl-commands","title":"Kubectl Commands","text":"<pre><code># List all LocustTests\nkubectl get locusttests\nkubectl get lotest  # short name\n\n# Describe a LocustTest\nkubectl describe locusttest &lt;name&gt;\n\n# Watch status changes\nkubectl get locusttest &lt;name&gt; -w\n\n# Delete a LocustTest\nkubectl delete locusttest &lt;name&gt;\n</code></pre>","tags":["api","reference","crd","specification"]},{"location":"api_reference/#printer-columns","title":"Printer Columns","text":"<p>When listing LocustTests, the following columns are displayed:</p> Column Description NAME Resource name PHASE Current phase (Pending/Running/Succeeded/Failed) WORKERS Requested worker count CONNECTED Connected worker count IMAGE Container image (priority column) AGE Time since creation","tags":["api","reference","crd","specification"]},{"location":"contribute/","title":"Contributing &amp; Development","text":"","tags":["contributing","development","community","open source","collaboration"]},{"location":"contribute/#ways-to-contribute","title":"Ways to Contribute","text":"<p>There are several ways you can contribute to the Locust K8s Operator project:</p>","tags":["contributing","development","community","open source","collaboration"]},{"location":"contribute/#for-everyone","title":"For Everyone","text":"<ul> <li>Reporting Issues: Found a bug or have a feature request? Open an issue \ud83d\udc4b</li> <li>Documentation: Help improve the documentation by suggesting clarifications or additions</li> <li>Community Support: Answer questions and help others in the issue tracker</li> </ul>","tags":["contributing","development","community","open source","collaboration"]},{"location":"contribute/#for-developers","title":"For Developers","text":"<p>Note: The following sections are intended for developers who want to contribute code to the project. If you're just using the operator, you can skip these sections.</p> <ul> <li>Code Contributions: Implement new features or fix bugs</li> <li>Testing: Improve test coverage and test in different environments</li> <li>Review: Review pull requests from other contributors</li> </ul>","tags":["contributing","development","community","open source","collaboration"]},{"location":"contribute/#project-status","title":"Project Status","text":"<p>The project is actively maintained and is under continuous development and improvement. If you have any request or want to chat, kindly open a ticket. If you wish to contribute code and/or ideas, please review the development documentation below.</p>","tags":["contributing","development","community","open source","collaboration"]},{"location":"contribute/#technology-stack","title":"Technology Stack","text":"<p>The operator is built with Go using the controller-runtime framework. Key technologies:</p> <ul> <li>Language: Go 1.24+</li> <li>Framework: controller-runtime / Operator SDK</li> <li>Testing: envtest, Ginkgo, Kind</li> <li>Build: Make, Docker</li> <li>Deployment: Helm, Kustomize</li> </ul>","tags":["contributing","development","community","open source","collaboration"]},{"location":"contribute/#development-documentation","title":"Development Documentation","text":"<p>For developers contributing to the Locust K8s Operator project, we provide detailed documentation on various development aspects:</p> <ul> <li>Local Development Guide: Setting up your development environment</li> <li>Testing Guide: Running unit, integration, and E2E tests</li> <li>Pull Request Process: Guidelines for submitting code changes</li> <li>How It Works: Architecture overview</li> </ul> <p>You can also refer to the comprehensive CONTRIBUTING.MD file in the GitHub repository for more information.</p>","tags":["contributing","development","community","open source","collaboration"]},{"location":"features/","title":"Features","text":"<ul> <li> <p> Cloud Native &amp; Kubernetes Integration</p> <p>Leverage the full power of Kubernetes for distributed performance testing. The operator is designed to be cloud-native, enabling seamless deployment and scaling on any Kubernetes cluster.</p> </li> <li> <p> Automation &amp; CI/CD</p> <p>Integrate performance testing directly into your CI/CD pipelines. Automate the deployment, execution, and teardown of your Locust tests for continuous performance validation.</p> </li> <li> <p> Governance &amp; Resource Management</p> <p>Maintain control over how resources are deployed and used. Configure resource requests and limits for Locust master and worker pods, and even disable CPU limits for performance-sensitive tests.</p> <p> Learn more</p> </li> <li> <p> Observability &amp; Monitoring</p> <p>Gain insights into test results and infrastructure usage. The operator supports Prometheus metrics out-of-the-box, allowing you to build rich monitoring dashboards.</p> <p> Learn more</p> </li> <li> <p> Cost Optimization</p> <p>Optimize cloud costs by deploying resources only when needed and for as long as needed. The operator's automatic cleanup feature ensures that resources are terminated after a test run.</p> <p> Learn more</p> </li> <li> <p> Test Isolation &amp; Parallelism</p> <p>Run multiple tests in parallel with guaranteed isolation. Each test runs in its own set of resources, preventing any cross-test interference.</p> </li> <li> <p> Private Image Registry Support</p> <p>Use images from private registries for your Locust tests. The operator supports <code>imagePullSecrets</code> and configurable <code>imagePullPolicy</code>.</p> <p> Learn more</p> </li> <li> <p> Lib ConfigMap Support</p> <p>Mount lib directories via ConfigMap for your Locust tests. This feature allows you to include shared libraries and modules without modifying test files or patching images, similar to the helm chart's <code>locust_lib_configmap</code> functionality.</p> </li> <li> <p> Advanced Scheduling</p> <p>Control where your Locust pods are scheduled using Kubernetes affinity and taint tolerations. This allows you to run tests on dedicated nodes or in specific availability zones.</p> <p> Learn more</p> </li> <li> <p> Kafka &amp; AWS MSK Integration</p> <p>Seamlessly integrate with Kafka and AWS MSK for performance testing of event-driven architectures. The operator provides out-of-the-box support for authenticated Kafka.</p> <p> Learn more</p> </li> <li> <p> Native OpenTelemetry Support</p> <p>Export traces and metrics directly from Locust using native OpenTelemetry integration. No sidecar required\u2014configure endpoints, protocols, and custom attributes directly in your CR.</p> <p> Learn more</p> </li> <li> <p> Secret &amp; ConfigMap Injection</p> <p>Securely inject credentials, API keys, and configuration from Kubernetes Secrets and ConfigMaps. Supports environment variables and file mounts with automatic prefix handling.</p> <p> Learn more</p> </li> <li> <p> Flexible Volume Mounting</p> <p>Mount test data, certificates, and configuration files from PersistentVolumes, ConfigMaps, or Secrets. Target specific components (master, worker, or both) with fine-grained control.</p> <p> Learn more</p> </li> <li> <p> Separate Resource Specs</p> <p>Configure resources, labels, and annotations independently for master and worker pods. Optimize each component based on its specific needs.</p> <p> Learn more</p> </li> <li> <p> Enhanced Status Tracking</p> <p>Monitor test progress with rich status information including phase (Pending, Running, Succeeded, Failed), Kubernetes conditions, and worker connection status.</p> <p> Learn more</p> </li> </ul>","tags":["features","capabilities","cloud native","kubernetes","automation"]},{"location":"getting_started/","title":"Getting started","text":"<p>Only few simple steps are needed to get a test up and running in the cluster. The following is a step-by-step guide on how to achieve this.</p>","tags":["tutorial","getting started","setup","guide","quickstart"]},{"location":"getting_started/#step-1-write-a-valid-locust-test-script","title":"Step 1: Write a valid Locust test script","text":"<p>For this example, we will be using the following script</p> demo_test.py<pre><code>from locust import HttpUser, task\n\nclass User(HttpUser): # (1)!\n    @task #(2)!\n    def get_employees(self) -&gt; None:\n        \"\"\"Get a list of employees.\"\"\"\n        self.client.get(\"/api/v1/employees\") #(3)!\n</code></pre> <ol> <li>Class representing <code>users</code> that will be simulated by Locust.</li> <li>One or more <code>task</code> that each simulated <code>user</code> will be performing.</li> <li>HTTP call to a specific endpoint.</li> </ol> <p>Note</p> <p>To be able to run performance tests effectivly, an understanding of Locust which is the underline load generation tool is required. All tests must be valid locust tests.</p> <p>Locust provide a very good and detail rich documentation that can be found here.</p>","tags":["tutorial","getting started","setup","guide","quickstart"]},{"location":"getting_started/#step-2-write-a-valid-custom-resource-for-locusttest-crd","title":"Step 2: Write a valid custom resource for LocustTest CRD","text":"<p>A simple custom resource for the previous test can be something like the following example;</p> v2 API (Recommended)v1 API (Deprecated) locusttest-cr.yaml<pre><code>apiVersion: locust.io/v2 # (1)!\nkind: LocustTest # (2)!\nmetadata:\n  name: demo-test # (3)!\nspec:\n  image: locustio/locust:2.20.0 # (4)!\n  master: # (5)!\n    command: \"--locustfile /lotest/src/demo_test.py --host https://dummy.restapiexample.com --users 100 --spawn-rate 3 --run-time 3m\"\n  worker: # (6)!\n    command: \"--locustfile /lotest/src/demo_test.py\"\n    replicas: 3 # (7)!\n  testFiles: # (8)!\n    configMapRef: demo-test-map\n    libConfigMapRef: demo-lib-map\n</code></pre> <ol> <li>API version - use <code>locust.io/v2</code> for new deployments.</li> <li>Resource kind.</li> <li>The name field used by the operator to infer the names of test generated resources.</li> <li>Image to use for the load generation pods. Always specify a version tag.</li> <li>Master configuration block with the command seed.</li> <li>Worker configuration block with command and replicas.</li> <li>The amount of worker nodes to spawn in the cluster (1-500).</li> <li>[Optional] ConfigMap references for test files and libraries.</li> </ol> <p>Deprecated</p> <p>The v1 API is deprecated. Use v2 for new deployments. See the Migration Guide.</p> locusttest-cr.yaml<pre><code>apiVersion: locust.io/v1 # (1)!\nkind: LocustTest # (2)!\nmetadata:\n  name: demo.test # (3)!\nspec:\n  image: locustio/locust:latest # (4)!\n  masterCommandSeed: # (5)!\n    --locustfile /lotest/src/demo_test.py\n    --host https://dummy.restapiexample.com\n    --users 100\n    --spawn-rate 3\n    --run-time 3m\n  workerCommandSeed: --locustfile /lotest/src/demo_test.py # (6)!\n  workerReplicas: 3 # (7)!\n  configMap: demo-test-map # (8)!\n  libConfigMap: demo-lib-map # (9)!\n</code></pre> <ol> <li>API version based on the deployed LocustTest CRD.</li> <li>Resource kind.</li> <li>The name field used by the operator to infer the names of test generated resources.</li> <li>Image to use for the load generation pods</li> <li>Seed command for the master node.</li> <li>Seed command for the worker node.</li> <li>The amount of worker nodes to spawn in the cluster.</li> <li>[Optional] Name of configMap to mount into the pod</li> <li>[Optional] Name of configMap containing lib directory files to mount at <code>/opt/locust/lib</code></li> </ol>","tags":["tutorial","getting started","setup","guide","quickstart"]},{"location":"getting_started/#other-options","title":"Other options","text":"","tags":["tutorial","getting started","setup","guide","quickstart"]},{"location":"getting_started/#labels-and-annotations","title":"Labels and annotations","text":"<p>You can add labels and annotations to generated Pods. For example:</p> v2 APIv1 API (Deprecated) locusttest-cr.yaml<pre><code>apiVersion: locust.io/v2\n...\nspec:\n  image: locustio/locust:2.20.0\n  master:\n    command: \"...\"\n    labels: # (1)!\n      locust.io/role: \"master\"\n      myapp.com/testId: \"abc-123\"\n    annotations: # (2)!\n      myapp.com/threads: \"1000\"\n  worker:\n    command: \"...\"\n    replicas: 3\n    labels:\n      locust.io/role: \"worker\"\n    annotations:\n      myapp.com/version: \"2.1.0\"\n</code></pre> <ol> <li>Labels are attached to master pods and can be used to identify pods belonging to a particular execution context.</li> <li>Annotations can include additional context about a test, such as configuration parameters.</li> </ol> locusttest-cr.yaml<pre><code>apiVersion: locust.io/v1\n...\nspec:\n  image: locustio/locust:latest\n  labels: # (1)!\n    master:\n      locust.io/role: \"master\"\n      myapp.com/testId: \"abc-123\"\n      myapp.com/tenantId: \"xyz-789\"\n    worker:\n      locust.io/role: \"worker\"\n  annotations: # (2)!\n    master:\n      myapp.com/threads: \"1000\"\n      myapp.com/version: \"2.1.0\"\n    worker:\n      myapp.com/version: \"2.1.0\"\n  ...\n</code></pre> <ol> <li>[Optional] Labels are attached to both master and worker pods.</li> <li>[Optional] Annotations are attached to master and worker pods.</li> </ol> <p>Both labels and annotations can be added to the Prometheus configuration, so that metrics are associated with the appropriate information, such as the test and tenant ids. You can read more about this in the Prometheus documentation site.</p>","tags":["tutorial","getting started","setup","guide","quickstart"]},{"location":"getting_started/#step-3-deploy-locust-k8s-operator-in-the-cluster","title":"Step 3: Deploy Locust k8s Operator in the cluster.","text":"<p>The recommended way to install the Operator is by using the official HELM chart. Documentation on how to perform that is available here.</p>","tags":["tutorial","getting started","setup","guide","quickstart"]},{"location":"getting_started/#step-4-deploy-test-as-a-configmap","title":"Step 4: Deploy test as a configMap","text":"<p>For the purposes of this example, the <code>demo_test.py</code> test previously demonstrated will be deployed into the cluster as a configMap that the Operator will mount to the load generation pods. To deploy the test as a configMap, run the bellow command following this template <code>kubectl create configmap &lt;configMap-name&gt; --from-file &lt;your_test.py&gt;</code>:</p> <ul> <li><code>kubectl create configmap demo-test-map --from-file demo_test.py</code></li> </ul>","tags":["tutorial","getting started","setup","guide","quickstart"]},{"location":"getting_started/#step-41-deploy-lib-files-as-a-configmap-optional","title":"Step 4.1: Deploy lib files as a configMap (Optional)","text":"<p>What are lib files and why use this feature?</p> <p>Lib files are Python modules and libraries that your Locust tests depend on. When your tests require custom helper functions, utilities, or shared code that should be available across multiple test files, this feature allows you to package and deploy them alongside your tests.</p>","tags":["tutorial","getting started","setup","guide","quickstart"]},{"location":"getting_started/#how-it-works","title":"How it works","text":"<p>The Locust Kubernetes Operator provides a mechanism to deploy your custom Python libraries as a ConfigMap, which will then be mounted to the <code>/opt/locust/lib</code> directory inside all Locust pods (both master and worker). This allows your test scripts to import and use these libraries.</p> <p>For example, if you have the following structure:</p> <pre><code>project/\n\u251c\u2500\u2500 my_test.py          # Your main Locust test file\n\u2514\u2500\u2500 lib/\n    \u251c\u2500\u2500 helpers.py      # Helper functions\n    \u251c\u2500\u2500 utils.py        # Utility functions\n    \u2514\u2500\u2500 models.py       # Data models\n</code></pre> <p>Your test might import these libraries like this:</p> <pre><code># in my_test.py\nfrom lib.helpers import some_helper_function\nfrom lib.utils import format_data\n</code></pre> <p>To make these imports work when your test runs in Kubernetes, you need to:</p> <ol> <li>Deploy your lib files as a ConfigMap</li> <li>Reference this ConfigMap in your LocustTest custom resource</li> </ol>","tags":["tutorial","getting started","setup","guide","quickstart"]},{"location":"getting_started/#step-by-step-instructions","title":"Step-by-step instructions","text":"<p>1. Create a ConfigMap from your lib directory:</p> <p>You can deploy all library files from a directory:</p> <pre><code># Deploy all files from the lib/ directory as a ConfigMap\nkubectl create configmap demo-lib-map --from-file=lib/\n</code></pre> <p>Alternatively, you can create it from individual files:</p> <pre><code># Deploy specific files as a ConfigMap\nkubectl create configmap demo-lib-map --from-file=lib/helpers.py --from-file=lib/utils.py\n</code></pre> <p>2. Reference the lib ConfigMap in your LocustTest custom resource:</p> v2 APIv1 API (Deprecated) <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: example-locusttest\nspec:\n  image: locustio/locust:2.20.0\n  testFiles:\n    configMapRef: demo-test-map    # Your test script ConfigMap\n    libConfigMapRef: demo-lib-map   # Your lib files ConfigMap\n  master:\n    command: \"--locustfile /lotest/src/my_test.py --host https://example.com\"\n  worker:\n    command: \"--locustfile /lotest/src/my_test.py\"\n    replicas: 2\n</code></pre> <pre><code>apiVersion: locust.io/v1\nkind: LocustTest\nmetadata:\n  name: example-locusttest\nspec:\n  image: locustio/locust:2.20.0\n  masterCommandSeed: \"--locustfile /lotest/src/my_test.py --host https://example.com\"\n  workerCommandSeed: \"--locustfile /lotest/src/my_test.py\"\n  workerReplicas: 2\n  configMap: demo-test-map    # Your test script ConfigMap\n  libConfigMap: demo-lib-map   # Your lib files ConfigMap\n</code></pre> <p>Organizing your code</p> <p>This feature is especially useful when:</p> <ol> <li>You have complex test scenarios that benefit from modular code</li> <li>You want to share code between multiple Locust tests</li> <li>You need to keep your test scripts clean by separating implementation details</li> </ol> <p>Fresh cluster resources</p> <p>Fresh cluster resources are allocated for each running test, meaning that tests DO NOT have any cross impact on each other.</p>","tags":["tutorial","getting started","setup","guide","quickstart"]},{"location":"getting_started/#step-5-start-the-test-by-deploying-the-locusttest-custom-resource","title":"Step 5: Start the test by deploying the LocustTest custom resource.","text":"<p>Deploying a custom resource, signals to the Operator the desire to start a test and thus the Operator starts creating and scheduling all needed resources. To do that, deploy the custom resource following this template <code>kubectl apply -f &lt;valid_cr&gt;.yaml</code>:</p> <ul> <li><code>kubectl apply -f locusttest-cr.yaml</code></li> </ul> <p>Tests are immutable</p> <p>LocustTest CRs are immutable after creation. If you need to change test parameters (image, command, replicas, etc.), delete the CR and recreate it:</p> <pre><code>kubectl delete -f locusttest-cr.yaml\n# Edit locusttest-cr.yaml with your changes\nkubectl apply -f locusttest-cr.yaml\n</code></pre> <p>See How does it work for more details on this design decision.</p>","tags":["tutorial","getting started","setup","guide","quickstart"]},{"location":"getting_started/#step-51-check-cluster-for-running-resources","title":"Step 5.1: Check cluster for running resources","text":"<p>At this point, it is possible to check the cluster and all required resources will be running based on the passed configuration in the custom resource.</p> <p>The Operator will create the following resources in the cluster for each valid custom resource:</p> <ul> <li>A kubernetes service for the master node so it is reachable by other worker nodes.</li> <li>A kubernetes Job to manage the master node.</li> <li>A kubernetes Job to manage the worker node.</li> </ul>","tags":["tutorial","getting started","setup","guide","quickstart"]},{"location":"getting_started/#step-52-access-the-locust-web-ui","title":"Step 5.2: Access the Locust web UI","text":"<p>The Locust web UI allows you to monitor test progress, view statistics, and control test execution in real time.</p> <p>The web UI runs on port 8089 of the master pod. To access it, use <code>kubectl port-forward</code>:</p> <pre><code>kubectl port-forward job/&lt;test-name&gt;-master 8089:8089\n</code></pre> <p>For the example above, this would be:</p> <pre><code>kubectl port-forward job/demo-test-master 8089:8089\n</code></pre> <p>Then open http://localhost:8089 in your browser.</p> <p>Using autostart</p> <p>When <code>master.autostart</code> is enabled (the default), the test begins automatically without interaction via the web UI. The web UI is still useful for monitoring progress and viewing real-time statistics.</p>","tags":["tutorial","getting started","setup","guide","quickstart"]},{"location":"getting_started/#step-6-clear-resources-after-test-run","title":"Step 6: Clear resources after test run","text":"<p>In order to remove the cluster resources after a test run, simply remove the custom resource and the Operator will react to this event by cleaning the cluster of all related resources. To delete a resource, run the below command following this template <code>kubectl delete -f &lt;valid_cr&gt;.yaml</code>:</p> <ul> <li><code>kubectl delete -f locusttest-cr.yaml</code></li> </ul>","tags":["tutorial","getting started","setup","guide","quickstart"]},{"location":"helm_deploy/","title":"HELM Deployment Guide","text":"<p>This guide provides comprehensive instructions for deploying the Locust Kubernetes Operator using its official Helm chart.</p>","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"helm_deploy/#quick-start","title":"Quick Start","text":"<p>For experienced users, here are the essential commands to get the operator running:</p> <pre><code>helm repo add locust-k8s-operator https://abdelrhmanhamouda.github.io/locust-k8s-operator/\nhelm repo update\nhelm install locust-operator locust-k8s-operator/locust-k8s-operator \\\n  --namespace locust-system --create-namespace\n</code></pre>","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"helm_deploy/#installation","title":"Installation","text":"","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"helm_deploy/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster (e.g., Minikube, GKE, EKS, AKS).</li> <li>Helm 3 installed on your local machine.</li> </ul>","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"helm_deploy/#step-1-add-the-helm-repository","title":"Step 1: Add the Helm Repository","text":"<p>First, add the Locust Kubernetes Operator Helm repository to your local Helm client:</p> <pre><code>helm repo add locust-k8s-operator https://abdelrhmanhamouda.github.io/locust-k8s-operator/\n</code></pre> <p>Next, update your local chart repository cache to ensure you have the latest version:</p> <pre><code>helm repo update\n</code></pre>","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"helm_deploy/#step-2-install-the-chart","title":"Step 2: Install the Chart","text":"<p>You can install the chart with a release name of your choice (e.g., <code>locust-operator</code>).</p> <p>Default Installation:</p> <p>To install the chart with the default configuration, run:</p> <pre><code>helm install locust-operator locust-k8s-operator/locust-k8s-operator \\\n  --namespace locust-system --create-namespace\n</code></pre> <p>Installation with a Custom Values File:</p> <p>For more advanced configurations, it's best to use a custom <code>values.yaml</code> file. Create a file named <code>my-values.yaml</code> and add your overrides:</p> v2 Helm Values (Recommended)v1 Helm Values (Deprecated) <pre><code># my-values.yaml\nreplicaCount: 2\n\nlocustPods:\n  resources:\n    limits:\n      cpu: \"2000m\"\n      memory: \"2048Mi\"\n    requests:\n      cpu: \"500m\"\n      memory: \"512Mi\"\n</code></pre> <pre><code># my-values.yaml (old format - still works via compatibility shims)\nreplicaCount: 2\n\nconfig:\n  loadGenerationPods:\n    resource:\n      cpuLimit: \"2000m\"\n      memLimit: \"2048Mi\"\n</code></pre> <p>Then, install the chart, specifying your custom values file and a target namespace:</p> <pre><code>helm install locust-operator locust-k8s-operator/locust-k8s-operator \\\n  --namespace locust-system \\\n  --create-namespace \\\n  -f my-values.yaml\n</code></pre>","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"helm_deploy/#verifying-the-installation","title":"Verifying the Installation","text":"<p>After installation, you can verify that the operator is running correctly by checking the pods in the target namespace:</p> <pre><code>kubectl get pods -n locust-system\n</code></pre> <p>You should see a pod with a name similar to <code>locust-operator-b5c9f4f7-xxxxx</code> in the <code>Running</code> state.</p> <p>To view the operator's logs, run:</p> <pre><code>kubectl logs -f -n locust-system -l app.kubernetes.io/name=locust-k8s-operator\n</code></pre>","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"helm_deploy/#configuration","title":"Configuration","text":"<p>The following tables list the configurable parameters of the Locust Operator Helm chart and their default values.</p> <p>v2.0 Changes</p> <p>The v2 Helm chart has been updated for the Go operator. Java-specific settings (Micronaut, JVM) have been removed. Backward compatibility shims are provided for common settings.</p>","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"helm_deploy/#deployment-settings","title":"Deployment Settings","text":"Parameter Description Default <code>replicaCount</code> Number of replicas for the operator deployment. <code>2</code> <code>image.repository</code> The repository of the Docker image. <code>lotest/locust-k8s-operator</code> <code>image.pullPolicy</code> The image pull policy. <code>IfNotPresent</code> <code>image.tag</code> Overrides the default image tag (defaults to the chart's <code>appVersion</code>). <code>\"\"</code> <code>image.pullSecrets</code> List of image pull secrets. <code>[]</code>","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"helm_deploy/#kubernetes-resources","title":"Kubernetes Resources","text":"Parameter Description Default <code>k8s.clusterRole.enabled</code> Deploy with a cluster-wide role (<code>true</code>) or a namespaced role (<code>false</code>). <code>true</code> <code>serviceAccount.create</code> Specifies whether a service account should be created. <code>true</code> <code>serviceAccount.name</code> The name of the service account to use. If empty and <code>serviceAccount.create</code> is <code>true</code>, a name is generated using the release name. If <code>serviceAccount.create</code> is <code>false</code>, defaults to <code>default</code>. <code>\"\"</code> <code>serviceAccount.annotations</code> Annotations to add to the service account. <code>{}</code>","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"helm_deploy/#operator-resources","title":"Operator Resources","text":"<p>The Go operator requires significantly fewer resources than the Java version:</p> Parameter Description Default <code>resources.limits.memory</code> Operator memory limit. <code>256Mi</code> <code>resources.limits.cpu</code> Operator CPU limit. <code>500m</code> <code>resources.requests.memory</code> Operator memory request. <code>64Mi</code> <code>resources.requests.cpu</code> Operator CPU request. <code>10m</code>","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"helm_deploy/#feature-toggles","title":"Feature Toggles","text":"Parameter Description Default <code>leaderElection.enabled</code> Enable leader election for HA deployments. <code>true</code> <code>metrics.enabled</code> Enable Prometheus metrics endpoint. <code>false</code> <code>metrics.port</code> Metrics server port. <code>8080</code> <code>metrics.secure</code> Use HTTPS for metrics endpoint. <code>false</code> <code>webhook.enabled</code> Enable conversion webhook (requires cert-manager). <code>false</code>","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"helm_deploy/#webhook-configuration","title":"Webhook Configuration","text":"<p>Required when <code>webhook.enabled: true</code>:</p> Parameter Description Default <code>webhook.port</code> Webhook server port. <code>9443</code> <code>webhook.certManager.enabled</code> Use cert-manager for TLS certificate management. <code>true</code> <p>Note</p> <p>The conversion webhook requires cert-manager to be installed in your cluster for automatic TLS certificate management.</p>","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"helm_deploy/#locust-pod-configuration","title":"Locust Pod Configuration","text":"Parameter Description Default <code>locustPods.resources.requests.cpu</code> CPU request for Locust pods. <code>250m</code> <code>locustPods.resources.requests.memory</code> Memory request for Locust pods. <code>128Mi</code> <code>locustPods.resources.requests.ephemeralStorage</code> Ephemeral storage request for Locust pods. <code>30M</code> <code>locustPods.resources.limits.cpu</code> CPU limit for Locust pods. Set to <code>\"\"</code> to unbind. <code>1000m</code> <code>locustPods.resources.limits.memory</code> Memory limit for Locust pods. Set to <code>\"\"</code> to unbind. <code>1024Mi</code> <code>locustPods.resources.limits.ephemeralStorage</code> Ephemeral storage limit for Locust pods. <code>50M</code> <code>locustPods.affinityInjection</code> Enable affinity injection from CRs. <code>true</code> <code>locustPods.tolerationsInjection</code> Enable tolerations injection from CRs. <code>true</code>","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"helm_deploy/#metrics-exporter","title":"Metrics Exporter","text":"Parameter Description Default <code>locustPods.metricsExporter.image</code> Metrics Exporter Docker image. <code>containersol/locust_exporter:v0.5.0</code> <code>locustPods.metricsExporter.port</code> Metrics Exporter port. <code>9646</code> <code>locustPods.metricsExporter.pullPolicy</code> Image pull policy for the metrics exporter. <code>IfNotPresent</code> <code>locustPods.metricsExporter.resources.requests.cpu</code> CPU request for metrics exporter. <code>100m</code> <code>locustPods.metricsExporter.resources.requests.memory</code> Memory request for metrics exporter. <code>64Mi</code> <code>locustPods.metricsExporter.resources.requests.ephemeralStorage</code> Ephemeral storage request for metrics exporter. <code>30M</code> <code>locustPods.metricsExporter.resources.limits.cpu</code> CPU limit for metrics exporter. <code>250m</code> <code>locustPods.metricsExporter.resources.limits.memory</code> Memory limit for metrics exporter. <code>128Mi</code> <code>locustPods.metricsExporter.resources.limits.ephemeralStorage</code> Ephemeral storage limit for metrics exporter. <code>50M</code> <p>Tip</p> <p>When using OpenTelemetry (<code>spec.observability.openTelemetry.enabled: true</code>), the metrics exporter sidecar is not deployed.</p>","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"helm_deploy/#job-configuration","title":"Job Configuration","text":"Parameter Description Default <code>locustPods.ttlSecondsAfterFinished</code> TTL for finished jobs. Set to <code>\"\"</code> to disable. <code>\"\"</code>","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"helm_deploy/#kafka-configuration","title":"Kafka Configuration","text":"Parameter Description Default <code>kafka.enabled</code> Enable Kafka configuration injection. <code>false</code> <code>kafka.bootstrapServers</code> Kafka bootstrap servers. <code>localhost:9092</code> <code>kafka.security.enabled</code> Enable Kafka security. <code>false</code> <code>kafka.security.protocol</code> Security protocol (<code>SASL_SSL</code>, <code>SASL_PLAINTEXT</code>, etc.). <code>SASL_PLAINTEXT</code> <code>kafka.security.saslMechanism</code> SASL mechanism. <code>SCRAM-SHA-512</code> <code>kafka.security.jaasConfig</code> JAAS configuration string. <code>\"\"</code> <code>kafka.credentials.secretName</code> Name of secret containing Kafka credentials. <code>\"\"</code> <code>kafka.credentials.usernameKey</code> Key in secret for username. <code>username</code> <code>kafka.credentials.passwordKey</code> Key in secret for password. <code>password</code>","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"helm_deploy/#opentelemetry-collector-optional","title":"OpenTelemetry Collector (Optional)","text":"<p>Deploy an OTel Collector alongside the operator:</p> Parameter Description Default <code>otelCollector.enabled</code> Deploy OTel Collector. <code>false</code> <code>otelCollector.image</code> Collector image. <code>otel/opentelemetry-collector-contrib:0.92.0</code> <code>otelCollector.replicas</code> Number of collector replicas. <code>1</code> <code>otelCollector.resources.requests.cpu</code> CPU request for collector. <code>50m</code> <code>otelCollector.resources.requests.memory</code> Memory request for collector. <code>64Mi</code> <code>otelCollector.resources.limits.cpu</code> CPU limit for collector. <code>200m</code> <code>otelCollector.resources.limits.memory</code> Memory limit for collector. <code>256Mi</code> <code>otelCollector.config</code> OTel Collector configuration (YAML string). See values.yaml","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"helm_deploy/#pod-scheduling","title":"Pod Scheduling","text":"Parameter Description Default <code>nodeSelector</code> Node selector for scheduling the operator pod. <code>{}</code> <code>tolerations</code> Tolerations for scheduling the operator pod. <code>[]</code> <code>affinity</code> Affinity rules for scheduling the operator pod. <code>{}</code> <code>podAnnotations</code> Annotations to add to the operator pod. <code>{}</code>","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"helm_deploy/#backward-compatibility","title":"Backward Compatibility","text":"<p>The following v1 paths are still supported via helper functions:</p> Old Path (v1) New Path (v2) <code>config.loadGenerationPods.resource.cpuRequest</code> <code>locustPods.resources.requests.cpu</code> <code>config.loadGenerationPods.resource.memLimit</code> <code>locustPods.resources.limits.memory</code> <code>config.loadGenerationPods.affinity.enableCrInjection</code> <code>locustPods.affinityInjection</code> <code>config.loadGenerationPods.kafka.*</code> <code>kafka.*</code> <p>Removed Settings</p> <p>The following Java-specific settings have been removed and have no effect in v2:</p> <ul> <li><code>appPort</code> - Fixed at 8081</li> <li><code>micronaut.*</code> - No Micronaut in Go operator</li> <li><code>livenessProbe.*</code> / <code>readinessProbe.*</code> - Fixed probes on <code>/healthz</code> and <code>/readyz</code></li> </ul>","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"helm_deploy/#upgrading-the-chart","title":"Upgrading the Chart","text":"<p>To upgrade an existing release to a new version, use the <code>helm upgrade</code> command:</p> <pre><code>helm upgrade locust-operator locust-k8s-operator/locust-k8s-operator -f my-values.yaml\n</code></pre>","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"helm_deploy/#uninstalling-the-chart","title":"Uninstalling the Chart","text":"<p>To uninstall and delete the <code>locust-operator</code> deployment, run:</p> <pre><code>helm uninstall locust-operator\n</code></pre> <p>This command will remove all the Kubernetes components associated with the chart and delete the release.</p>","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"helm_deploy/#next-steps","title":"Next Steps","text":"<p>Once the operator is installed, you're ready to start running performance tests! Head over to the Getting Started guide to learn how to deploy your first <code>LocustTest</code>.</p>","tags":["deployment","helm","installation","kubernetes","setup"]},{"location":"how_does_it_work/","title":"How does it work","text":"<p>To run a performance test, basic configuration is provided through a simple and intuitive Kubernetes custom resource. Once deployed, the Operator does all the heavy work of creating and scheduling the resources while making sure that all created load generation pods can effectively communicate with each other.</p>"},{"location":"how_does_it_work/#architecture-overview","title":"Architecture Overview","text":"<p>The Locust K8s Operator is built using Go with the controller-runtime framework, following the standard Kubernetes operator pattern.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Kubernetes Cluster                       \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                          \u2502\n\u2502  \u2502   LocustTest  \u2502 \u25c4\u2500\u2500 User creates Custom Resource         \u2502\n\u2502  \u2502      CR       \u2502                                          \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                          \u2502\n\u2502          \u2502 watches                                          \u2502\n\u2502          \u25bc                                                  \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502   Operator    \u2502\u2500\u2500\u2500\u2500\u25ba\u2502  Creates owned resources:       \u2502  \u2502\n\u2502  \u2502  Controller   \u2502     \u2502  \u2022 Master Service               \u2502  \u2502\n\u2502  \u2502               \u2502     \u2502  \u2022 Master Job (1 pod)           \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502  \u2022 Worker Job (N pods)          \u2502  \u2502\n\u2502          \u2502             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502          \u2502 updates                                          \u2502\n\u2502          \u25bc                                                  \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                          \u2502\n\u2502  \u2502    Status     \u2502 \u25c4\u2500\u2500 Phase, Conditions, Worker Count      \u2502\n\u2502  \u2502  Subresource  \u2502                                          \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"how_does_it_work/#key-design-decisions","title":"Key Design Decisions","text":""},{"location":"how_does_it_work/#immutable-tests","title":"Immutable Tests","text":"<p>Tests are immutable by design. Once a LocustTest CR is created, updates to its <code>spec</code> are ignored by the operator. The operator sets a <code>SpecDrifted</code> condition on the CR to indicate when spec changes have been detected but not applied.</p> <p>To change test parameters (image, commands, replicas, etc.), delete and recreate the CR:</p> <pre><code># Delete the existing test\nkubectl delete locusttest &lt;test-name&gt;\n\n# Edit your CR YAML with the desired changes, then re-apply\nkubectl apply -f locusttest-cr.yaml\n</code></pre> <p>This design ensures:</p> <ul> <li>Predictable behavior \u2014 each test run uses exactly the configuration it was created with</li> <li>Clean test isolation \u2014 no mid-flight configuration drift</li> <li>Simple lifecycle \u2014 create, run, observe, delete</li> </ul>"},{"location":"how_does_it_work/#owner-references","title":"Owner References","text":"<p>All created resources (Jobs, Services) have owner references pointing to the LocustTest CR. This enables:</p> <ul> <li>Automatic garbage collection on CR deletion</li> <li>Clear resource ownership in <code>kubectl get</code></li> <li>No orphaned resources</li> </ul>"},{"location":"how_does_it_work/#status-tracking","title":"Status Tracking","text":"<p>The operator maintains rich status information:</p> <pre><code>status:\n  phase: Running\n  expectedWorkers: 5\n  connectedWorkers: 5\n  startTime: \"2026-01-15T10:00:00Z\"\n  conditions:\n    - type: Ready\n      status: \"True\"\n      lastTransitionTime: \"2026-01-15T10:00:05Z\"\n      reason: AllWorkersConnected\n      message: \"All 5 workers connected to master\"\n</code></pre>"},{"location":"how_does_it_work/#demo","title":"Demo","text":"<p>Since a \"Picture Is Worth a Thousand Words\", here is a gif! </p>"},{"location":"how_does_it_work/#steps-performed-in-demo","title":"Steps performed in demo","text":"<ul> <li> Test ConfigMap created in cluster</li> <li> LocustTest CR deployed into the cluster</li> <li> The Operator creating, configuring and scheduling test resources on CR creation event</li> <li> The Operator cleaning up test resources after test CR has been removed</li> </ul>"},{"location":"integration-testing/","title":"Testing Guide","text":"<p>This document describes the comprehensive testing setup for the Locust K8s Operator, covering unit tests, integration tests (envtest), and end-to-end tests.</p>"},{"location":"integration-testing/#overview","title":"Overview","text":"<p>The operator uses a multi-layered testing strategy:</p> Test Type Framework Scope Speed Unit Tests Go testing Individual functions Fast (~seconds) Integration Tests envtest Controller + API Server Medium (~30s) E2E Tests Ginkgo + Kind Full cluster deployment Slow (~5-10min)"},{"location":"integration-testing/#test-structure","title":"Test Structure","text":"<pre><code>locust-k8s-operator/\n\u251c\u2500\u2500 api/\n\u2502   \u251c\u2500\u2500 v1/\n\u2502   \u2502   \u2514\u2500\u2500 *_test.go              # v1 API tests\n\u2502   \u2514\u2500\u2500 v2/\n\u2502       \u251c\u2500\u2500 *_test.go              # v2 API tests\n\u2502       \u2514\u2500\u2500 locusttest_webhook_test.go  # Webhook validation tests\n\u251c\u2500\u2500 internal/\n\u2502   \u251c\u2500\u2500 config/\n\u2502   \u2502   \u2514\u2500\u2500 config_test.go         # Configuration tests\n\u2502   \u251c\u2500\u2500 controller/\n\u2502   \u2502   \u251c\u2500\u2500 suite_test.go          # envtest setup\n\u2502   \u2502   \u251c\u2500\u2500 locusttest_controller_test.go  # Unit tests\n\u2502   \u2502   \u2514\u2500\u2500 integration_test.go    # Integration tests\n\u2502   \u2514\u2500\u2500 resources/\n\u2502       \u251c\u2500\u2500 job_test.go            # Job builder tests\n\u2502       \u251c\u2500\u2500 service_test.go        # Service builder tests\n\u2502       \u251c\u2500\u2500 labels_test.go         # Label builder tests\n\u2502       \u251c\u2500\u2500 env_test.go            # Environment builder tests\n\u2502       \u2514\u2500\u2500 command_test.go        # Command builder tests\n\u2514\u2500\u2500 test/\n    \u2514\u2500\u2500 e2e/\n        \u251c\u2500\u2500 e2e_suite_test.go      # E2E test setup\n        \u2514\u2500\u2500 e2e_test.go            # E2E test scenarios\n</code></pre>"},{"location":"integration-testing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Go 1.24+: Required for running tests</li> <li>Docker: Required for E2E tests (Kind)</li> <li>Kind: Required for E2E tests</li> </ul>"},{"location":"integration-testing/#running-tests","title":"Running Tests","text":""},{"location":"integration-testing/#unit-integration-tests-envtest","title":"Unit &amp; Integration Tests (envtest)","text":"<p>The primary test command runs both unit tests and integration tests using envtest:</p> <pre><code># Run all tests with coverage\nmake test\n\n# Run tests with verbose output\ngo test ./... -v\n\n# Run specific package tests\ngo test ./internal/resources/... -v\ngo test ./internal/controller/... -v\ngo test ./api/v2/... -v\n\n# Run specific test by name\ngo test ./internal/controller/... -v -run TestReconcile\n\n# Generate coverage report\nmake test\ngo tool cover -html=cover.out -o coverage.html\n</code></pre>"},{"location":"integration-testing/#e2e-tests-kind","title":"E2E Tests (Kind)","text":"<p>End-to-end tests run against a real Kubernetes cluster using Kind:</p> <pre><code># Run E2E tests (creates Kind cluster automatically)\nmake test-e2e\n\n# Run E2E tests with verbose output\nKIND_CLUSTER=locust-test go test ./test/e2e/ -v -ginkgo.v\n\n# Cleanup E2E test cluster\nmake cleanup-test-e2e\n</code></pre>"},{"location":"integration-testing/#ci-pipeline","title":"CI Pipeline","text":"<p>All tests run automatically in GitHub Actions:</p> <pre><code># Run the same checks as CI locally\nmake ci\n\n# This runs:\n# - make lint (golangci-lint)\n# - make test (unit + integration tests)\n</code></pre>"},{"location":"integration-testing/#test-fixtures","title":"Test Fixtures","text":"<p>Test fixtures and sample data are located in:</p> <ul> <li><code>internal/testdata/</code> - Test fixtures for unit tests</li> <li><code>config/samples/</code> - Sample CRs for integration/E2E tests</li> </ul>"},{"location":"integration-testing/#troubleshooting","title":"Troubleshooting","text":""},{"location":"integration-testing/#common-issues","title":"Common Issues","text":""},{"location":"integration-testing/#envtest-binary-issues","title":"envtest Binary Issues","text":"<pre><code># Re-download envtest binaries\nmake setup-envtest\n\n# Verify binaries are installed\nls bin/k8s/\n</code></pre>"},{"location":"integration-testing/#test-timeouts","title":"Test Timeouts","text":"<pre><code># Increase timeout for slow systems\ngo test ./... -v -timeout 10m\n</code></pre>"},{"location":"integration-testing/#kind-cluster-issues","title":"Kind Cluster Issues","text":"<pre><code># Check if cluster exists\nkind get clusters\n\n# Delete and recreate\nkind delete cluster --name locust-k8s-operator-test-e2e\nmake test-e2e\n</code></pre>"},{"location":"integration-testing/#debug-mode","title":"Debug Mode","text":"<p>Run tests with verbose logging: <pre><code># Verbose test output\ngo test ./internal/controller/... -v -ginkgo.v\n\n# With debug logs from controller\ngo test ./internal/controller/... -v -args -zap-log-level=debug\n</code></pre></p>"},{"location":"integration-testing/#writing-new-tests","title":"Writing New Tests","text":""},{"location":"integration-testing/#guidelines","title":"Guidelines","text":"<ol> <li>Unit tests: Test pure functions in isolation</li> <li>Integration tests: Test controller behavior with envtest</li> <li>E2E tests: Test user-facing scenarios in real cluster</li> </ol>"},{"location":"integration-testing/#test-naming-conventions","title":"Test Naming Conventions","text":"<pre><code>// Unit tests: Test&lt;FunctionName&gt;_&lt;Scenario&gt;\nfunc TestBuildMasterJob_WithEnvConfig(t *testing.T) {}\n\n// Integration tests: Describe/Context/It\nDescribe(\"LocustTest Controller\", func() {\n    Context(\"When creating a LocustTest\", func() {\n        It(\"Should create master Job\", func() {})\n    })\n})\n</code></pre>"},{"location":"integration-testing/#adding-integration-tests","title":"Adding Integration Tests","text":"<ol> <li>Add test to <code>internal/controller/integration_test.go</code></li> <li>Use <code>k8sClient</code> for Kubernetes operations</li> <li>Use <code>Eventually</code> for async assertions</li> <li>Clean up resources in <code>AfterEach</code></li> </ol>"},{"location":"integration-testing/#related-documentation","title":"Related Documentation","text":"<ul> <li>Local Development - Development setup</li> <li>Contributing - Contribution guidelines</li> <li>Pull Request Process - PR workflow</li> </ul>"},{"location":"license/","title":"License","text":"<p>Open source licensed under Apache-2.0 license (see LICENSE file for details).</p>"},{"location":"local-development/","title":"Local Development Guide","text":"<p>This guide describes the setup and workflow for local development on the Locust K8s Operator project. It's intended for developers who want to contribute code changes.</p>"},{"location":"local-development/#development-setup","title":"Development Setup","text":""},{"location":"local-development/#prerequisites","title":"Prerequisites","text":"<ul> <li>Go 1.24+: Required for building the operator</li> <li>Docker: Running Docker daemon for building images</li> <li>kubectl: Kubernetes CLI for cluster interaction</li> <li>Kind or Minikube: Local Kubernetes cluster for testing</li> <li>Helm 3.x: For chart packaging and installation</li> </ul>"},{"location":"local-development/#initial-setup","title":"Initial Setup","text":"<ol> <li> <p>Clone the repository:    <pre><code>git clone https://github.com/AbdelrhmanHamouda/locust-k8s-operator.git\ncd locust-k8s-operator\n</code></pre></p> </li> <li> <p>Install dependencies and tools:    <pre><code># Download Go dependencies\nmake tidy\n\n# Install development tools (controller-gen, envtest, etc.)\nmake controller-gen\nmake envtest\nmake kustomize\n</code></pre></p> </li> </ol>"},{"location":"local-development/#development-guidelines","title":"Development Guidelines","text":"<ul> <li> <p>This project follows the Conventional Commits standard to automate Semantic Versioning and Keep A Changelog with Commitizen.</p> </li> <li> <p>All code should include appropriate tests. See the integration testing guide for details on the test setup.</p> </li> </ul>"},{"location":"local-development/#common-development-commands","title":"Common Development Commands","text":"<p>The project uses a <code>Makefile</code> for common development tasks. Run <code>make help</code> to see all available targets.</p>"},{"location":"local-development/#build-test","title":"Build &amp; Test","text":"<pre><code># Build the operator binary\nmake build\n\n# Run all tests (unit + integration via envtest)\nmake test\n\n# Run linter\nmake lint\n\n# Run linter with auto-fix\nmake lint-fix\n\n# Run all CI checks locally\nmake ci\n</code></pre>"},{"location":"local-development/#code-generation","title":"Code Generation","text":"<pre><code># Generate CRDs, RBAC, and webhook manifests\nmake manifests\n\n# Generate DeepCopy implementations\nmake generate\n\n# Format code\nmake fmt\n\n# Run go vet\nmake vet\n</code></pre>"},{"location":"local-development/#running-locally","title":"Running Locally","text":"<pre><code># Run the operator locally against your current kubeconfig cluster\nmake run\n\n# Install CRDs into the cluster\nmake install\n\n# Uninstall CRDs from the cluster\nmake uninstall\n</code></pre>"},{"location":"local-development/#local-testing-with-kind","title":"Local Testing with Kind","text":"<p>For local development and testing, Kind (Kubernetes in Docker) is the recommended approach.</p>"},{"location":"local-development/#steps","title":"Steps","text":"<ol> <li>Create a Kind Cluster</li> </ol> <pre><code>kind create cluster --name locust-dev\n</code></pre> <ol> <li>Build and Load the Docker Image</li> </ol> <pre><code># Build the Docker image\nmake docker-build IMG=locust-k8s-operator:dev\n\n# Load the image into Kind\nkind load docker-image locust-k8s-operator:dev --name locust-dev\n</code></pre> <ol> <li>Deploy the Operator</li> </ol> <p>Option A: Using kustomize (for development):    <pre><code># Deploy CRDs and operator\nmake deploy IMG=locust-k8s-operator:dev\n</code></pre></p> <p>Option B: Using Helm (for production-like testing):    <pre><code># Package the Helm chart\nhelm package ../charts/locust-k8s-operator\n\n# Install with local image\nhelm install locust-operator locust-k8s-operator-*.tgz \\\n  --set image.repository=locust-k8s-operator \\\n  --set image.tag=dev \\\n  --set image.pullPolicy=IfNotPresent\n</code></pre></p> <ol> <li>Verify the Deployment</li> </ol> <p>!!! note \"Development vs Production Namespaces\"        The <code>make deploy</code> command generates a namespace based on your project name. For production deployments, use the <code>locust-system</code> namespace as documented in the Helm Deployment Guide.</p> <pre><code># Check pods in the generated namespace\nkubectl get pods -A | grep locust\n\n# Follow operator logs\nkubectl logs -f -n &lt;namespace&gt; deployment/&lt;deployment-name&gt;\n</code></pre> <ol> <li>Test with a Sample CR</li> </ol> <pre><code># Create a test ConfigMap with a simple Locust script\nkubectl create configmap locust-test --from-literal=locustfile.py='\nfrom locust import HttpUser, task\nclass TestUser(HttpUser):\n    @task\n    def hello(self):\n        self.client.get(\"/\")\n'\n\n# Apply a sample LocustTest CR\nkubectl apply -f config/samples/locust_v2_locusttest.yaml\n\n# Watch the resources\nkubectl get locusttests,jobs,pods -w\n</code></pre> <ol> <li>Cleanup</li> </ol> <pre><code># Remove the operator\nmake undeploy\n\n# Delete the Kind cluster\nkind delete cluster --name locust-dev\n</code></pre>"},{"location":"local-development/#writing-documentation","title":"Writing Documentation","text":"<p>All documentation is located under the <code>docs/</code> directory. The documentation is hosted on GitHub Pages and updated automatically with each release. To manage and build the documentation, the project uses MkDocs &amp; Material for MkDocs framework.</p>"},{"location":"local-development/#preview-documentation-locally","title":"Preview Documentation Locally","text":"<pre><code># Install MkDocs (if not installed)\npip install mkdocs mkdocs-material\n\n# Serve documentation locally\nmkdocs serve\n\n# Build documentation\nmkdocs build --strict\n</code></pre> <p>During development, the CI workflow will build the documentation as part of the validation.</p>"},{"location":"metrics_and_dashboards/","title":"Metrics &amp; Dashboards","text":"","tags":["monitoring","metrics","dashboards","prometheus","observability"]},{"location":"metrics_and_dashboards/#opentelemetry-metrics-traces","title":"OpenTelemetry Metrics &amp; Traces","text":"<p>New in v2.0</p> <p>Native OpenTelemetry support is available in the v2 API.</p>","tags":["monitoring","metrics","dashboards","prometheus","observability"]},{"location":"metrics_and_dashboards/#native-opentelemetry-support","title":"Native OpenTelemetry Support","text":"<p>Locust 2.x includes native OpenTelemetry support, which the operator can configure automatically. This provides both metrics and distributed tracing without requiring the metrics exporter sidecar.</p>","tags":["monitoring","metrics","dashboards","prometheus","observability"]},{"location":"metrics_and_dashboards/#configuring-otel","title":"Configuring OTel","text":"<p>Enable OpenTelemetry in your LocustTest CR:</p> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: otel-test\nspec:\n  image: locustio/locust:2.20.0\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://example.com\"\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 5\n  observability:\n    openTelemetry:\n      enabled: true\n      endpoint: \"otel-collector.monitoring:4317\"\n      protocol: \"grpc\"\n</code></pre> <p>See Advanced Topics - OpenTelemetry for detailed configuration options.</p>","tags":["monitoring","metrics","dashboards","prometheus","observability"]},{"location":"metrics_and_dashboards/#otel-collector-setup","title":"OTel Collector Setup","text":"<p>For a complete observability setup, deploy an OTel Collector. Example configuration:</p> <pre><code># otel-collector-config.yaml\nreceivers:\n  otlp:\n    protocols:\n      grpc:\n        endpoint: 0.0.0.0:4317\n      http:\n        endpoint: 0.0.0.0:4318\n\nexporters:\n  prometheus:\n    endpoint: 0.0.0.0:8889\n  otlphttp:\n    endpoint: http://jaeger-collector:4318\n    tls:\n      insecure: true\n\nservice:\n  pipelines:\n    metrics:\n      receivers: [otlp]\n      exporters: [prometheus]\n    traces:\n      receivers: [otlp]\n      exporters: [otlphttp]\n</code></pre> <p>Tip</p> <p>The Helm chart includes an optional OTel Collector deployment. Enable it with <code>otelCollector.enabled: true</code>.</p>","tags":["monitoring","metrics","dashboards","prometheus","observability"]},{"location":"metrics_and_dashboards/#operator-metrics","title":"Operator Metrics","text":"<p>The Go operator exposes controller-runtime metrics on port 8080:</p> Metric Description <code>controller_runtime_reconcile_total</code> Total reconciliations <code>controller_runtime_reconcile_errors_total</code> Reconciliation errors <code>controller_runtime_reconcile_time_seconds</code> Reconciliation duration <code>workqueue_depth</code> Current queue depth <code>workqueue_adds_total</code> Items added to queue <p>These metrics can be scraped by Prometheus using the standard <code>/metrics</code> endpoint on the operator pod.</p>","tags":["monitoring","metrics","dashboards","prometheus","observability"]},{"location":"metrics_and_dashboards/#enabling-operator-metrics","title":"Enabling Operator Metrics","text":"<p>Enable metrics in your Helm values:</p> <pre><code>metrics:\n  enabled: true\n</code></pre> <p>Then configure Prometheus to scrape the operator:</p> <pre><code>scrape_configs:\n  - job_name: 'locust-operator'\n    kubernetes_sd_configs:\n      - role: pod\n    relabel_configs:\n      - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_name]\n        action: keep\n        regex: locust-k8s-operator\n      - source_labels: [__address__]\n        action: replace\n        regex: ([^:]+)(?::\\d+)?\n        replacement: $1:8080\n        target_label: __address__\n</code></pre>","tags":["monitoring","metrics","dashboards","prometheus","observability"]},{"location":"metrics_and_dashboards/#servicemonitor-prometheus-operator","title":"ServiceMonitor (Prometheus Operator)","text":"<p>If using the Prometheus Operator, create a ServiceMonitor to automatically discover and scrape operator metrics:</p> HTTP MetricsHTTPS Metrics (Recommended) <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: locust-operator-metrics\n  namespace: locust-operator-system\n  labels:\n    app.kubernetes.io/name: locust-k8s-operator\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: locust-k8s-operator\n  endpoints:\n    - port: metrics\n      path: /metrics\n      interval: 30s\n</code></pre> <p>Ensure Helm values have: <pre><code>metrics:\n  enabled: true\n  secure: false  # HTTP metrics\n</code></pre></p> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: locust-operator-metrics\n  namespace: locust-operator-system\n  labels:\n    app.kubernetes.io/name: locust-k8s-operator\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: locust-k8s-operator\n  endpoints:\n    - port: metrics\n      path: /metrics\n      interval: 30s\n      scheme: https\n      bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token\n      tlsConfig:\n        insecureSkipVerify: true  # For development only\n</code></pre> <p>Ensure Helm values have: <pre><code>metrics:\n  enabled: true\n  secure: true  # Enable HTTPS metrics (default: false)\n</code></pre></p> <p>Production TLS</p> <p>For production, use cert-manager to manage TLS certificates instead of <code>insecureSkipVerify: true</code>. See the operator's <code>config/prometheus/</code> directory for examples.</p>","tags":["monitoring","metrics","dashboards","prometheus","observability"]},{"location":"metrics_and_dashboards/#operator-metrics-queries","title":"Operator Metrics Queries","text":"<p>Useful PromQL queries for operator monitoring:</p> <pre><code># Reconciliation rate (per second)\nrate(controller_runtime_reconcile_total[5m])\n\n# Reconciliation error rate\nrate(controller_runtime_reconcile_errors_total[5m])\n\n# Average reconciliation duration\nrate(controller_runtime_reconcile_time_seconds_sum[5m]) \n  / rate(controller_runtime_reconcile_time_seconds_count[5m])\n\n# Current workqueue depth\nworkqueue_depth\n\n# Queue processing rate\nrate(workqueue_adds_total[5m])\n</code></pre>","tags":["monitoring","metrics","dashboards","prometheus","observability"]},{"location":"metrics_and_dashboards/#locust-test-metrics","title":"Locust Test Metrics","text":"<p>Two Metrics Approaches - Choose One</p> <p>The operator provides two mutually exclusive methods for collecting Locust test metrics:</p> <p>1. Prometheus Exporter Sidecar (default, v1 &amp; v2 API)</p> <ul> <li>Uses <code>containersol/locust_exporter</code> sidecar on port 9646</li> <li>Exposes Prometheus-formatted metrics</li> <li>Works with Prometheus scraping</li> <li>Documented in this section below</li> </ul> <p>2. Native OpenTelemetry (v2 API only)</p> <ul> <li>Locust exports directly via OTLP protocol</li> <li>No sidecar container needed</li> <li>Metrics sent to OTel Collector</li> <li>See OpenTelemetry section above</li> </ul> <p>When OTel is enabled, the exporter sidecar is NOT deployed. All Prometheus exporter documentation below only applies to non-OTel mode.</p>","tags":["monitoring","metrics","dashboards","prometheus","observability"]},{"location":"metrics_and_dashboards/#metrics-exporter-sidecar-non-otel-mode","title":"Metrics Exporter Sidecar (Non-OTel Mode)","text":"<p>When OpenTelemetry is not enabled, the operator automatically injects a Prometheus metrics exporter sidecar into the Locust master pod. This exporter scrapes Locust's built-in stats endpoint and exposes metrics in Prometheus format.</p> <p>What the Operator Creates Automatically:</p> <ol> <li>Metrics Exporter Sidecar Container:</li> <li>Image: <code>containersol/locust_exporter:v0.5.0</code></li> <li>Port: 9646</li> <li> <p>Path: <code>/metrics</code></p> </li> <li> <p>Kubernetes Service: <code>&lt;test-name&gt;-master</code></p> </li> <li>Includes metrics port 9646</li> <li> <p>Provides stable DNS endpoint</p> </li> <li> <p>Pod Annotations (for Prometheus auto-discovery):    <pre><code>prometheus.io/scrape: \"true\"\nprometheus.io/path: \"/metrics\"\nprometheus.io/port: \"9646\"\n</code></pre></p> </li> </ol> <p>No manual setup required - the operator handles everything.</p>","tags":["monitoring","metrics","dashboards","prometheus","observability"]},{"location":"metrics_and_dashboards/#available-locust-metrics","title":"Available Locust Metrics","text":"<p>The exporter provides these key metrics from Locust:</p> Metric Type Description <code>locust_requests_total</code> Counter Total number of requests <code>locust_requests_current_rps</code> Gauge Current requests per second <code>locust_requests_current_fail_per_sec</code> Gauge Current failures per second <code>locust_requests_avg_response_time</code> Gauge Average response time (ms) <code>locust_requests_min_response_time</code> Gauge Minimum response time (ms) <code>locust_requests_max_response_time</code> Gauge Maximum response time (ms) <code>locust_requests_avg_content_length</code> Gauge Average response size (bytes) <code>locust_users</code> Gauge Current number of simulated users <code>locust_errors</code> Counter Total errors by type <p>For the complete list, see the locust_exporter documentation.</p>","tags":["monitoring","metrics","dashboards","prometheus","observability"]},{"location":"metrics_and_dashboards/#locust-metrics-queries","title":"Locust Metrics Queries","text":"<p>Useful PromQL queries for load test monitoring:</p> <pre><code># Total request rate across all endpoints\nsum(rate(locust_requests_total[1m]))\n\n# Error rate\nsum(rate(locust_errors[1m]))\n\n# Average response time\navg(locust_requests_avg_response_time)\n\n# Max response time (locust_exporter exposes gauges, not histograms)\nmax(locust_requests_max_response_time)\n\n# Current active users\nsum(locust_users)\n\n# Request rate by endpoint\nsum(rate(locust_requests_total[1m])) by (name, method)\n\n# Error percentage\n100 * sum(rate(locust_errors[1m])) \n  / sum(rate(locust_requests_total[1m]))\n</code></pre>","tags":["monitoring","metrics","dashboards","prometheus","observability"]},{"location":"metrics_and_dashboards/#integration-examples","title":"Integration Examples","text":"<p>The metrics are automatically exposed by the operator-created Service and pod annotations. Simply configure your monitoring tools to discover them:</p> <p>Prometheus: Configure Kubernetes service discovery to scrape pods with <code>prometheus.io/scrape: \"true\"</code> annotation. The operator adds these annotations automatically - no manual configuration of individual tests needed.</p> <p>Grafana: Connect to your Prometheus datasource and create dashboards using the PromQL queries above. Import panels from existing Locust dashboard examples.</p> <p>NewRelic: Deploy a Prometheus agent configured to scrape Kubernetes pods with <code>prometheus.io/scrape: true</code> and forward metrics to NewRelic. See Issue #118 for production deployment patterns.</p> <p>DataDog: Configure the DataDog agent's Prometheus integration to auto-discover and scrape pods with <code>prometheus.io/*</code> annotations. The DataDog agent automatically finds operator-created test pods.</p> <p>Production Deployment</p> <p>For large-scale deployments, see Issue #118 which documents production patterns used with thousands of tests in NewRelic and DataDog environments.</p>","tags":["monitoring","metrics","dashboards","prometheus","observability"]},{"location":"migration/","title":"Migration Guide: v1 to v2","text":"<p>This guide helps existing users of the Locust Kubernetes Operator upgrade from v1 to v2. The v2 release is a complete rewrite in Go, bringing significant performance improvements and new features.</p>","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#overview","title":"Overview","text":"","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#why-we-rewrote-in-go","title":"Why We Rewrote in Go","text":"<p>The v2 operator was rewritten from Java to Go for several key reasons:</p> Aspect Java (v1) Go (v2) Memory footprint ~256MB ~64MB Startup time ~60 seconds &lt;1 second Framework Java Operator SDK Operator SDK / controller-runtime Ecosystem alignment Minority Majority of K8s operators","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#what-changes-for-users","title":"What Changes for Users","text":"<ul> <li>API Version: New <code>locust.io/v2</code> API with grouped configuration</li> <li>Backward Compatibility: v1 CRs continue to work via automatic conversion</li> <li>New Features: OpenTelemetry, secret injection, volume mounting, separate resource specs</li> <li>Helm Chart: Updated values structure (backward compatible)</li> </ul>","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#compatibility-guarantees","title":"Compatibility Guarantees","text":"<ul> <li>v1 API: Fully supported via conversion webhook (deprecated, will be removed in v3)</li> <li>Existing CRs: Work without modification</li> <li>Helm Values: Backward compatibility shims for common settings</li> </ul>","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#before-you-begin","title":"Before You Begin","text":"","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes 1.25+</li> <li>Helm 3.x</li> <li>cert-manager v1.14+ (required for conversion webhook)</li> </ul>","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#backup-recommendations","title":"Backup Recommendations","text":"<p>Before upgrading, back up your existing resources:</p> <pre><code># Export all LocustTest CRs\nkubectl get locusttests -A -o yaml &gt; locusttests-backup.yaml\n\n# Export operator Helm values\nhelm get values locust-operator -n &lt;namespace&gt; &gt; values-backup.yaml\n</code></pre> <p>Critical: Webhook Required for v1 API Compatibility</p> <p>If you have existing v1 <code>LocustTest</code> CRs, the conversion webhook is required for them to continue working after upgrading to v2. Without it, v1 CRs will fail CRD schema validation.</p> <p>You must:</p> <ol> <li>Install cert-manager before upgrading</li> <li>Enable the webhook during upgrade: <code>--set webhook.enabled=true</code></li> <li>Verify the webhook is running after upgrade</li> </ol> <p>If you only use v2 CRs (or are starting fresh), the webhook is optional.</p>","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#step-1-update-helm-chart","title":"Step 1: Update Helm Chart","text":"","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#upgrade-command","title":"Upgrade Command","text":"<pre><code># Update Helm repository\nhelm repo update locust-k8s-operator\n\n# Upgrade to v2 (with webhook for v1 CR compatibility)\nhelm upgrade locust-operator locust-k8s-operator/locust-k8s-operator \\\n  --namespace locust-system \\\n  --version 2.0.0 \\\n  --set webhook.enabled=true\n\n# If you don't need v1 API compatibility, you can omit --set webhook.enabled=true\n</code></pre> <p>CRD Upgrade</p> <p>Helm automatically upgrades the CRD when using <code>helm upgrade</code>. The v2 CRD includes conversion webhook configuration when webhooks are enabled, allowing the API server to convert between v1 and v2 formats transparently.</p>","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#new-helm-values","title":"New Helm Values","text":"<p>The v2 chart introduces a cleaner structure. Key changes:</p> Old Path (v1) New Path (v2) Notes <code>config.loadGenerationPods.resource.cpuRequest</code> <code>locustPods.resources.requests.cpu</code> Backward compatible <code>config.loadGenerationPods.resource.memLimit</code> <code>locustPods.resources.limits.memory</code> Backward compatible <code>config.loadGenerationPods.affinity.enableCrInjection</code> <code>locustPods.affinityInjection</code> Backward compatible <code>micronaut.*</code> N/A Removed (Java-specific) <code>appPort</code> N/A Fixed at 8081 N/A <code>webhook.enabled</code> New: Enable conversion webhook N/A <code>leaderElection.enabled</code> New: Enable leader election","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#operator-resource-defaults","title":"Operator Resource Defaults","text":"<p>The Go operator controller requires significantly fewer resources than the Java version:</p> <pre><code>resources:\n  limits:\n    memory: 256Mi\n    cpu: 500m\n  requests:\n    memory: 64Mi\n    cpu: 10m\n</code></pre>","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#step-2-verify-existing-crs","title":"Step 2: Verify Existing CRs","text":"<p>The conversion webhook automatically converts v1 CRs to v2 format when stored. Verify your existing CRs work:</p> <pre><code># List all LocustTests\nkubectl get locusttests -A\n\n# Check a specific CR\nkubectl describe locusttest &lt;name&gt;\n</code></pre>","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#verify-conversion","title":"Verify Conversion","text":"<p>You can read a v1 CR as v2 to verify conversion:</p> <pre><code># Read as v2 (even if created as v1)\nkubectl get locusttest &lt;name&gt; -o yaml | grep \"apiVersion:\"\n# Should show: apiVersion: locust.io/v2\n</code></pre> <p>Deprecation Warning</p> <p>When using the v1 API, you'll see a deprecation warning in kubectl output. This is expected and indicates the conversion webhook is working.</p>","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#step-3-migrate-crs-to-v2-format-recommended","title":"Step 3: Migrate CRs to v2 Format (Recommended)","text":"<p>While v1 CRs continue to work, migrating to v2 format is recommended to access new features.</p>","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#field-mapping-reference","title":"Field Mapping Reference","text":"v1 Field v2 Field Notes <code>masterCommandSeed</code> <code>master.command</code> Direct mapping <code>workerCommandSeed</code> <code>worker.command</code> Direct mapping <code>workerReplicas</code> <code>worker.replicas</code> Direct mapping <code>image</code> <code>image</code> No change <code>imagePullPolicy</code> <code>imagePullPolicy</code> No change <code>imagePullSecrets</code> <code>imagePullSecrets</code> No change <code>configMap</code> <code>testFiles.configMapRef</code> Grouped under testFiles <code>libConfigMap</code> <code>testFiles.libConfigMapRef</code> Grouped under testFiles <code>labels.master</code> <code>master.labels</code> Grouped under master <code>labels.worker</code> <code>worker.labels</code> Grouped under worker <code>annotations.master</code> <code>master.annotations</code> Grouped under master <code>annotations.worker</code> <code>worker.annotations</code> Grouped under worker <code>affinity.nodeAffinity</code> <code>scheduling.affinity</code> Uses native K8s Affinity \u26a0\ufe0f<sup>1</sup> <code>tolerations</code> <code>scheduling.tolerations</code> Uses native K8s Tolerations N/A <code>master.resources</code> New: Separate resource specs for master N/A <code>worker.resources</code> New: Separate resource specs for worker N/A <code>master.extraArgs</code> New: Additional CLI arguments for master N/A <code>worker.extraArgs</code> New: Additional CLI arguments for worker N/A <code>master.autostart</code> Auto-added during conversion (default: true) N/A <code>master.autoquit</code> Auto-added during conversion (enabled: true, timeout: 60s)","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#example-transformation","title":"Example Transformation","text":"v1 Format (Deprecated)v2 Format <pre><code>apiVersion: locust.io/v1\nkind: LocustTest\nmetadata:\n  name: example-test\nspec:\n  image: locustio/locust:2.20.0\n  masterCommandSeed: --locustfile /lotest/src/test.py --host https://example.com\n  workerCommandSeed: --locustfile /lotest/src/test.py\n  workerReplicas: 5\n  configMap: test-scripts\n  labels:\n    master:\n      team: platform\n    worker:\n      team: platform\n</code></pre> <pre><code>apiVersion: locust.io/v2\nkind: LocustTest\nmetadata:\n  name: example-test\nspec:\n  image: locustio/locust:2.20.0\n  master:\n    command: \"--locustfile /lotest/src/test.py --host https://example.com\"\n    labels:\n      team: platform\n  worker:\n    command: \"--locustfile /lotest/src/test.py\"\n    replicas: 5\n    labels:\n      team: platform\n  testFiles:\n    configMapRef: test-scripts\n</code></pre>","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#lossy-conversion-details","title":"Lossy Conversion Details","text":"<p>V2-Only Fields Not Preserved in V1</p> <p>When reading v2 CRs as v1 (or during rollback to v1), the following v2-exclusive fields will be lost:</p> <p>Master/Worker Configuration:</p> <ul> <li><code>master.resources</code> - Separate resource specs for master pod</li> <li><code>worker.resources</code> - Separate resource specs for worker pod</li> <li><code>master.extraArgs</code> - Additional CLI arguments for master</li> <li><code>worker.extraArgs</code> - Additional CLI arguments for worker</li> <li><code>master.autostart</code> - Autostart configuration</li> <li><code>master.autoquit</code> - Autoquit configuration</li> </ul> <p>Test Files:</p> <ul> <li><code>testFiles.srcMountPath</code> - Custom mount path for test files</li> <li><code>testFiles.libMountPath</code> - Custom mount path for library files</li> </ul> <p>Scheduling:</p> <ul> <li><code>scheduling.nodeSelector</code> - Node selector (v1 only supports nodeAffinity)</li> <li>Complex affinity rules (see warning above)</li> </ul> <p>Environment &amp; Secrets:</p> <ul> <li><code>env.configMapRefs</code> - ConfigMap environment injection</li> <li><code>env.secretRefs</code> - Secret environment injection</li> <li><code>env.variables</code> - Individual environment variables</li> <li><code>env.secretMounts</code> - Secret file mounts</li> </ul> <p>Volumes:</p> <ul> <li><code>volumes</code> - Volume definitions</li> <li><code>volumeMounts</code> - Volume mounts with target selection</li> </ul> <p>Observability:</p> <ul> <li><code>observability.openTelemetry</code> - OpenTelemetry configuration</li> </ul> <p>Status:</p> <ul> <li>All <code>status</code> subresource fields (v1 has no status implementation)</li> </ul> <p>Recommendation: Before rolling back from v2 to v1, backup your v2 CRs to preserve this configuration.</p>","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#step-4-leverage-new-features","title":"Step 4: Leverage New Features","text":"<p>After migrating to v2, you can use new features:</p>","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#opentelemetry-support","title":"OpenTelemetry Support","text":"<pre><code>spec:\n  observability:\n    openTelemetry:\n      enabled: true\n      endpoint: \"otel-collector.monitoring:4317\"\n      protocol: \"grpc\"\n</code></pre> <p> Learn more about OpenTelemetry</p>","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#secret-configmap-injection","title":"Secret &amp; ConfigMap Injection","text":"<pre><code>spec:\n  env:\n    secretRefs:\n      - name: api-credentials\n        prefix: \"API_\"\n    configMapRefs:\n      - name: app-config\n    variables:\n      - name: TARGET_HOST\n        value: \"https://api.example.com\"\n</code></pre> <p> Learn more about Environment Injection</p>","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#volume-mounting","title":"Volume Mounting","text":"<pre><code>spec:\n  volumes:\n    - name: test-data\n      persistentVolumeClaim:\n        claimName: test-data-pvc\n  volumeMounts:\n    - name: test-data\n      mountPath: /data\n      target: both  # master, worker, or both\n</code></pre> <p> Learn more about Volume Mounting</p>","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#separate-resource-specs","title":"Separate Resource Specs","text":"<pre><code>spec:\n  master:\n    resources:\n      requests:\n        memory: \"256Mi\"\n        cpu: \"100m\"\n  worker:\n    resources:\n      requests:\n        memory: \"512Mi\"\n        cpu: \"500m\"\n</code></pre> <p> Learn more about Separate Resources</p>","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#troubleshooting","title":"Troubleshooting","text":"","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#common-issues","title":"Common Issues","text":"","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#conversion-webhook-not-working","title":"Conversion Webhook Not Working","text":"<p>Symptom: v1 CRs fail with schema validation errors</p> <p>Solution: Ensure cert-manager is installed and the webhook is enabled:</p> <pre><code># Check cert-manager\nkubectl get pods -n cert-manager\n\n# Enable webhook in Helm\nhelm upgrade locust-operator locust-k8s-operator/locust-k8s-operator \\\n  --set webhook.enabled=true\n</code></pre>","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#resources-not-created","title":"Resources Not Created","text":"<p>Symptom: LocustTest CR created but no Jobs/Services appear</p> <p>Solution: Check operator logs:</p> <pre><code>kubectl logs -n locust-system -l app.kubernetes.io/name=locust-k8s-operator\n</code></pre>","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#status-not-updating","title":"Status Not Updating","text":"<p>Symptom: LocustTest status remains empty</p> <p>Solution: Verify RBAC permissions include <code>locusttests/status</code>:</p> <pre><code>kubectl auth can-i update locusttests/status --as=system:serviceaccount:locust-system:locust-operator\n</code></pre>","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#how-to-get-help","title":"How to Get Help","text":"<ul> <li>GitHub Issues</li> </ul>","tags":["migration","upgrade","v2","guide"]},{"location":"migration/#rollback-procedure","title":"Rollback Procedure","text":"<p>If you need to revert to v1:</p> <pre><code># Rollback Helm release\nhelm rollback locust-operator &lt;previous-revision&gt; -n locust-system\n\n# Or reinstall v1\nhelm install locust-operator locust-k8s-operator/locust-k8s-operator \\\n  --version 1.1.1 \\\n  -f values-backup.yaml\n</code></pre> <p>Note</p> <p>After rollback, v2-specific fields in CRs will be lost. Ensure you have backups of any v2-only configurations.</p> <ol> <li> <p>Affinity Conversion Note: When converting v2 \u2192 v1, complex affinity rules may be simplified. Only <code>NodeSelectorOpIn</code> operators are preserved, and only the first value from multi-value expressions is kept. Pod affinity/anti-affinity and preferred scheduling rules are not preserved in v1.\u00a0\u21a9</p> </li> </ol>","tags":["migration","upgrade","v2","guide"]},{"location":"pull-request-process/","title":"Pull Request Process","text":"<p>This document outlines the process for submitting pull requests to the Locust K8s Operator project. Following these guidelines helps maintain code quality and ensures a smooth review process.</p>"},{"location":"pull-request-process/#before-creating-a-pull-request","title":"Before Creating a Pull Request","text":"<ol> <li> <p>Discuss Changes First: Before making significant changes, please discuss the proposed changes via an issue in the GitHub repository.</p> </li> <li> <p>Follow Coding Conventions: Ensure your code follows the project's coding standards and conventions.</p> </li> <li> <p>Write Tests: All new features or bug fixes should be covered by appropriate tests. See the testing guide for details on the testing setup.</p> </li> </ol>"},{"location":"pull-request-process/#pull-request-workflow","title":"Pull Request Workflow","text":"<ol> <li> <p>Fork and Clone: Fork the repository and clone it locally.</p> </li> <li> <p>Create a Branch: Create a branch for your changes using a descriptive name:    <pre><code>git checkout -b feature/your-feature-name\n</code></pre></p> </li> <li> <p>Make Your Changes: Implement your changes, following Go coding standards.</p> </li> <li> <p>Commit Your Changes: Use the Conventional Commits standard for commit messages. This is important as the commit messages directly influence the content of the CHANGELOG.md and version increments.</p> </li> </ol> <p>Examples of good commit messages:    <pre><code>feat: add support for OpenTelemetry metrics export\nfix: correct volume mount path validation\ndocs: update API reference for v2 fields\nrefactor: simplify resource builder functions\ntest: add integration tests for env injection\n</code></pre></p> <ol> <li> <p>Run Tests Locally: Run tests and linting to ensure your changes don't break existing functionality:    <pre><code>cd locust-k8s-operator\n\n# Run all CI checks (lint + tests)\nmake ci\n\n# Or run individually:\nmake lint        # Run linter\nmake test        # Run unit + integration tests\nmake test-e2e    # Run E2E tests (requires Docker)\n</code></pre></p> </li> <li> <p>Generate Manifests: If you modified API types, regenerate manifests:    <pre><code>make generate    # Generate DeepCopy methods\nmake manifests   # Generate CRDs, RBAC, webhooks\n</code></pre></p> </li> <li> <p>Submit Your Pull Request: Push your branch to your fork and submit a pull request to the main repository.</p> </li> </ol>"},{"location":"pull-request-process/#pull-request-requirements","title":"Pull Request Requirements","text":""},{"location":"pull-request-process/#code-quality","title":"Code Quality","text":"<ul> <li> Code follows Go conventions and project style</li> <li> No linting errors (<code>make lint</code> passes)</li> <li> All tests pass (<code>make test</code> passes)</li> <li> New code has appropriate test coverage (\u226580% for new packages)</li> </ul>"},{"location":"pull-request-process/#documentation","title":"Documentation","text":"<ul> <li> API changes are reflected in <code>docs/api_reference.md</code></li> <li> New features are documented in <code>docs/features.md</code> or <code>docs/advanced_topics.md</code></li> <li> Breaking changes are noted in the PR description</li> <li> Helm chart updates include <code>docs/helm_deploy.md</code> changes</li> </ul>"},{"location":"pull-request-process/#commit-messages","title":"Commit Messages","text":"<ul> <li> Follow Conventional Commits standard</li> <li> Each commit represents a logical unit of change</li> <li> Commit messages are clear and descriptive</li> </ul>"},{"location":"pull-request-process/#tests","title":"Tests","text":"<ul> <li> Unit tests for new/modified functions</li> <li> Integration tests for controller behavior changes</li> <li> Existing tests updated if behavior changes</li> <li> No test regressions</li> </ul>"},{"location":"pull-request-process/#ci-pipeline-checks","title":"CI Pipeline Checks","text":"<p>The following checks run automatically on each PR:</p> Check Description Command Lint golangci-lint static analysis <code>make lint</code> Test Unit + integration tests <code>make test</code> Build Binary compilation <code>make build</code> Manifests CRD/RBAC generation <code>make manifests</code> <p>All checks must pass before a PR can be merged.</p>"},{"location":"pull-request-process/#review-process","title":"Review Process","text":"<ol> <li> <p>Initial Review: Maintainers will review your PR to ensure it meets the project's requirements.</p> </li> <li> <p>CI Checks: The CI system will run tests and other checks against your PR. Make sure these pass.</p> </li> <li> <p>Feedback: Maintainers may request changes or improvements to your PR.</p> </li> <li> <p>Merge: Once approved and CI passes, a maintainer will merge your PR.</p> </li> </ol>"},{"location":"pull-request-process/#after-your-pr-is-merged","title":"After Your PR is Merged","text":"<ol> <li> <p>Update Your Fork: Keep your fork up to date with the main repository:    <pre><code>git checkout master\ngit pull upstream master\ngit push origin master\n</code></pre></p> </li> <li> <p>Celebrate: Thank you for contributing to the Locust K8s Operator project! Your efforts help make the project better for everyone.</p> </li> </ol> <p>Remember that this is a collaborative open-source project. Constructive feedback and discussions are welcome, and all interactions should adhere to the project's Code of Conduct.</p>"}]}