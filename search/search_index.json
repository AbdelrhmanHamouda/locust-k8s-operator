{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Locust Kubernetes Operator","text":"<p>Enable performance testing for the modern era!</p> <p>Utilize the full power of Locust in the cloud.</p> <p> </p>"},{"location":"#at-a-glance","title":"At a glance","text":"<p>The Operator is designed to unlock seamless &amp; effortless distributed performance testing in the cloud and enable continues integration for CI / CD. By design, the entire system is cloud native and focuses on automation and CI practices. One strong feature about the system is its ability to horizontally scale to meet any required performance demands.</p>"},{"location":"#what-does-it-offer","title":"What does it offer","text":"<ul> <li> <p> Cloud Native</p> <p>Leverage the full power of Kubernetes and cloud-native technologies for distributed performance testing.</p> </li> <li> <p> Automation &amp; CI</p> <p>Integrate performance testing directly into your CI/CD pipelines for continuous validation.</p> </li> <li> <p> Governance</p> <p>Maintain control over how resources are deployed and used in the cloud.</p> </li> <li> <p> Observability</p> <p>Gain insights into test results and infrastructure usage with built-in observability features.</p> </li> </ul> <p>Check out the full list of features!</p> <p></p>"},{"location":"#whom-is-it-for","title":"Whom is it for","text":"<p>It is built for...</p> <p></p>"},{"location":"#where-can-it-run","title":"Where can it run","text":"<p>Due to its design, the Operator can be deployed on any kubernetes cluster. Meaning that it is possible to have a full cloud native performance testing system anywhere in a matter of seconds.</p>"},{"location":"#limits","title":"Limits","text":"<p>The only real limit to this approach is the amount of cluster resources a given team or an organization is willing to dedicate to performance testing.</p>"},{"location":"advanced_topics/","title":"Advanced topics","text":"<p>Basic configuration is not always enough to satisfy the performance test needs, for example when needing to work with Kafka and MSK. Below is a collection of topics of an advanced nature. This list will be keep growing as the tool matures more and more.</p>"},{"location":"advanced_topics/#kafka-aws-msk-configuration","title":"Kafka &amp; AWS MSK configuration","text":"<p>Generally speaking, the usage of Kafka in a locustfile is identical to how it would be used anywhere else within the cloud context. Thus, no special setup is needed for the purposes of performance testing with the Operator. That being said, if an organization is using kafka in production, chances are that authenticated kafka is being used. One of the main providers of such managed service is AWS in the form of MSK. For that end, the Operator have an out-of-the-box support for MSK.</p> <p>To enable performance testing with MSK, a central/global Kafka user can be created by the \"cloud admin\" or \"the team\" responsible for the Operator deployment within the organization. The Operator can then be easily configured to inject the configuration of that user as environment variables in all generated resources. Those variables can be used by the test to establish authentication with the kafka broker.</p> Variable Name Description <code>KAFKA_BOOTSTRAP_SERVERS</code> Kafka bootstrap servers <code>KAFKA_SECURITY_ENABLED</code> - <code>KAFKA_SECURITY_PROTOCOL_CONFIG</code> Security protocol. Options: <code>PLAINTEXT</code>, <code>SASL_PLAINTEXT</code>, <code>SASL_SSL</code>, <code>SSL</code> <code>KAFKA_SASL_MECHANISM</code> Authentication mechanism. Options: <code>PLAINTEXT</code>, <code>SCRAM-SHA-256</code>, <code>SCRAM-SHA-512</code> <code>KAFKA_USERNAME</code> The username used to authenticate Kafka clients with the Kafka server <code>KAFKA_PASSWORD</code> The password used to authenticate Kafka clients with the Kafka server"},{"location":"advanced_topics/#dedicated-kubernetes-nodes","title":"Dedicated Kubernetes Nodes","text":"<p>To run test resources on dedicated Kubernetes node(s), the Operator support deploying resources with Affinity and Taint Tolerations.</p>"},{"location":"advanced_topics/#affinity","title":"Affinity","text":"<p>This allows generated resources to have specific Affinity options.</p> <p>Note</p> <p>The Custom Resource Definition Spec is designed with modularity and expandability in mind. This means that although a specific set of Kubernetes Affinity options are supported today, extending this support based on need is a streamlined and easy processes. If additonal support is needed, don't hesitate to open a feature request.</p>"},{"location":"advanced_topics/#affinity-options","title":"Affinity Options","text":"<p>The specification for affinity is defined as follows</p> <code>affinity-spec.yaml</code> <pre><code>apiVersion: locust.io/v1\n...\nspec:\n...\naffinity:\n    nodeAffinity:\n        requiredDuringSchedulingIgnoredDuringExecution\n            &lt;label-key&gt;: &lt;label-value&gt;\n            ...\n...\n</code></pre>"},{"location":"advanced_topics/#node-affinity","title":"Node Affinity","text":"<p>This optional section causes generated pods to declare specific Node Affinity so Kubernetes scheduler becomes aware of this requirement.</p> <p>The implementation from the Custom Resource perspective is strongly influenced by Kubernetes native definition of node affinity. However, the implementation is on purpose slightly simplified in order to allow users to have easier time working with affinity.</p> <p>The <code>nodeAffinity</code> section supports declaring node affinity under <code>requiredDuringSchedulingIgnoredDuringExecution</code>. Meaning that any declared affinity labels must be present on nodes in order for resources to be deployed on them.</p> <p>Example:</p> <p>In the below example, generated pods will declare 3 required labels (keys and values) to be present on nodes before they are scheduled.</p> <code>node-affinity-example.yaml</code> <pre><code>apiVersion: locust.io/v1\n...\nspec:\n    ...\n    affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeAffinityLabel1: locust-cloud-tests\n            nodeAffinityLabel2: performance-nodes\n            nodeAffinityLabel3: high-memory\n            ...\n    ...\n</code></pre>"},{"location":"advanced_topics/#taint-tolerations","title":"Taint Tolerations","text":"<p>This allows generated resources to have specific Taint Tolerations options.</p>"},{"location":"advanced_topics/#toleration-options","title":"Toleration Options","text":"<p>The specification for tolerations is defined as follows</p> <code>taint-tolerations-spec.yaml</code> Example <pre><code>apiVersion: locust.io/v1\n...\nspec:\n  ...\n  tolerations:\n    - key: &lt;string value&gt;\n      operator: &lt;\"Exists\", \"Equal\"&gt;\n      value: &lt;string value&gt; #(1)!\n      effect: &lt;\"NoSchedule\", \"PreferNoSchedule\", \"NoExecute\"&gt;\n    ...\n</code></pre> <ol> <li>Optional when <code>operator</code> value is set to <code>Exists</code>.</li> </ol> <pre><code>apiVersion: locust.io/v1\n...\nspec:\n  ...\n  tolerations:\n    - key: taint-A\n      operator: Equal\n      value: ssd\n      effect: NoSchedule\n    - key: taint-B\n      operator: Exists\n      effect: NoExecute\n</code></pre>"},{"location":"advanced_topics/#resource-management","title":"Resource Management","text":"<p>The operator allows for fine-grained control over the resource requests and limits for the Locust master and worker pods. This is useful for ensuring that your load tests have the resources they need, and for preventing them from consuming too many resources on your cluster.</p> <p>Configuration is done via the <code>application.yml</code> file or through Helm values. The following properties are available:</p> <ul> <li><code>locust.operator.resource.pod-mem-request</code></li> <li><code>locust.operator.resource.pod-cpu-request</code></li> <li><code>locust.operator.resource.pod-ephemeral-storage-request</code></li> <li><code>locust.operator.resource.pod-mem-limit</code></li> <li><code>locust.operator.resource.pod-cpu-limit</code></li> <li><code>locust.operator.resource.pod-ephemeral-storage-limit</code></li> </ul>"},{"location":"advanced_topics/#disabling-cpu-limits","title":"Disabling CPU Limits","text":"<p>In some scenarios, particularly during performance-sensitive tests, you may want to disable CPU limits to prevent throttling. This can be achieved by setting the <code>pod-cpu-limit</code> property to a blank string.</p> Example <pre><code>locust:\n  operator:\n    resource:\n      pod-cpu-limit: \"\" # (1)!\n</code></pre> <ol> <li>Setting the CPU limit to an empty string disables it.</li> </ol> <p>Note</p> <p>When the CPU limit is disabled, the pod is allowed to use as much CPU as is available on the node. This can be useful for maximizing performance, but it can also lead to resource contention if not managed carefully.</p>"},{"location":"advanced_topics/#usage-of-a-private-image-registry","title":"Usage of a private image registry","text":"<p>Images from a private image registry can be used through various methods as described in the kubernetes documentation, one of those methods depends on setting <code>imagePullSecrets</code> for pods. This is supported in the operator by simply setting the <code>imagePullSecrets</code> option in the deployed custom resource. For example:</p> locusttest-pull-secret-cr.yaml<pre><code>apiVersion: locust.io/v1\n...\nspec:\n  image: ghcr.io/mycompany/locust:latest # (1)!\n  imagePullSecrets: # (2)!\n    - gcr-secret\n  ...\n</code></pre> <ol> <li>Specify which Locust image to use for both master and worker containers.</li> <li>[Optional] Specify an existing pull secret to use for master and worker pods.</li> </ol>"},{"location":"advanced_topics/#image-pull-policy","title":"Image pull policy","text":"<p>Kubernetes uses the image tag and pull policy to control when kubelet attempts to download (pull) a container image. The image pull policy can be defined through the <code>imagePullPolicy</code> option, as explained in the kubernetes documentation. When using the operator, the <code>imagePullPolicy</code> option can be directly configured in the custom resource. For example:</p> locusttest-pull-policy-cr.yaml<pre><code>apiVersion: locust.io/v1\n...\nspec:\n  image: ghcr.io/mycompany/locust:latest # (1)!\n  imagePullPolicy: Always # (2)!\n  ...\n</code></pre> <ol> <li>Specify which Locust image to use for both master and worker containers.</li> <li>[Optional] Specify the pull policy to use for containers defined within master and worker containers. Supported options include <code>Always</code>, <code>IfNotPresent</code> and <code>Never</code>.</li> </ol>"},{"location":"advanced_topics/#automatic-cleanup-for-finished-master-and-worker-jobs","title":"Automatic Cleanup for Finished Master and Worker Jobs","text":"<p>Once load tests finish, master and worker jobs remain available in Kubernetes. You can set up a time-to-live (TTL) value in the operator's Helm chart, so that kubernetes jobs are eligible for cascading removal once the TTL expires. This means that Master and Worker jobs and their dependent objects (e.g., pods) will be deleted.</p> <p>Note that setting up a TTL will not delete <code>LocustTest</code> or <code>ConfigMap</code> resources.</p> <p>To set a TTL value, override the key <code>ttlSecondsAfterFinished</code> in <code>values.yaml</code>:</p> <code>values.yaml</code> <pre><code>...\nconfig:\n  loadGenerationJobs:\n    # Either leave empty or use an empty string to avoid setting this option\n    ttlSecondsAfterFinished: 3600 # (1)!\n...\n</code></pre> <ol> <li>Time in seconds to keep the job after it finishes.</li> </ol> <p>You can also use Helm's CLI arguments: <code>helm install ... --set config.loadGenerationJobs.ttlSecondsAfterFinished=0</code>.</p> <p>Read more about the <code>ttlSecondsAfterFinished</code> parameter in Kubernetes's official documentation.</p>"},{"location":"advanced_topics/#kubernetes-support-for-ttlsecondsafterfinished","title":"Kubernetes Support for <code>ttlSecondsAfterFinished</code>","text":"<p>Support for parameter <code>ttlSecondsAfterFinished</code> was added in Kubernetes v1.12. In case you're deploying the locust operator to a Kubernetes cluster that does not support <code>ttlSecondsAfterFinished</code>, you may leave the Helm key empty or use an empty string. In this case, job definitions will not include the parameter.</p>"},{"location":"contribute/","title":"Contribute","text":"<p>There's plenty to do, come say hi in the issues! \ud83d\udc4b</p> <p>Also check out the CONTRIBUTING.MD \ud83e\udd13</p>"},{"location":"contribute/#project-status","title":"project status","text":"<p>The project is actively maintained and is under continues development and improvement. If you have any request or want to chat, kindly open a ticket. If you wish to contribute code and / or ideas, kindly check the contribution section.</p>"},{"location":"features/","title":"Features","text":"<ul> <li> <p> Cloud Native &amp; Kubernetes Integration</p> <p>Leverage the full power of Kubernetes for distributed performance testing. The operator is designed to be cloud-native, enabling seamless deployment and scaling on any Kubernetes cluster.</p> </li> <li> <p> Automation &amp; CI/CD</p> <p>Integrate performance testing directly into your CI/CD pipelines. Automate the deployment, execution, and teardown of your Locust tests for continuous performance validation.</p> </li> <li> <p> Governance &amp; Resource Management</p> <p>Maintain control over how resources are deployed and used. Configure resource requests and limits for Locust master and worker pods, and even disable CPU limits for performance-sensitive tests.</p> </li> <li> <p> Observability &amp; Monitoring</p> <p>Gain insights into test results and infrastructure usage. The operator supports Prometheus metrics out-of-the-box, allowing you to build rich monitoring dashboards.</p> </li> <li> <p> Cost Optimization</p> <p>Optimize cloud costs by deploying resources only when needed and for as long as needed. The operator's automatic cleanup feature ensures that resources are terminated after a test run.</p> </li> <li> <p> Test Isolation &amp; Parallelism</p> <p>Run multiple tests in parallel with guaranteed isolation. Each test runs in its own set of resources, preventing any cross-test interference.</p> </li> <li> <p> Private Image Registry Support</p> <p>Use images from private registries for your Locust tests. The operator supports <code>imagePullSecrets</code> and configurable <code>imagePullPolicy</code>.</p> </li> <li> <p> Advanced Scheduling</p> <p>Control where your Locust pods are scheduled using Kubernetes affinity and taint tolerations. This allows you to run tests on dedicated nodes or in specific availability zones.</p> </li> <li> <p> Kafka &amp; AWS MSK Integration</p> <p>Seamlessly integrate with Kafka and AWS MSK for performance testing of event-driven architectures. The operator provides out-of-the-box support for authenticated Kafka.</p> </li> </ul>"},{"location":"getting_started/","title":"Getting started","text":"<p>Only few simple steps are needed to get a test up and running in the cluster. The following is a step-by-step guide on how to achieve this.</p>"},{"location":"getting_started/#step-1-write-a-valid-locust-test-script","title":"Step 1: Write a valid Locust test script","text":"<p>For this example, we will be using the following script</p> demo_test.py<pre><code>from locust import HttpUser, task\n\nclass User(HttpUser): # (1)!\n    @task #(2)!\n    def get_employees(self) -&gt; None:\n        \"\"\"Get a list of employees.\"\"\"\n        self.client.get(\"/api/v1/employees\") #(3)!\n</code></pre> <ol> <li>Class representing <code>users</code> that will be simulated by Locust.</li> <li>One or more <code>task</code> that each simulated <code>user</code> will be performing.</li> <li>HTTP call to a specific endpoint.</li> </ol> <p>Note</p> <p>To be able to run performance tests effectivly, an understanding of Locust which is the underline load generation tool is required. All tests must be valid locust tests.</p> <p>Locust provide a very good and detail rich documentation that can be found here.</p>"},{"location":"getting_started/#step-2-write-a-valid-custom-resource-for-locusttest-crd","title":"Step 2: Write a valid custom resource for LocustTest CRD","text":"<p>A simple custom resource for the previous test can be something like the following example;</p> <p>To streamline this step, intensive-brew should be used. It is a simple cli tool that converts a declarative yaml into a compatible LocustTest kubernetes custom resource.</p> locusttest-cr.yaml<pre><code>apiVersion: locust.io/v1 # (1)!\nkind: LocustTest # (2)!\nmetadata:\n  name: demo.test # (3)!\nspec:\n  image: locustio/locust:latest # (4)!\n  masterCommandSeed: # (5)!\n    --locustfile /lotest/src/demo_test.py\n    --host https://dummy.restapiexample.com\n    --users 100\n    --spawn-rate 3\n    --run-time 3m\n  workerCommandSeed: --locustfile /lotest/src/demo_test.py # (6)!\n  workerReplicas: 3 # (7)!\n  configMap: demo-test-map # (8)!\n</code></pre> <ol> <li>API version based on the deployed LocustTest CRD.</li> <li>Resource kind.</li> <li>The name field used by the operator to infer the names of test generated resources. While this value is insignificant to the Operator     itself, it is important to keep a good convention here since it helps in tracking resources across the cluster when needed.</li> <li>Image to use for the load generation pods</li> <li>Seed command for the master node. The Operator will append to this seed command/s all operational parameters needed for the master     to perform its job e.g. ports, rebalancing settings, timeouts, etc...</li> <li>Seed command for the worker node. The Operator will append to this seed command/s all operational parameters needed for the worker     to perform its job e.g. ports, master node url, master node ports, etc...</li> <li>The amount of worker nodes to spawn in the cluster.</li> <li>[Optional] Name of configMap to mount into the pod</li> </ol>"},{"location":"getting_started/#other-options","title":"Other options","text":""},{"location":"getting_started/#labels-and-annotations","title":"Labels and annotations","text":"<p>You can add labels and annotations to generated Pods. For example:</p> locusttest-cr.yaml<pre><code>apiVersion: locust.io/v1\n...\nspec:\n  image: locustio/locust:latest\n  labels: # (1)!\n    master:\n      locust.io/role: \"master\"\n      myapp.com/testId: \"abc-123\"\n      myapp.com/tenantId: \"xyz-789\"\n    worker:\n      locust.io/role: \"worker\"\n  annotations: # (2)!\n    master:\n      myapp.com/threads: \"1000\"\n      myapp.com/version: \"2.1.0\"\n    worker:\n      myapp.com/version: \"2.1.0\"\n  ...\n</code></pre> <ol> <li>[Optional] Labels are attached to both master and worker pods. They can later be used to identify pods belonging to a particular execution context. This is useful, for example, when tests are deployed programmatically. A launcher application can query the Kubernetes API for specific resources.</li> <li>[Optional] Annotations too are attached to master and worker pods. They can be used to include additional context about a test. For example, configuration parameters of the software system being tested.</li> </ol> <p>Both labels and annotations can be added to the Prometheus configuration, so that metrics are associated with the appropriate information, such as the test and tenant ids. You can read more about this in the Prometheus documentation site.</p>"},{"location":"getting_started/#step-3-deploy-locust-k8s-operator-in-the-cluster","title":"Step 3: Deploy Locust k8s Operator in the cluster.","text":"<p>The recommended way to install the Operator is by using the official HELM chart. Documentation on how to perform that is available here.</p>"},{"location":"getting_started/#step-4-deploy-test-as-a-configmap","title":"Step 4: Deploy test as a configMap","text":"<p>For the purposes of this example, the <code>demo_test.py</code> test previously demonstrated will be deployed into the cluster as a configMap that the Operator will mount to the load generation pods. To deploy the test as a configMap, run the bellow command following this template <code>kubectl create configmap &lt;configMap-name&gt; --from-file &lt;your_test.py&gt;</code>:</p> <ul> <li><code>kubectl create configmap demo-test-map --from-file demo_test.py</code></li> </ul> <p>Fresh cluster resources</p> <p>Fresh cluster resources are allocated for each running test, meaning that tests DO NOT have any cross impact on each other.</p>"},{"location":"getting_started/#step-5-start-the-test-by-deploying-the-locusttest-custom-resource","title":"Step 5: Start the test by deploying the LocustTest custom resource.","text":"<p>Deploying a custom resource, signals to the Operator the desire to start a test and thus the Operator starts creating and scheduling all needed resources. To do that, deploy the custom resource following this template <code>kubectl apply -f &lt;valid_cr&gt;.yaml</code>:</p> <ul> <li><code>kubectl apply -f locusttest-cr.yaml</code></li> </ul>"},{"location":"getting_started/#step-51-check-cluster-for-running-resources","title":"Step 5.1: Check cluster for running resources","text":"<p>At this point, it is possible to check the cluster and all required resources will be running based on the passed configuration in the custom resource.</p> <p>The Operator will create the following resources in the cluster for each valid custom resource:</p> <ul> <li>A kubernetes service for the master node so it is reachable by other worker nodes.</li> <li>A kubernetes Job to manage the master node.</li> <li>A kubernetes Job to manage the worker node.</li> </ul>"},{"location":"getting_started/#step-6-clear-resources-after-test-run","title":"Step 6: Clear resources after test run","text":"<p>In order to remove the cluster resources after a test run, simply remove the custom resource and the Operator will react to this event by cleaning the cluster of all related resources. To delete a resource, run the below command following this template <code>kubectl delete -f &lt;valid_cr&gt;.yaml</code>:</p> <ul> <li><code>kubectl delete -f locusttest-cr.yaml</code></li> </ul>"},{"location":"helm_deploy/","title":"HELM Deployment Guide","text":"<p>This guide provides comprehensive instructions for deploying the Locust Kubernetes Operator using its official Helm chart.</p>"},{"location":"helm_deploy/#quick-start","title":"Quick Start","text":"<p>For experienced users, here are the essential commands to get the operator running in the <code>default</code> namespace:</p> <pre><code>helm repo add locust-k8s-operator https://abdelrhmanhamouda.github.io/locust-k8s-operator/\nhelm repo update\nhelm install locust-operator locust-k8s-operator/locust-k8s-operator\n</code></pre>"},{"location":"helm_deploy/#installation","title":"Installation","text":""},{"location":"helm_deploy/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster (e.g., Minikube, GKE, EKS, AKS).</li> <li>Helm 3 installed on your local machine.</li> </ul>"},{"location":"helm_deploy/#step-1-add-the-helm-repository","title":"Step 1: Add the Helm Repository","text":"<p>First, add the Locust Kubernetes Operator Helm repository to your local Helm client:</p> <pre><code>helm repo add locust-k8s-operator https://abdelrhmanhamouda.github.io/locust-k8s-operator/\n</code></pre> <p>Next, update your local chart repository cache to ensure you have the latest version:</p> <pre><code>helm repo update\n</code></pre>"},{"location":"helm_deploy/#step-2-install-the-chart","title":"Step 2: Install the Chart","text":"<p>You can install the chart with a release name of your choice (e.g., <code>locust-operator</code>).</p> <p>Default Installation:</p> <p>To install the chart with the default configuration into the currently active namespace, run:</p> <pre><code>helm install locust-operator locust-k8s-operator/locust-k8s-operator\n</code></pre> <p>Installation with a Custom Values File:</p> <p>For more advanced configurations, it's best to use a custom <code>values.yaml</code> file. Create a file named <code>my-values.yaml</code> and add your overrides:</p> <pre><code># my-values.yaml\nreplicaCount: 2\n\nconfig:\n  loadGenerationPods:\n    resource:\n      cpuLimit: \"2000m\"\n      memLimit: \"2048Mi\"\n</code></pre> <p>Then, install the chart, specifying your custom values file and a target namespace:</p> <pre><code>helm install locust-operator locust-k8s-operator/locust-k8s-operator \\\n  --namespace locust-system \\\n  --create-namespace \\\n  -f my-values.yaml\n</code></pre>"},{"location":"helm_deploy/#verifying-the-installation","title":"Verifying the Installation","text":"<p>After installation, you can verify that the operator is running correctly by checking the pods in the target namespace:</p> <pre><code>kubectl get pods -n locust-system\n</code></pre> <p>You should see a pod with a name similar to <code>locust-operator-b5c9f4f7-xxxxx</code> in the <code>Running</code> state.</p> <p>To view the operator's logs, run:</p> <pre><code>kubectl logs -f -n locust-system -l app.kubernetes.io/name=locust-k8s-operator\n</code></pre>"},{"location":"helm_deploy/#configuration","title":"Configuration","text":"<p>The following tables list the configurable parameters of the Locust Operator Helm chart and their default values.</p>"},{"location":"helm_deploy/#general-configuration","title":"General Configuration","text":"Parameter Description Default <code>appPort</code> The port that the operator will listen on. <code>8080</code>"},{"location":"helm_deploy/#deployment-settings","title":"Deployment Settings","text":"Parameter Description Default <code>replicaCount</code> Number of replicas for the operator deployment. <code>1</code> <code>image.repository</code> The repository of the Docker image. <code>lotest/locust-k8s-operator</code> <code>image.pullPolicy</code> The image pull policy. <code>IfNotPresent</code> <code>image.tag</code> Overrides the default image tag (defaults to the chart's <code>appVersion</code>). <code>\"\"</code>"},{"location":"helm_deploy/#kubernetes-resources","title":"Kubernetes Resources","text":"Parameter Description Default <code>k8s.customResourceDefinition.deploy</code> Specifies whether to deploy the <code>LocustTest</code> CRD. <code>true</code> <code>k8s.clusterRole.enabled</code> Deploy with a cluster-wide role (<code>true</code>) or a namespaced role (<code>false</code>). <code>true</code> <code>serviceAccount.create</code> Specifies whether a service account should be created. <code>true</code> <code>serviceAccount.name</code> The name of the service account to use. If not set, a name is generated. <code>\"\"</code> <code>resources</code> Resource requests and limits for the operator pod. <code>{}</code>"},{"location":"helm_deploy/#operator-configuration","title":"Operator Configuration","text":"Parameter Description Default <code>config.loadGenerationJobs.ttlSecondsAfterFinished</code> Time-to-live in seconds for finished load generation jobs. Set to <code>\"\"</code> to disable. <code>\"\"</code>"},{"location":"helm_deploy/#load-generation-pods","title":"Load Generation Pods","text":"Parameter Description Default <code>config.loadGenerationPods.resource.cpuRequest</code> CPU resource request for load generation pods. <code>250m</code> <code>config.loadGenerationPods.resource.memRequest</code> Memory resource request for load generation pods. <code>128Mi</code> <code>config.loadGenerationPods.resource.ephemeralRequest</code> Ephemeral storage request for load generation pods. <code>30M</code> <code>config.loadGenerationPods.resource.cpuLimit</code> CPU resource limit for load generation pods. Set to <code>\"\"</code> to unbind. <code>1000m</code> <code>config.loadGenerationPods.resource.memLimit</code> Memory resource limit for load generation pods. Set to <code>\"\"</code> to unbind. <code>1024Mi</code> <code>config.loadGenerationPods.resource.ephemeralLimit</code> Ephemeral storage limit for load generation pods. Set to <code>\"\"</code> to unbind. <code>50M</code> <code>config.loadGenerationPods.affinity.enableCrInjection</code> Enable Custom Resource injection for affinity settings. <code>true</code> <code>config.loadGenerationPods.taintTolerations.enableCrInjection</code> Enable Custom Resource injection for taint tolerations. <code>true</code>"},{"location":"helm_deploy/#metrics-exporter","title":"Metrics Exporter","text":"Parameter Description Default <code>config.loadGenerationPods.metricsExporter.image</code> Metrics Exporter Docker image. <code>containersol/locust_exporter:v0.5.0</code> <code>config.loadGenerationPods.metricsExporter.port</code> Metrics Exporter port. <code>9646</code> <code>config.loadGenerationPods.metricsExporter.pullPolicy</code> Image pull policy for the metrics exporter. <code>IfNotPresent</code>"},{"location":"helm_deploy/#pod-scheduling","title":"Pod Scheduling","text":"Parameter Description Default <code>nodeSelector</code> Node selector for scheduling the operator pod. <code>{}</code> <code>tolerations</code> Tolerations for scheduling the operator pod. <code>[]</code> <code>affinity</code> Affinity rules for scheduling the operator pod. <code>{}</code>"},{"location":"helm_deploy/#liveness-and-readiness-probes","title":"Liveness and Readiness Probes","text":"Parameter Description Default <code>livenessProbe.initialDelaySeconds</code> Initial delay for the liveness probe. <code>10</code> <code>livenessProbe.periodSeconds</code> How often to perform the liveness probe. <code>20</code> <code>livenessProbe.timeoutSeconds</code> When the liveness probe times out. <code>10</code> <code>livenessProbe.failureThreshold</code> When to give up on the liveness probe. <code>1</code> <code>readinessProbe.initialDelaySeconds</code> Initial delay for the readiness probe. <code>30</code> <code>readinessProbe.periodSeconds</code> How often to perform the readiness probe. <code>20</code> <code>readinessProbe.timeoutSeconds</code> When the readiness probe times out. <code>10</code> <code>readinessProbe.failureThreshold</code> When to give up on the readiness probe. <code>1</code>"},{"location":"helm_deploy/#advanced-configuration","title":"Advanced Configuration","text":"<p>The following sections cover advanced configuration options. For a complete list of parameters, refer to the <code>values.yaml</code> file in the chart.</p>"},{"location":"helm_deploy/#kafka-configuration","title":"Kafka Configuration","text":"Parameter Description Default <code>config.loadGenerationPods.kafka.bootstrapServers</code> Kafka bootstrap servers. <code>localhost:9092</code> <code>config.loadGenerationPods.kafka.acl.enabled</code> Enable ACL settings for Kafka. <code>false</code> <code>config.loadGenerationPods.kafka.sasl.mechanism</code> SASL mechanism for authentication. <code>SCRAM-SHA-512</code>"},{"location":"helm_deploy/#micronaut-metrics","title":"Micronaut Metrics","text":"Parameter Description Default <code>micronaut.metrics.enabled</code> Enable/disable all Micronaut metrics. <code>true</code> <code>micronaut.metrics.export.prometheus.step</code> The step size (duration) for Prometheus metrics export. <code>PT30S</code>"},{"location":"helm_deploy/#upgrading-the-chart","title":"Upgrading the Chart","text":"<p>To upgrade an existing release to a new version, use the <code>helm upgrade</code> command:</p> <pre><code>helm upgrade locust-operator locust-k8s-operator/locust-k8s-operator -f my-values.yaml\n</code></pre>"},{"location":"helm_deploy/#uninstalling-the-chart","title":"Uninstalling the Chart","text":"<p>To uninstall and delete the <code>locust-operator</code> deployment, run:</p> <pre><code>helm uninstall locust-operator\n</code></pre> <p>This command will remove all the Kubernetes components associated with the chart and delete the release.</p>"},{"location":"helm_deploy/#next-steps","title":"Next Steps","text":"<p>Once the operator is installed, you're ready to start running performance tests! Head over to the Getting Started guide to learn how to deploy your first <code>LocustTest</code>.</p>"},{"location":"how_does_it_work/","title":"How does it work","text":"<p>To run a performance test, basic configuration is provided through a simple and intuitive kubernetes custom resource. Once deployed the Operator does all the heavy work of creating and scheduling the resources while making sure that all created load generation pods can effectively communicate with each other.</p> <p>To handle the challenge of delivering test script/s from local environment to the cluster and in turn to the deployed locust pods, the Operator support dynamic volume mounting from a configMaps source. This is indicated by a simple optional configuration. Meaning, if the configuration is present, the volume is mounted, and if it is not, no volume is mounted.</p> <p>Since a \"Picture Is Worth a Thousand Words\", here is a gif! </p>"},{"location":"how_does_it_work/#steps-performed-in-demo","title":"Steps performed in demo","text":"<ul> <li>Test configmap created in cluster.</li> <li>LocustTest CR deployed into the cluster.</li> <li>The Operator creating, configuring and scheduling test resources on CR creation event.</li> <li>The Operator cleaning up test resources after test CR has been removed event.</li> </ul>"},{"location":"license/","title":"License","text":"<p>Open source licensed under Apache-2.0 license (see LICENSE file for details).</p>"},{"location":"metrics_and_dashboards/","title":"Metrics &amp; Dashboards","text":"<p>The Locust Kubernetes Operator is designed with observability in mind, providing out-of-the-box support for Prometheus metrics. This allows you to gain deep insights into your performance tests and the operator's behavior.</p>"},{"location":"metrics_and_dashboards/#prometheus-metrics-exporter","title":"Prometheus Metrics Exporter","text":"<p>By default, the operator deploys a Prometheus metrics exporter alongside each Locust master and worker pod. This exporter collects detailed metrics from the Locust instances and exposes them in a format that Prometheus can scrape.</p>"},{"location":"metrics_and_dashboards/#key-metrics","title":"Key Metrics","text":"<p>Some of the key metrics you can monitor include:</p> <ul> <li><code>locust_requests_total</code>: The total number of requests made.</li> <li><code>locust_requests_failed_total</code>: The total number of failed requests.</li> <li><code>locust_response_time_seconds</code>: The response time of requests.</li> <li><code>locust_users</code>: The number of simulated users.</li> </ul>"},{"location":"metrics_and_dashboards/#configuration","title":"Configuration","text":"<p>To enable Prometheus to scrape these metrics, you'll need to configure a scrape job in your <code>prometheus.yml</code> file. Here's an example configuration:</p> <pre><code>scrape_configs:\n  - job_name: 'locust'\n    kubernetes_sd_configs:\n      - role: pod\n    relabel_configs:\n      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]\n        action: keep\n        regex: true\n      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]\n        action: replace\n        target_label: __metrics_path__\n        regex: (.+)\n      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]\n        action: replace\n        regex: ([^:]+)(?::\\d+)?;(\\d+)\n        replacement: $1:$2\n        target_label: __address__\n      - action: labelmap\n        regex: __meta_kubernetes_pod_label_(.+)\n</code></pre>"},{"location":"metrics_and_dashboards/#grafana-dashboards","title":"Grafana Dashboards","text":"<p>Once you have your metrics flowing into Prometheus, you can create powerful and informative dashboards in Grafana to visualize your test results. You can build panels to track key performance indicators (KPIs) such as response times, request rates, and error rates.</p> <p>There are also community-built Grafana dashboards available for Locust that you can adapt for your needs.</p>"},{"location":"metrics_and_dashboards/#operator-metrics","title":"Operator Metrics","text":"<p>In addition to the Locust-specific metrics, the operator itself exposes a set of metrics through Micronaut's metrics module. These metrics provide insights into the operator's health and performance, including JVM metrics, uptime, and more. You can find these metrics by scraping the operator's pod on the <code>/health</code> endpoint.</p>"},{"location":"roadmap/","title":"Roadmap","text":"<p>The following is a list of planned features and improvements for the Locust Kubernetes Operator. This list is not exhaustive and may change over time.</p> <ul> <li> <p>Enhanced Observability: Provide out-of-the-box Grafana dashboard examples and more detailed Prometheus configuration guides to make monitoring even easier.</p> </li> <li> <p>Event-Driven Actions: Integrate with notification systems like Microsoft Teams or Slack to send alerts on test completion, failure, or other significant events.</p> </li> <li> <p>Advanced Benchmarking: Investigate the feasibility of incorporating external metrics into test results. This would allow for more sophisticated pass/fail criteria, such as assessing the performance of a Kafka-based service by its consumer lag.</p> </li> <li> <p>Dynamic Updates: Add support for updating a <code>LocustTest</code> custom resource while a test is running. This would allow for dynamically adjusting test parameters without restarting the test.</p> </li> <li> <p>Web UI/Dashboard: Explore the possibility of creating a simple web UI or dashboard for managing and monitoring tests directly through the operator.</p> </li> </ul>"}]}