{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Locust Kubernetes Operator \u2693 Enable performance testing for the modern era! Utilize the full power of Locust in the cloud. At a glance \u2693 The Operator is designed to unlock seamless & effortless distributed performance testing in the cloud and enable continues integration for CI / CD . By design, the entire system is cloud native and focuses on automation and CI practices. One strong feature about the system is its ability to horizontally scale to meet any required performance demands. What does it offer \u2693 Fundamentally, the Operator provide the following as part of its core offerings; cloud native , automation & CI , governance , Observability . Distributed cloud performance testing : Locust is a great and very powerful load testing tool. It is capable of generating a significant amount of load specially when configured correctly. That being said, there is only so much a single instance and vertical scaling can do. Luckily, Locust has a native out of the box support for distributed mode. This Locust Kubernetes Operator project leverage this feature and adds systems and functionalities to address challenges and situations that are exclusive to the cloud context. Low barrier of entry : Utilizing the power of the Operator lowers significantly the barrier of entry to run in the cloud. From an end-user perspective, running a performance test in the cloud becomes a single command operation. Test isolation and Parallel tests : By default, the Operator is able to support any number of Parallel test executions with an absolute guarantee that each test is fully protected from being polluted by the existence of any number of other tests. Automation & CI : By having automation as a core focus point, teams and organizations can build performance testing directly into CI/CD pipelines. Meaning that every new service, feature or system can be potentially tested and validated for performance SLOs / SLAs. Separation of concerns : By using the Operator , engineering teams can focus on building a robust performance test/s and SREs DevOps teams can focus on managing the resources. Governance : Enable organizations to have governance over what / how resources are deployed and run on the cloud. Cloud cost optimization : Using the Operator enables for a more effective control over the cloud cost . Since resources are only deployed when needed and only for as long as needed, the cost of performance testing is kept to a minimum. Observability : For both engineering teams and cloud admins, the Operator unlocks the ability to build observability & monitoring dashboards in order to analyse test results during test runtime or retroactively (interesting for teams) and infrastructure usage and resource monitoring ( interesting for cloud admins, SREs, etc...). Whom is it for \u2693 It is built for... Where can it run \u2693 Due to its design, the Operator can be deployed on any kubernetes cluster. Meaning that it is possible to have a full cloud native performance testing system anywhere in a matter of seconds. Limits \u2693 The only real limit to this approach is the amount of cluster resources a given team or an organization is willing to dedicate to performance testing. How does it work \u2693 To run a performance test, basic configuration is provided through a simple and intuitive kubernetes custom resource. Once deployed the Operator does all the heavy work of creating and scheduling the resources while making sure that all created load generation pods can effectively communicate with each other. To handle the challenge of delivering test script/s from local environment to the cluster and in turn to the deployed locust pods, the Operator support dynamic volume mounting from a configMaps source. This is indicated by a simple optional configuration. Meaning, if the configuration is present, the volume is mounted, and if it is not, no volume is mounted. Since a \" Picture Is Worth a Thousand Words \", here is a gif! Steps performed in demo \u2693 Test configmap created in cluster. LocustTest CR deployed into the cluster. The Operator creating, configuring and scheduling test resources on CR creation event. The Operator cleaning up test resources after test CR has been removed event. Getting started \u2693 Only 4 simple steps are needed to get a test up and running in the cluster: Valid Locust test script. Valid custom resource for LocustTest CRD: ( example ). To streamline this step, intensive-brew should be used. It is a simple cli tool that converts a declarative yaml into a compatible LocustTest kubernetes custom resource. ( Coming soon! ) Deploy test as a configMap kubectl create configmap <configMap-name> --from-file <your_test.py> Start the test by deploying the LocustTest custom resource. kubectl apply -f <valid_cr>.yaml Remove all performance test resources by removing the LocustTest custom resource. kubectl delete -f <valid_cr>.yaml Roadmap \u2693 Not in a particular order: Support HELM In depth \"getting started\" documentation Add traceability labels to generated resources Support for deploying test resources with node affinity / node taints Dashboard examples (Grafana + prometheus configuration) Enable event driven actions Integration with MSTeams: Push notification on test run completion / termination events UNDER_INVESTIGATION Benchmarking and collection of non-test generated metrics Investigation is on going to study the feasibility of supplying Locust pods with external metrics that are collected from service / system under-test. Test pods can then use this information to assess pass / fail criteria. This is especially useful in non-REST based services e.g. assess kafka(streams) microservice based on its consumer lag performance coming from the kafka broker. project status \u2693 The project is actively maintained and is under continues development and improvement. If you have any request or want to chat, kindly open a ticket. If you wish to contribute code and / or ideas, kindly check the contribution section. Contribute \u2693 There's plenty to do, come say hi in the issues ! \ud83d\udc4b Also check out the CONTRIBUTING.MD \ud83e\udd13 License \u2693 Open source licensed under Apache-2.0 license (see LICENSE file for details).","title":"Home"},{"location":"#locust-kubernetes-operator","text":"Enable performance testing for the modern era! Utilize the full power of Locust in the cloud.","title":"Locust Kubernetes Operator"},{"location":"#at-a-glance","text":"The Operator is designed to unlock seamless & effortless distributed performance testing in the cloud and enable continues integration for CI / CD . By design, the entire system is cloud native and focuses on automation and CI practices. One strong feature about the system is its ability to horizontally scale to meet any required performance demands.","title":"At a glance"},{"location":"#what-does-it-offer","text":"Fundamentally, the Operator provide the following as part of its core offerings; cloud native , automation & CI , governance , Observability . Distributed cloud performance testing : Locust is a great and very powerful load testing tool. It is capable of generating a significant amount of load specially when configured correctly. That being said, there is only so much a single instance and vertical scaling can do. Luckily, Locust has a native out of the box support for distributed mode. This Locust Kubernetes Operator project leverage this feature and adds systems and functionalities to address challenges and situations that are exclusive to the cloud context. Low barrier of entry : Utilizing the power of the Operator lowers significantly the barrier of entry to run in the cloud. From an end-user perspective, running a performance test in the cloud becomes a single command operation. Test isolation and Parallel tests : By default, the Operator is able to support any number of Parallel test executions with an absolute guarantee that each test is fully protected from being polluted by the existence of any number of other tests. Automation & CI : By having automation as a core focus point, teams and organizations can build performance testing directly into CI/CD pipelines. Meaning that every new service, feature or system can be potentially tested and validated for performance SLOs / SLAs. Separation of concerns : By using the Operator , engineering teams can focus on building a robust performance test/s and SREs DevOps teams can focus on managing the resources. Governance : Enable organizations to have governance over what / how resources are deployed and run on the cloud. Cloud cost optimization : Using the Operator enables for a more effective control over the cloud cost . Since resources are only deployed when needed and only for as long as needed, the cost of performance testing is kept to a minimum. Observability : For both engineering teams and cloud admins, the Operator unlocks the ability to build observability & monitoring dashboards in order to analyse test results during test runtime or retroactively (interesting for teams) and infrastructure usage and resource monitoring ( interesting for cloud admins, SREs, etc...).","title":"What does it offer"},{"location":"#whom-is-it-for","text":"It is built for...","title":"Whom is it for"},{"location":"#where-can-it-run","text":"Due to its design, the Operator can be deployed on any kubernetes cluster. Meaning that it is possible to have a full cloud native performance testing system anywhere in a matter of seconds.","title":"Where can it run"},{"location":"#limits","text":"The only real limit to this approach is the amount of cluster resources a given team or an organization is willing to dedicate to performance testing.","title":"Limits"},{"location":"#how-does-it-work","text":"To run a performance test, basic configuration is provided through a simple and intuitive kubernetes custom resource. Once deployed the Operator does all the heavy work of creating and scheduling the resources while making sure that all created load generation pods can effectively communicate with each other. To handle the challenge of delivering test script/s from local environment to the cluster and in turn to the deployed locust pods, the Operator support dynamic volume mounting from a configMaps source. This is indicated by a simple optional configuration. Meaning, if the configuration is present, the volume is mounted, and if it is not, no volume is mounted. Since a \" Picture Is Worth a Thousand Words \", here is a gif!","title":"How does it work"},{"location":"#steps-performed-in-demo","text":"Test configmap created in cluster. LocustTest CR deployed into the cluster. The Operator creating, configuring and scheduling test resources on CR creation event. The Operator cleaning up test resources after test CR has been removed event.","title":"Steps performed in demo"},{"location":"#getting-started","text":"Only 4 simple steps are needed to get a test up and running in the cluster: Valid Locust test script. Valid custom resource for LocustTest CRD: ( example ). To streamline this step, intensive-brew should be used. It is a simple cli tool that converts a declarative yaml into a compatible LocustTest kubernetes custom resource. ( Coming soon! ) Deploy test as a configMap kubectl create configmap <configMap-name> --from-file <your_test.py> Start the test by deploying the LocustTest custom resource. kubectl apply -f <valid_cr>.yaml Remove all performance test resources by removing the LocustTest custom resource. kubectl delete -f <valid_cr>.yaml","title":"Getting started"},{"location":"#roadmap","text":"Not in a particular order: Support HELM In depth \"getting started\" documentation Add traceability labels to generated resources Support for deploying test resources with node affinity / node taints Dashboard examples (Grafana + prometheus configuration) Enable event driven actions Integration with MSTeams: Push notification on test run completion / termination events UNDER_INVESTIGATION Benchmarking and collection of non-test generated metrics Investigation is on going to study the feasibility of supplying Locust pods with external metrics that are collected from service / system under-test. Test pods can then use this information to assess pass / fail criteria. This is especially useful in non-REST based services e.g. assess kafka(streams) microservice based on its consumer lag performance coming from the kafka broker.","title":"Roadmap"},{"location":"#project-status","text":"The project is actively maintained and is under continues development and improvement. If you have any request or want to chat, kindly open a ticket. If you wish to contribute code and / or ideas, kindly check the contribution section.","title":"project status"},{"location":"#contribute","text":"There's plenty to do, come say hi in the issues ! \ud83d\udc4b Also check out the CONTRIBUTING.MD \ud83e\udd13","title":"Contribute"},{"location":"#license","text":"Open source licensed under Apache-2.0 license (see LICENSE file for details).","title":"License"},{"location":"example/","text":"Welcome to MkDocs \u2693 For full documentation visit mkdocs.org . Commands \u2693 mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout \u2693 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files. Test \u2693 Warning Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Hello this is something thos is something else Ctrl + Alt + Del Text can be deleted and replacement text added . This can also be combined into one a single operation. Highlighting is also possible and comments can be added inline . Formatting can also be applied to blocks by putting the opening and closing tags on separate lines and adding new lines between the tags and the content. Lorem ipsum[^1] dolor sit amet, consectetur adipiscing elit. 1 Task List item 1 item A item B more text item a item b item c item C item 2 item 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. \u21a9","title":"Example"},{"location":"example/#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"example/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"example/#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"example/#test","text":"Warning Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Hello this is something thos is something else Ctrl + Alt + Del Text can be deleted and replacement text added . This can also be combined into one a single operation. Highlighting is also possible and comments can be added inline . Formatting can also be applied to blocks by putting the opening and closing tags on separate lines and adding new lines between the tags and the content. Lorem ipsum[^1] dolor sit amet, consectetur adipiscing elit. 1 Task List item 1 item A item B more text item a item b item c item C item 2 item 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. \u21a9","title":"Test"}]}