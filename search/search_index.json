{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Locust Kubernetes Operator","text":"<p>Enable performance testing for the modern era!</p> <p>Utilize the full power of Locust in the cloud.</p> <p> </p>"},{"location":"#at-a-glance","title":"At a glance","text":"<p>The Operator is designed to unlock seamless &amp; effortless distributed performance testing in the cloud and enable continues integration for CI / CD. By design, the entire system is cloud native and focuses on automation and CI practices. One strong feature about the system is its ability to horizontally scale to meet any required performance demands.</p>"},{"location":"#what-does-it-offer","title":"What does it offer","text":"<ul> <li> <p> Cloud Native</p> <p>Leverage the full power of Kubernetes and cloud-native technologies for distributed performance testing.</p> </li> <li> <p> Automation &amp; CI</p> <p>Integrate performance testing directly into your CI/CD pipelines for continuous validation.</p> </li> <li> <p> Governance</p> <p>Maintain control over how resources are deployed and used in the cloud.</p> </li> <li> <p> Observability</p> <p>Gain insights into test results and infrastructure usage with built-in observability features.</p> </li> </ul> <p>Check out the full list of features!</p> <p></p>"},{"location":"#whom-is-it-for","title":"Whom is it for","text":"<p>It is built for...</p> <p></p>"},{"location":"#where-can-it-run","title":"Where can it run","text":"<p>Due to its design, the Operator can be deployed on any kubernetes cluster. Meaning that it is possible to have a full cloud native performance testing system anywhere in a matter of seconds.</p>"},{"location":"#limits","title":"Limits","text":"<p>The only real limit to this approach is the amount of cluster resources a given team or an organization is willing to dedicate to performance testing.</p>"},{"location":"advanced_topics/","title":"Advanced topics","text":"<p>Basic configuration is not always enough to satisfy the performance test needs, for example when needing to work with Kafka and MSK. Below is a collection of topics of an advanced nature. This list will be keep growing as the tool matures more and more.</p>"},{"location":"advanced_topics/#kafka-aws-msk-configuration","title":"Kafka &amp; AWS MSK configuration","text":"<p>Generally speaking, the usage of Kafka in a locustfile is identical to how it would be used anywhere else within the cloud context. Thus, no special setup is needed for the purposes of performance testing with the Operator. That being said, if an organization is using kafka in production, chances are that authenticated kafka is being used. One of the main providers of such managed service is AWS in the form of MSK. For that end, the Operator have an out-of-the-box support for MSK.</p> <p>To enable performance testing with MSK, a central/global Kafka user can be created by the \"cloud admin\" or \"the team\" responsible for the Operator deployment within the organization. The Operator can then be easily configured to inject the configuration of that user as environment variables in all generated resources. Those variables can be used by the test to establish authentication with the kafka broker.</p> Variable Name Description <code>KAFKA_BOOTSTRAP_SERVERS</code> Kafka bootstrap servers <code>KAFKA_SECURITY_ENABLED</code> - <code>KAFKA_SECURITY_PROTOCOL_CONFIG</code> Security protocol. Options: <code>PLAINTEXT</code>, <code>SASL_PLAINTEXT</code>, <code>SASL_SSL</code>, <code>SSL</code> <code>KAFKA_SASL_MECHANISM</code> Authentication mechanism. Options: <code>PLAINTEXT</code>, <code>SCRAM-SHA-256</code>, <code>SCRAM-SHA-512</code> <code>KAFKA_USERNAME</code> The username used to authenticate Kafka clients with the Kafka server <code>KAFKA_PASSWORD</code> The password used to authenticate Kafka clients with the Kafka server"},{"location":"advanced_topics/#dedicated-kubernetes-nodes","title":"Dedicated Kubernetes Nodes","text":"<p>To run test resources on dedicated Kubernetes node(s), the Operator support deploying resources with Affinity and Taint Tolerations.</p>"},{"location":"advanced_topics/#affinity","title":"Affinity","text":"<p>This allows generated resources to have specific Affinity options.</p> <p>Note</p> <p>The Custom Resource Definition Spec is designed with modularity and expandability in mind. This means that although a specific set of Kubernetes Affinity options are supported today, extending this support based on need is a streamlined and easy processes. If additonal support is needed, don't hesitate to open a feature request.</p>"},{"location":"advanced_topics/#affinity-options","title":"Affinity Options","text":"<p>The specification for affinity is defined as follows</p> <code>affinity-spec.yaml</code> <pre><code>apiVersion: locust.io/v1\n...\nspec:\n...\naffinity:\n    nodeAffinity:\n        requiredDuringSchedulingIgnoredDuringExecution\n            &lt;label-key&gt;: &lt;label-value&gt;\n            ...\n...\n</code></pre>"},{"location":"advanced_topics/#node-affinity","title":"Node Affinity","text":"<p>This optional section causes generated pods to declare specific Node Affinity so Kubernetes scheduler becomes aware of this requirement.</p> <p>The implementation from the Custom Resource perspective is strongly influenced by Kubernetes native definition of node affinity. However, the implementation is on purpose slightly simplified in order to allow users to have easier time working with affinity.</p> <p>The <code>nodeAffinity</code> section supports declaring node affinity under <code>requiredDuringSchedulingIgnoredDuringExecution</code>. Meaning that any declared affinity labels must be present on nodes in order for resources to be deployed on them.</p> <p>Example:</p> <p>In the below example, generated pods will declare 3 required labels (keys and values) to be present on nodes before they are scheduled.</p> <code>node-affinity-example.yaml</code> <pre><code>apiVersion: locust.io/v1\n...\nspec:\n    ...\n    affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeAffinityLabel1: locust-cloud-tests\n            nodeAffinityLabel2: performance-nodes\n            nodeAffinityLabel3: high-memory\n            ...\n    ...\n</code></pre>"},{"location":"advanced_topics/#taint-tolerations","title":"Taint Tolerations","text":"<p>This allows generated resources to have specific Taint Tolerations options.</p>"},{"location":"advanced_topics/#toleration-options","title":"Toleration Options","text":"<p>The specification for tolerations is defined as follows</p> <code>taint-tolerations-spec.yaml</code> Example <pre><code>apiVersion: locust.io/v1\n...\nspec:\n  ...\n  tolerations:\n    - key: &lt;string value&gt;\n      operator: &lt;\"Exists\", \"Equal\"&gt;\n      value: &lt;string value&gt; #(1)!\n      effect: &lt;\"NoSchedule\", \"PreferNoSchedule\", \"NoExecute\"&gt;\n    ...\n</code></pre> <ol> <li>Optional when <code>operator</code> value is set to <code>Exists</code>.</li> </ol> <pre><code>apiVersion: locust.io/v1\n...\nspec:\n  ...\n  tolerations:\n    - key: taint-A\n      operator: Equal\n      value: ssd\n      effect: NoSchedule\n    - key: taint-B\n      operator: Exists\n      effect: NoExecute\n</code></pre>"},{"location":"advanced_topics/#resource-management","title":"Resource Management","text":"<p>The operator allows for fine-grained control over the resource requests and limits for the Locust master and worker pods. This is useful for ensuring that your load tests have the resources they need, and for preventing them from consuming too many resources on your cluster.</p> <p>Configuration is done via the <code>application.yml</code> file or through Helm values. The following properties are available:</p> <ul> <li><code>locust.operator.resource.pod-mem-request</code></li> <li><code>locust.operator.resource.pod-cpu-request</code></li> <li><code>locust.operator.resource.pod-ephemeral-storage-request</code></li> <li><code>locust.operator.resource.pod-mem-limit</code></li> <li><code>locust.operator.resource.pod-cpu-limit</code></li> <li><code>locust.operator.resource.pod-ephemeral-storage-limit</code></li> </ul>"},{"location":"advanced_topics/#disabling-cpu-limits","title":"Disabling CPU Limits","text":"<p>In some scenarios, particularly during performance-sensitive tests, you may want to disable CPU limits to prevent throttling. This can be achieved by setting the <code>cpuLimit</code> property to a blank string in your Helm values.</p> Helm Values Example <pre><code>config:\n  loadGenerationPods:\n    resource:\n      cpuLimit: \"\" # (1)!\n</code></pre> <ol> <li>Setting the CPU limit to an empty string disables it.</li> </ol> <p>Note</p> <p>When the CPU limit is disabled, the pod is allowed to use as much CPU as is available on the node. This can be useful for maximizing performance, but it can also lead to resource contention if not managed carefully.</p>"},{"location":"advanced_topics/#usage-of-a-private-image-registry","title":"Usage of a private image registry","text":"<p>Images from a private image registry can be used through various methods as described in the kubernetes documentation, one of those methods depends on setting <code>imagePullSecrets</code> for pods. This is supported in the operator by simply setting the <code>imagePullSecrets</code> option in the deployed custom resource. For example:</p> locusttest-pull-secret-cr.yaml<pre><code>apiVersion: locust.io/v1\n...\nspec:\n  image: ghcr.io/mycompany/locust:latest # (1)!\n  imagePullSecrets: # (2)!\n    - gcr-secret\n  ...\n</code></pre> <ol> <li>Specify which Locust image to use for both master and worker containers.</li> <li>[Optional] Specify an existing pull secret to use for master and worker pods.</li> </ol>"},{"location":"advanced_topics/#image-pull-policy","title":"Image pull policy","text":"<p>Kubernetes uses the image tag and pull policy to control when kubelet attempts to download (pull) a container image. The image pull policy can be defined through the <code>imagePullPolicy</code> option, as explained in the kubernetes documentation. When using the operator, the <code>imagePullPolicy</code> option can be directly configured in the custom resource. For example:</p> locusttest-pull-policy-cr.yaml<pre><code>apiVersion: locust.io/v1\n...\nspec:\n  image: ghcr.io/mycompany/locust:latest # (1)!\n  imagePullPolicy: Always # (2)!\n  ...\n</code></pre> <ol> <li>Specify which Locust image to use for both master and worker containers.</li> <li>[Optional] Specify the pull policy to use for containers defined within master and worker containers. Supported options include <code>Always</code>, <code>IfNotPresent</code> and <code>Never</code>.</li> </ol>"},{"location":"advanced_topics/#automatic-cleanup-for-finished-master-and-worker-jobs","title":"Automatic Cleanup for Finished Master and Worker Jobs","text":"<p>Once load tests finish, master and worker jobs remain available in Kubernetes. You can set up a time-to-live (TTL) value in the operator's Helm chart, so that kubernetes jobs are eligible for cascading removal once the TTL expires. This means that Master and Worker jobs and their dependent objects (e.g., pods) will be deleted.</p> <p>Note that setting up a TTL will not delete <code>LocustTest</code> or <code>ConfigMap</code> resources.</p> <p>To set a TTL value, override the key <code>ttlSecondsAfterFinished</code> in <code>values.yaml</code>:</p> <code>values.yaml</code> <pre><code>...\nconfig:\n  loadGenerationJobs:\n    # Either leave empty or use an empty string to avoid setting this option\n    ttlSecondsAfterFinished: 3600 # (1)!\n...\n</code></pre> <ol> <li>Time in seconds to keep the job after it finishes.</li> </ol> <p>You can also use Helm's CLI arguments: <code>helm install ... --set config.loadGenerationJobs.ttlSecondsAfterFinished=0</code>.</p> <p>Read more about the <code>ttlSecondsAfterFinished</code> parameter in Kubernetes's official documentation.</p>"},{"location":"advanced_topics/#kubernetes-support-for-ttlsecondsafterfinished","title":"Kubernetes Support for <code>ttlSecondsAfterFinished</code>","text":"<p>Support for parameter <code>ttlSecondsAfterFinished</code> was added in Kubernetes v1.12. In case you're deploying the locust operator to a Kubernetes cluster that does not support <code>ttlSecondsAfterFinished</code>, you may leave the Helm key empty or use an empty string. In this case, job definitions will not include the parameter.</p>"},{"location":"contribute/","title":"Contributing &amp; Development","text":""},{"location":"contribute/#ways-to-contribute","title":"Ways to Contribute","text":"<p>There are several ways you can contribute to the Locust K8s Operator project:</p>"},{"location":"contribute/#for-everyone","title":"For Everyone","text":"<ul> <li>Reporting Issues: Found a bug or have a feature request? Open an issue \ud83d\udc4b</li> <li>Documentation: Help improve the documentation by suggesting clarifications or additions</li> <li>Community Support: Answer questions and help others in the issue tracker</li> </ul>"},{"location":"contribute/#for-developers","title":"For Developers","text":"<p>Note: The following sections are intended for developers who want to contribute code to the project. If you're just using the operator, you can skip these sections.</p> <ul> <li>Code Contributions: Implement new features or fix bugs</li> <li>Testing: Improve test coverage and test in different environments</li> <li>Review: Review pull requests from other contributors</li> </ul>"},{"location":"contribute/#project-status","title":"Project Status","text":"<p>The project is actively maintained and is under continuous development and improvement. If you have any request or want to chat, kindly open a ticket. If you wish to contribute code and/or ideas, please review the development documentation below.</p>"},{"location":"contribute/#development-documentation","title":"Development Documentation","text":"<p>For developers contributing to the Locust K8s Operator project, we provide detailed documentation on various development aspects:</p> <ul> <li>Local Development Guide: Setting up your development environment</li> <li>Integration Testing Guide: Running and creating integration tests</li> <li>Pull Request Process: Guidelines for submitting code changes</li> </ul> <p>You can also refer to the comprehensive CONTRIBUTING.MD file in the GitHub repository for more information.</p>"},{"location":"features/","title":"Features","text":"<ul> <li> <p> Cloud Native &amp; Kubernetes Integration</p> <p>Leverage the full power of Kubernetes for distributed performance testing. The operator is designed to be cloud-native, enabling seamless deployment and scaling on any Kubernetes cluster.</p> </li> <li> <p> Automation &amp; CI/CD</p> <p>Integrate performance testing directly into your CI/CD pipelines. Automate the deployment, execution, and teardown of your Locust tests for continuous performance validation.</p> </li> <li> <p> Governance &amp; Resource Management</p> <p>Maintain control over how resources are deployed and used. Configure resource requests and limits for Locust master and worker pods, and even disable CPU limits for performance-sensitive tests.</p> </li> <li> <p> Observability &amp; Monitoring</p> <p>Gain insights into test results and infrastructure usage. The operator supports Prometheus metrics out-of-the-box, allowing you to build rich monitoring dashboards.</p> </li> <li> <p> Cost Optimization</p> <p>Optimize cloud costs by deploying resources only when needed and for as long as needed. The operator's automatic cleanup feature ensures that resources are terminated after a test run.</p> </li> <li> <p> Test Isolation &amp; Parallelism</p> <p>Run multiple tests in parallel with guaranteed isolation. Each test runs in its own set of resources, preventing any cross-test interference.</p> </li> <li> <p> Private Image Registry Support</p> <p>Use images from private registries for your Locust tests. The operator supports <code>imagePullSecrets</code> and configurable <code>imagePullPolicy</code>.</p> </li> <li> <p> Lib ConfigMap Support</p> <p>Mount lib directories via ConfigMap for your Locust tests. This feature allows you to include shared libraries and modules without modifying test files or patching images, similar to the helm chart's <code>locust_lib_configmap</code> functionality.</p> </li> <li> <p> Advanced Scheduling</p> <p>Control where your Locust pods are scheduled using Kubernetes affinity and taint tolerations. This allows you to run tests on dedicated nodes or in specific availability zones.</p> </li> <li> <p> Kafka &amp; AWS MSK Integration</p> <p>Seamlessly integrate with Kafka and AWS MSK for performance testing of event-driven architectures. The operator provides out-of-the-box support for authenticated Kafka.</p> </li> </ul>"},{"location":"getting_started/","title":"Getting started","text":"<p>Only few simple steps are needed to get a test up and running in the cluster. The following is a step-by-step guide on how to achieve this.</p>"},{"location":"getting_started/#step-1-write-a-valid-locust-test-script","title":"Step 1: Write a valid Locust test script","text":"<p>For this example, we will be using the following script</p> demo_test.py<pre><code>from locust import HttpUser, task\n\nclass User(HttpUser): # (1)!\n    @task #(2)!\n    def get_employees(self) -&gt; None:\n        \"\"\"Get a list of employees.\"\"\"\n        self.client.get(\"/api/v1/employees\") #(3)!\n</code></pre> <ol> <li>Class representing <code>users</code> that will be simulated by Locust.</li> <li>One or more <code>task</code> that each simulated <code>user</code> will be performing.</li> <li>HTTP call to a specific endpoint.</li> </ol> <p>Note</p> <p>To be able to run performance tests effectivly, an understanding of Locust which is the underline load generation tool is required. All tests must be valid locust tests.</p> <p>Locust provide a very good and detail rich documentation that can be found here.</p>"},{"location":"getting_started/#step-2-write-a-valid-custom-resource-for-locusttest-crd","title":"Step 2: Write a valid custom resource for LocustTest CRD","text":"<p>A simple custom resource for the previous test can be something like the following example;</p> <p>To streamline this step, intensive-brew should be used. It is a simple cli tool that converts a declarative yaml into a compatible LocustTest kubernetes custom resource.</p> locusttest-cr.yaml<pre><code>apiVersion: locust.io/v1 # (1)!\nkind: LocustTest # (2)!\nmetadata:\n  name: demo.test # (3)!\nspec:\n  image: locustio/locust:latest # (4)!\n  masterCommandSeed: # (5)!\n    --locustfile /lotest/src/demo_test.py\n    --host https://dummy.restapiexample.com\n    --users 100\n    --spawn-rate 3\n    --run-time 3m\n  workerCommandSeed: --locustfile /lotest/src/demo_test.py # (6)!\n  workerReplicas: 3 # (7)!\n  configMap: demo-test-map # (8)!\n  libConfigMap: demo-lib-map # (9)!\n</code></pre> <ol> <li>API version based on the deployed LocustTest CRD.</li> <li>Resource kind.</li> <li>The name field used by the operator to infer the names of test generated resources. While this value is insignificant to the Operator     itself, it is important to keep a good convention here since it helps in tracking resources across the cluster when needed.</li> <li>Image to use for the load generation pods</li> <li>Seed command for the master node. The Operator will append to this seed command/s all operational parameters needed for the master     to perform its job e.g. ports, rebalancing settings, timeouts, etc...</li> <li>Seed command for the worker node. The Operator will append to this seed command/s all operational parameters needed for the worker     to perform its job e.g. ports, master node url, master node ports, etc...</li> <li>The amount of worker nodes to spawn in the cluster.</li> <li>[Optional] Name of configMap to mount into the pod</li> <li>[Optional] Name of configMap containing lib directory files to mount at <code>/opt/locust/lib</code></li> </ol>"},{"location":"getting_started/#other-options","title":"Other options","text":""},{"location":"getting_started/#labels-and-annotations","title":"Labels and annotations","text":"<p>You can add labels and annotations to generated Pods. For example:</p> locusttest-cr.yaml<pre><code>apiVersion: locust.io/v1\n...\nspec:\n  image: locustio/locust:latest\n  labels: # (1)!\n    master:\n      locust.io/role: \"master\"\n      myapp.com/testId: \"abc-123\"\n      myapp.com/tenantId: \"xyz-789\"\n    worker:\n      locust.io/role: \"worker\"\n  annotations: # (2)!\n    master:\n      myapp.com/threads: \"1000\"\n      myapp.com/version: \"2.1.0\"\n    worker:\n      myapp.com/version: \"2.1.0\"\n  ...\n</code></pre> <ol> <li>[Optional] Labels are attached to both master and worker pods. They can later be used to identify pods belonging to a particular execution context. This is useful, for example, when tests are deployed programmatically. A launcher application can query the Kubernetes API for specific resources.</li> <li>[Optional] Annotations too are attached to master and worker pods. They can be used to include additional context about a test. For example, configuration parameters of the software system being tested.</li> </ol> <p>Both labels and annotations can be added to the Prometheus configuration, so that metrics are associated with the appropriate information, such as the test and tenant ids. You can read more about this in the Prometheus documentation site.</p>"},{"location":"getting_started/#step-3-deploy-locust-k8s-operator-in-the-cluster","title":"Step 3: Deploy Locust k8s Operator in the cluster.","text":"<p>The recommended way to install the Operator is by using the official HELM chart. Documentation on how to perform that is available here.</p>"},{"location":"getting_started/#step-4-deploy-test-as-a-configmap","title":"Step 4: Deploy test as a configMap","text":"<p>For the purposes of this example, the <code>demo_test.py</code> test previously demonstrated will be deployed into the cluster as a configMap that the Operator will mount to the load generation pods. To deploy the test as a configMap, run the bellow command following this template <code>kubectl create configmap &lt;configMap-name&gt; --from-file &lt;your_test.py&gt;</code>:</p> <ul> <li><code>kubectl create configmap demo-test-map --from-file demo_test.py</code></li> </ul>"},{"location":"getting_started/#step-41-deploy-lib-files-as-a-configmap-optional","title":"Step 4.1: Deploy lib files as a configMap (Optional)","text":"<p>What are lib files and why use this feature?</p> <p>Lib files are Python modules and libraries that your Locust tests depend on. When your tests require custom helper functions, utilities, or shared code that should be available across multiple test files, this feature allows you to package and deploy them alongside your tests.</p>"},{"location":"getting_started/#how-it-works","title":"How it works","text":"<p>The Locust Kubernetes Operator provides a mechanism to deploy your custom Python libraries as a ConfigMap, which will then be mounted to the <code>/opt/locust/lib</code> directory inside all Locust pods (both master and worker). This allows your test scripts to import and use these libraries.</p> <p>For example, if you have the following structure:</p> <pre><code>project/\n\u251c\u2500\u2500 my_test.py          # Your main Locust test file\n\u2514\u2500\u2500 lib/\n    \u251c\u2500\u2500 helpers.py      # Helper functions\n    \u251c\u2500\u2500 utils.py        # Utility functions\n    \u2514\u2500\u2500 models.py       # Data models\n</code></pre> <p>Your test might import these libraries like this:</p> <pre><code># in my_test.py\nfrom lib.helpers import some_helper_function\nfrom lib.utils import format_data\n</code></pre> <p>To make these imports work when your test runs in Kubernetes, you need to:</p> <ol> <li>Deploy your lib files as a ConfigMap</li> <li>Reference this ConfigMap in your LocustTest custom resource</li> </ol>"},{"location":"getting_started/#step-by-step-instructions","title":"Step-by-step instructions","text":"<p>1. Create a ConfigMap from your lib directory:</p> <p>You can deploy all library files from a directory:</p> <pre><code># Deploy all files from the lib/ directory as a ConfigMap\nkubectl create configmap demo-lib-map --from-file=lib/\n</code></pre> <p>Alternatively, you can create it from individual files:</p> <pre><code># Deploy specific files as a ConfigMap\nkubectl create configmap demo-lib-map --from-file=lib/helpers.py --from-file=lib/utils.py\n</code></pre> <p>2. Reference the lib ConfigMap in your LocustTest custom resource:</p> <pre><code>apiVersion: locust.io/v1\nkind: LocustTest\nmetadata:\n  name: example-locusttest\nspec:\n  masterConfig:\n    replicas: 1\n  workerConfig:\n    replicas: 2\n  configMap: demo-test-map    # Your test script ConfigMap\n  libConfigMap: demo-lib-map   # Your lib files ConfigMap\n</code></pre> <p>Organizing your code</p> <p>This feature is especially useful when:</p> <ol> <li>You have complex test scenarios that benefit from modular code</li> <li>You want to share code between multiple Locust tests</li> <li>You need to keep your test scripts clean by separating implementation details</li> </ol> <p>Fresh cluster resources</p> <p>Fresh cluster resources are allocated for each running test, meaning that tests DO NOT have any cross impact on each other.</p>"},{"location":"getting_started/#step-5-start-the-test-by-deploying-the-locusttest-custom-resource","title":"Step 5: Start the test by deploying the LocustTest custom resource.","text":"<p>Deploying a custom resource, signals to the Operator the desire to start a test and thus the Operator starts creating and scheduling all needed resources. To do that, deploy the custom resource following this template <code>kubectl apply -f &lt;valid_cr&gt;.yaml</code>:</p> <ul> <li><code>kubectl apply -f locusttest-cr.yaml</code></li> </ul>"},{"location":"getting_started/#step-51-check-cluster-for-running-resources","title":"Step 5.1: Check cluster for running resources","text":"<p>At this point, it is possible to check the cluster and all required resources will be running based on the passed configuration in the custom resource.</p> <p>The Operator will create the following resources in the cluster for each valid custom resource:</p> <ul> <li>A kubernetes service for the master node so it is reachable by other worker nodes.</li> <li>A kubernetes Job to manage the master node.</li> <li>A kubernetes Job to manage the worker node.</li> </ul>"},{"location":"getting_started/#step-6-clear-resources-after-test-run","title":"Step 6: Clear resources after test run","text":"<p>In order to remove the cluster resources after a test run, simply remove the custom resource and the Operator will react to this event by cleaning the cluster of all related resources. To delete a resource, run the below command following this template <code>kubectl delete -f &lt;valid_cr&gt;.yaml</code>:</p> <ul> <li><code>kubectl delete -f locusttest-cr.yaml</code></li> </ul>"},{"location":"helm_deploy/","title":"HELM Deployment Guide","text":"<p>This guide provides comprehensive instructions for deploying the Locust Kubernetes Operator using its official Helm chart.</p>"},{"location":"helm_deploy/#quick-start","title":"Quick Start","text":"<p>For experienced users, here are the essential commands to get the operator running in the <code>default</code> namespace:</p> <pre><code>helm repo add locust-k8s-operator https://abdelrhmanhamouda.github.io/locust-k8s-operator/\nhelm repo update\nhelm install locust-operator locust-k8s-operator/locust-k8s-operator\n</code></pre>"},{"location":"helm_deploy/#installation","title":"Installation","text":""},{"location":"helm_deploy/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster (e.g., Minikube, GKE, EKS, AKS).</li> <li>Helm 3 installed on your local machine.</li> </ul>"},{"location":"helm_deploy/#step-1-add-the-helm-repository","title":"Step 1: Add the Helm Repository","text":"<p>First, add the Locust Kubernetes Operator Helm repository to your local Helm client:</p> <pre><code>helm repo add locust-k8s-operator https://abdelrhmanhamouda.github.io/locust-k8s-operator/\n</code></pre> <p>Next, update your local chart repository cache to ensure you have the latest version:</p> <pre><code>helm repo update\n</code></pre>"},{"location":"helm_deploy/#step-2-install-the-chart","title":"Step 2: Install the Chart","text":"<p>You can install the chart with a release name of your choice (e.g., <code>locust-operator</code>).</p> <p>Default Installation:</p> <p>To install the chart with the default configuration into the currently active namespace, run:</p> <pre><code>helm install locust-operator locust-k8s-operator/locust-k8s-operator\n</code></pre> <p>Installation with a Custom Values File:</p> <p>For more advanced configurations, it's best to use a custom <code>values.yaml</code> file. Create a file named <code>my-values.yaml</code> and add your overrides:</p> <pre><code># my-values.yaml\nreplicaCount: 2\n\nconfig:\n  loadGenerationPods:\n    resource:\n      cpuLimit: \"2000m\"\n      memLimit: \"2048Mi\"\n</code></pre> <p>Then, install the chart, specifying your custom values file and a target namespace:</p> <pre><code>helm install locust-operator locust-k8s-operator/locust-k8s-operator \\\n  --namespace locust-system \\\n  --create-namespace \\\n  -f my-values.yaml\n</code></pre>"},{"location":"helm_deploy/#verifying-the-installation","title":"Verifying the Installation","text":"<p>After installation, you can verify that the operator is running correctly by checking the pods in the target namespace:</p> <pre><code>kubectl get pods -n locust-system\n</code></pre> <p>You should see a pod with a name similar to <code>locust-operator-b5c9f4f7-xxxxx</code> in the <code>Running</code> state.</p> <p>To view the operator's logs, run:</p> <pre><code>kubectl logs -f -n locust-system -l app.kubernetes.io/name=locust-k8s-operator\n</code></pre>"},{"location":"helm_deploy/#configuration","title":"Configuration","text":"<p>The following tables list the configurable parameters of the Locust Operator Helm chart and their default values.</p>"},{"location":"helm_deploy/#general-configuration","title":"General Configuration","text":"Parameter Description Default <code>appPort</code> The port that the operator will listen on. <code>8080</code>"},{"location":"helm_deploy/#deployment-settings","title":"Deployment Settings","text":"Parameter Description Default <code>replicaCount</code> Number of replicas for the operator deployment. <code>1</code> <code>image.repository</code> The repository of the Docker image. <code>lotest/locust-k8s-operator</code> <code>image.pullPolicy</code> The image pull policy. <code>IfNotPresent</code> <code>image.tag</code> Overrides the default image tag (defaults to the chart's <code>appVersion</code>). <code>\"\"</code>"},{"location":"helm_deploy/#kubernetes-resources","title":"Kubernetes Resources","text":"Parameter Description Default <code>k8s.customResourceDefinition.deploy</code> Specifies whether to deploy the <code>LocustTest</code> CRD. <code>true</code> <code>k8s.clusterRole.enabled</code> Deploy with a cluster-wide role (<code>true</code>) or a namespaced role (<code>false</code>). <code>true</code> <code>serviceAccount.create</code> Specifies whether a service account should be created. <code>true</code> <code>serviceAccount.name</code> The name of the service account to use. If not set, a name is generated. <code>\"\"</code> <code>resources</code> Resource requests and limits for the operator pod. <code>{}</code>"},{"location":"helm_deploy/#operator-configuration","title":"Operator Configuration","text":"Parameter Description Default <code>config.loadGenerationJobs.ttlSecondsAfterFinished</code> Time-to-live in seconds for finished load generation jobs. Set to <code>\"\"</code> to disable. <code>\"\"</code>"},{"location":"helm_deploy/#load-generation-pods","title":"Load Generation Pods","text":"Parameter Description Default <code>config.loadGenerationPods.resource.cpuRequest</code> CPU resource request for load generation pods. <code>250m</code> <code>config.loadGenerationPods.resource.memRequest</code> Memory resource request for load generation pods. <code>128Mi</code> <code>config.loadGenerationPods.resource.ephemeralRequest</code> Ephemeral storage request for load generation pods. <code>30M</code> <code>config.loadGenerationPods.resource.cpuLimit</code> CPU resource limit for load generation pods. Set to <code>\"\"</code> to unbind. <code>1000m</code> <code>config.loadGenerationPods.resource.memLimit</code> Memory resource limit for load generation pods. Set to <code>\"\"</code> to unbind. <code>1024Mi</code> <code>config.loadGenerationPods.resource.ephemeralLimit</code> Ephemeral storage limit for load generation pods. Set to <code>\"\"</code> to unbind. <code>50M</code> <code>config.loadGenerationPods.affinity.enableCrInjection</code> Enable Custom Resource injection for affinity settings. <code>true</code> <code>config.loadGenerationPods.taintTolerations.enableCrInjection</code> Enable Custom Resource injection for taint tolerations. <code>true</code>"},{"location":"helm_deploy/#metrics-exporter","title":"Metrics Exporter","text":"Parameter Description Default <code>config.loadGenerationPods.metricsExporter.image</code> Metrics Exporter Docker image. <code>containersol/locust_exporter:v0.5.0</code> <code>config.loadGenerationPods.metricsExporter.port</code> Metrics Exporter port. <code>9646</code> <code>config.loadGenerationPods.metricsExporter.pullPolicy</code> Image pull policy for the metrics exporter. <code>IfNotPresent</code>"},{"location":"helm_deploy/#pod-scheduling","title":"Pod Scheduling","text":"Parameter Description Default <code>nodeSelector</code> Node selector for scheduling the operator pod. <code>{}</code> <code>tolerations</code> Tolerations for scheduling the operator pod. <code>[]</code> <code>affinity</code> Affinity rules for scheduling the operator pod. <code>{}</code>"},{"location":"helm_deploy/#liveness-and-readiness-probes","title":"Liveness and Readiness Probes","text":"Parameter Description Default <code>livenessProbe.initialDelaySeconds</code> Initial delay for the liveness probe. <code>10</code> <code>livenessProbe.periodSeconds</code> How often to perform the liveness probe. <code>20</code> <code>livenessProbe.timeoutSeconds</code> When the liveness probe times out. <code>10</code> <code>livenessProbe.failureThreshold</code> When to give up on the liveness probe. <code>1</code> <code>readinessProbe.initialDelaySeconds</code> Initial delay for the readiness probe. <code>30</code> <code>readinessProbe.periodSeconds</code> How often to perform the readiness probe. <code>20</code> <code>readinessProbe.timeoutSeconds</code> When the readiness probe times out. <code>10</code> <code>readinessProbe.failureThreshold</code> When to give up on the readiness probe. <code>1</code>"},{"location":"helm_deploy/#advanced-configuration","title":"Advanced Configuration","text":"<p>The following sections cover advanced configuration options. For a complete list of parameters, refer to the <code>values.yaml</code> file in the chart.</p>"},{"location":"helm_deploy/#kafka-configuration","title":"Kafka Configuration","text":"Parameter Description Default <code>config.loadGenerationPods.kafka.bootstrapServers</code> Kafka bootstrap servers. <code>localhost:9092</code> <code>config.loadGenerationPods.kafka.acl.enabled</code> Enable ACL settings for Kafka. <code>false</code> <code>config.loadGenerationPods.kafka.sasl.mechanism</code> SASL mechanism for authentication. <code>SCRAM-SHA-512</code>"},{"location":"helm_deploy/#micronaut-metrics","title":"Micronaut Metrics","text":"Parameter Description Default <code>micronaut.metrics.enabled</code> Enable/disable all Micronaut metrics. <code>true</code> <code>micronaut.metrics.export.prometheus.step</code> The step size (duration) for Prometheus metrics export. <code>PT30S</code>"},{"location":"helm_deploy/#upgrading-the-chart","title":"Upgrading the Chart","text":"<p>To upgrade an existing release to a new version, use the <code>helm upgrade</code> command:</p> <pre><code>helm upgrade locust-operator locust-k8s-operator/locust-k8s-operator -f my-values.yaml\n</code></pre>"},{"location":"helm_deploy/#uninstalling-the-chart","title":"Uninstalling the Chart","text":"<p>To uninstall and delete the <code>locust-operator</code> deployment, run:</p> <pre><code>helm uninstall locust-operator\n</code></pre> <p>This command will remove all the Kubernetes components associated with the chart and delete the release.</p>"},{"location":"helm_deploy/#next-steps","title":"Next Steps","text":"<p>Once the operator is installed, you're ready to start running performance tests! Head over to the Getting Started guide to learn how to deploy your first <code>LocustTest</code>.</p>"},{"location":"how_does_it_work/","title":"How does it work","text":"<p>To run a performance test, basic configuration is provided through a simple and intuitive kubernetes custom resource. Once deployed the Operator does all the heavy work of creating and scheduling the resources while making sure that all created load generation pods can effectively communicate with each other.</p> <p>To handle the challenge of delivering test script/s from local environment to the cluster and in turn to the deployed locust pods, the Operator support dynamic volume mounting from a configMaps source. This is indicated by a simple optional configuration. Meaning, if the configuration is present, the volume is mounted, and if it is not, no volume is mounted.</p> <p>Since a \"Picture Is Worth a Thousand Words\", here is a gif! </p>"},{"location":"how_does_it_work/#steps-performed-in-demo","title":"Steps performed in demo","text":"<ul> <li> Test configmap created in cluster.</li> <li> LocustTest CR deployed into the cluster.</li> <li> The Operator creating, configuring and scheduling test resources on CR creation event.</li> <li> The Operator cleaning up test resources after test CR has been removed event.</li> </ul>"},{"location":"integration-testing/","title":"Integration Testing Guide","text":"<p>This document describes the comprehensive integration testing setup for the Locust K8s Operator, which validates the complete end-to-end functionality beyond unit tests.</p>"},{"location":"integration-testing/#overview","title":"Overview","text":"<p>The integration test suite performs the following workflow: 1. Build - Creates the operator Docker image 2. Package - Packages the Helm chart 3. Deploy - Spins up a K8s cluster (K3s) and installs the operator 4. Test - Deploys a LocustTest CR and validates operator behavior 5. Validate - Ensures Locust master/workers are running correctly 6. Cleanup - Removes all resources and tears down the environment</p>"},{"location":"integration-testing/#architecture","title":"Architecture","text":""},{"location":"integration-testing/#test-framework","title":"Test Framework","text":"<ul> <li>Testing Framework: JUnit 5 with Testcontainers</li> <li>Kubernetes Cluster: K3s via Testcontainers for local development, KinD in CI environment</li> <li>Build System: Gradle with custom integration test source set</li> <li>Container Management: Docker with Jib plugin for image building</li> </ul>"},{"location":"integration-testing/#test-structure","title":"Test Structure","text":"<pre><code>src/integrationTest/\n\u251c\u2500\u2500 java/com/locust/operator/\n\u2502   \u2514\u2500\u2500 LocustOperatorIntegrationTest.java    # Main integration test\n\u2514\u2500\u2500 resources/\n    \u2514\u2500\u2500 application-test.yml                   # Test configuration\n</code></pre>"},{"location":"integration-testing/#prerequisites","title":"Prerequisites","text":""},{"location":"integration-testing/#local-development","title":"Local Development","text":"<ul> <li>Docker: Running Docker daemon</li> <li>Java 21: Required for building the operator</li> <li>Helm 3.x: For chart packaging and installation</li> <li>Gradle: Uses project's gradle wrapper</li> </ul>"},{"location":"integration-testing/#cicd-github-actions","title":"CI/CD (GitHub Actions)","text":"<ul> <li>Uses Ubuntu latest runner</li> <li>Automatically installs all dependencies</li> <li>Runs on PR and push to main branch</li> </ul>"},{"location":"integration-testing/#running-integration-tests","title":"Running Integration Tests","text":""},{"location":"integration-testing/#option-1-using-the-integration-test-script-recommended","title":"Option 1: Using the Integration Test Script (Recommended)","text":"<pre><code># Make script executable (first time only)\nchmod +x scripts/run-integration-test.sh\n\n# Run integration tests\n./scripts/run-integration-test.sh\n</code></pre> <p>The script performs several helpful functions: - Checks prerequisites (Docker, Helm, Java) - Cleans up previous runs and Docker resources - Runs the integration tests with proper error handling - Provides detailed error reporting and logs - Shows test results and report locations</p>"},{"location":"integration-testing/#option-2-using-gradle-directly","title":"Option 2: Using Gradle Directly","text":"<pre><code># Run integration tests\n./gradlew integrationTest -PrunIntegrationTests\n\n# Run with verbose output\n./gradlew integrationTest -PrunIntegrationTests --info\n\n# Run specific test class\n./gradlew integrationTest -PrunIntegrationTests --tests=\"LocustOperatorIntegrationTest\"\n</code></pre>"},{"location":"integration-testing/#option-3-in-cicd","title":"Option 3: In CI/CD","text":"<p>Integration tests run automatically in GitHub Actions: - On pull requests to <code>main</code> or <code>master</code> - On pushes to <code>main</code> or <code>master</code> - Can be triggered manually via <code>workflow_dispatch</code></p>"},{"location":"integration-testing/#test-scenarios","title":"Test Scenarios","text":""},{"location":"integration-testing/#test-1-operator-deployment","title":"Test 1: Operator Deployment","text":"<ul> <li>Creates operator namespace</li> <li>Installs operator via Helm chart</li> <li>Validates operator deployment is ready</li> <li>Verifies operator pod is running</li> </ul>"},{"location":"integration-testing/#test-2-locusttest-deployment","title":"Test 2: LocustTest Deployment","text":"<ul> <li>Creates test namespace and ConfigMap with simple Locust script</li> <li>Deploys LocustTest custom resource</li> <li>Validates master and worker deployments are created</li> <li>Ensures all pods reach Running state</li> </ul>"},{"location":"integration-testing/#test-3-locusttest-execution","title":"Test 3: LocustTest Execution","text":"<ul> <li>Verifies Locust master web interface starts</li> <li>Checks master logs for successful initialization</li> <li>Validates workers connect to master</li> <li>Confirms test environment is functional</li> </ul>"},{"location":"integration-testing/#test-4-cleanup","title":"Test 4: Cleanup","text":"<ul> <li>Deletes LocustTest custom resource</li> <li>Verifies all managed resources are cleaned up</li> <li>Uninstalls operator</li> <li>Validates complete cleanup</li> </ul>"},{"location":"integration-testing/#configuration","title":"Configuration","text":""},{"location":"integration-testing/#integration-test-configuration","title":"Integration Test Configuration","text":"<p>Located in <code>gradle/integration-test.gradle</code>: - Defines separate source set for integration tests - Configures dependencies (Testcontainers, Kubernetes client, etc.) - Sets up test reporting and timeouts - Links to main build pipeline</p>"},{"location":"integration-testing/#test-application-configuration","title":"Test Application Configuration","text":"<p>Located in <code>src/integrationTest/resources/application-test.yml</code>: - Configures logging levels for test visibility - Sets timeouts for different test phases - Defines resource locations and image names</p>"},{"location":"integration-testing/#ci-configuration","title":"CI Configuration","text":"<p>Located in <code>.github/workflows/integration-test.yml</code>: - GitHub Actions workflow for automated testing - Includes caching for Gradle and Docker layers - Uploads test results as artifacts - Uses KinD (Kubernetes in Docker) cluster with custom configuration in <code>.github/kind-config.yaml</code> - Uses Helm 3.12.0 for chart installation</p>"},{"location":"integration-testing/#sample-locusttest-resource","title":"Sample LocustTest Resource","text":"<p>The integration test creates this sample LocustTest CR:</p> <pre><code>apiVersion: locust.io/v1\nkind: LocustTest\nmetadata:\n  name: integration-test\n  namespace: locust-tests\nspec:\n  masterConfig:\n    replicas: 1\n    image: locustio/locust:2.15.1\n    resources:\n      requests:\n        memory: \"128Mi\"\n        cpu: \"100m\"\n      limits:\n        memory: \"256Mi\"\n        cpu: \"200m\"\n  workerConfig:\n    replicas: 2\n    image: locustio/locust:2.15.1\n    resources:\n      requests:\n        memory: \"128Mi\"\n        cpu: \"100m\"\n      limits:\n        memory: \"256Mi\"\n        cpu: \"200m\"\n  configMap: locust-test-scripts\n</code></pre>"},{"location":"integration-testing/#test-reports-and-artifacts","title":"Test Reports and Artifacts","text":""},{"location":"integration-testing/#local-testing","title":"Local Testing","text":"<ul> <li>HTML Report: <code>build/reports/integration-tests/index.html</code></li> <li>JUnit XML: <code>build/test-results/integration-test/</code></li> <li>Logs: <code>/tmp/locust-integration-test-{timestamp}.log</code></li> </ul>"},{"location":"integration-testing/#ci-testing","title":"CI Testing","text":"<ul> <li>Test results uploaded as GitHub Actions artifacts</li> <li>Available for download from the Actions run page</li> <li>Includes both HTML reports and raw XML results</li> </ul>"},{"location":"integration-testing/#troubleshooting","title":"Troubleshooting","text":""},{"location":"integration-testing/#common-issues","title":"Common Issues","text":""},{"location":"integration-testing/#docker-permission-errors","title":"Docker Permission Errors","text":"<pre><code># On Linux, ensure user is in docker group\nsudo usermod -aG docker $USER\n# Then logout and login again\n</code></pre>"},{"location":"integration-testing/#k3s-container-startup-issues","title":"K3s Container Startup Issues","text":"<ul> <li>Ensure Docker has enough resources (4GB+ RAM recommended)</li> <li>Check Docker daemon is running: <code>docker info</code></li> <li>Verify no conflicting containers: <code>docker ps -a</code></li> </ul>"},{"location":"integration-testing/#helm-chart-packaging-failures","title":"Helm Chart Packaging Failures","text":"<ul> <li>Ensure Helm is installed: <code>helm version</code></li> <li>Check chart syntax: <code>helm lint charts/locust-k8s-operator</code></li> <li>Verify chart dependencies: <code>helm dependency list charts/locust-k8s-operator</code></li> </ul>"},{"location":"integration-testing/#integration-test-timeouts","title":"Integration Test Timeouts","text":"<ul> <li>Tests have generous timeouts but may need adjustment for slower systems</li> <li>Modify timeouts in <code>application-test.yml</code> if needed</li> <li>Check system resources during test execution</li> </ul>"},{"location":"integration-testing/#debug-mode","title":"Debug Mode","text":"<p>Enable debug logging by setting: <pre><code>logger:\n  levels:\n    com.locust: DEBUG\n    org.testcontainers: DEBUG\n</code></pre></p>"},{"location":"integration-testing/#manual-debugging","title":"Manual Debugging","text":"<p>If tests fail, you can manually inspect the K3s cluster: 1. The test creates temporary kubeconfig files 2. Look for log messages indicating kubeconfig location 3. Use <code>kubectl</code> with the temporary kubeconfig to inspect cluster state</p>"},{"location":"integration-testing/#performance-considerations","title":"Performance Considerations","text":""},{"location":"integration-testing/#resource-requirements","title":"Resource Requirements","text":"<ul> <li>Memory: ~4GB available RAM recommended</li> <li>CPU: 2+ cores for reasonable performance</li> <li>Disk: ~10GB for images and temporary files</li> <li>Network: Internet access for pulling images</li> </ul>"},{"location":"integration-testing/#execution-time","title":"Execution Time","text":"<ul> <li>Full test suite: ~10-15 minutes</li> <li>Individual test phases:</li> <li>Cluster startup: ~2-3 minutes</li> <li>Image building: ~3-5 minutes</li> <li>Deployment validation: ~2-3 minutes</li> <li>Test execution: ~2-3 minutes</li> <li>Cleanup: ~1-2 minutes</li> </ul>"},{"location":"integration-testing/#future-enhancements","title":"Future Enhancements","text":""},{"location":"integration-testing/#planned-improvements","title":"Planned Improvements","text":"<ul> <li> Multi-scenario testing (different LocustTest configurations)</li> <li> Performance benchmarking integration</li> <li> Integration with libConfigMap feature testing</li> <li> Cross-platform testing (ARM64 support)</li> <li> Parallel test execution for faster CI</li> </ul>"},{"location":"integration-testing/#extension-points","title":"Extension Points","text":"<ul> <li>Add custom test scenarios in separate test classes</li> <li>Extend with custom Kubernetes resources validation</li> <li>Integrate with monitoring and observability testing</li> <li>Add chaos engineering tests for resilience validation</li> </ul>"},{"location":"integration-testing/#related-documentation","title":"Related Documentation","text":"<ul> <li>How It Works - Operator architecture overview</li> <li>Contributing - Development guidelines</li> <li>LibConfigMap Feature - Feature implementation details</li> </ul>"},{"location":"license/","title":"License","text":"<p>Open source licensed under Apache-2.0 license (see LICENSE file for details).</p>"},{"location":"local-development/","title":"Local Development Guide","text":"<p>This guide describes the setup and workflow for local development on the Locust K8s Operator project. It's intended for developers who want to contribute code changes.</p>"},{"location":"local-development/#development-setup","title":"Development Setup","text":"Initial Setup  1. Clone the repository:    <pre><code>git clone https://github.com/AbdelrhmanHamouda/locust-k8s-operator.git\ncd locust-k8s-operator\n</code></pre>  2. Install [pre-commit](https://pre-commit.com/) and set up the git hooks:    <pre><code>pre-commit install --install-hooks\npre-commit install --hook-type commit-msg\n</code></pre>"},{"location":"local-development/#development-guidelines","title":"Development Guidelines","text":"<ul> <li> <p>This project follows the Conventional Commits standard to automate Semantic Versioning and Keep A Changelog with Commitizen.</p> </li> <li> <p>All code should include appropriate tests. See the integration testing guide for details on the integration test setup.</p> </li> </ul>"},{"location":"local-development/#local-testing-with-minikube-and-helm","title":"Local Testing with Minikube and Helm","text":"<p>For local development and testing, you can use Minikube to create a local Kubernetes cluster. This allows you to test the operator and your changes in an environment that closely resembles a production setup.</p>"},{"location":"local-development/#prerequisites","title":"Prerequisites","text":"<ul> <li>Minikube</li> <li>Helm</li> </ul>"},{"location":"local-development/#steps","title":"Steps","text":"<ol> <li>Start Minikube</li> </ol> <p>Start a local Kubernetes cluster using Minikube:</p> <pre><code>minikube start\n</code></pre> <ol> <li>Build and Load the Docker Image</li> </ol> <p>If you've made changes to the operator's source code, you'll need to build a new Docker image and load it into your Minikube cluster. This project uses the Jib Gradle plugin to build images directly, so you don't need a <code>Dockerfile</code>.</p> <p>First, build the image to your local Docker daemon:    <pre><code>./gradlew jibDockerBuild\n</code></pre></p> <p>Next, load the image into Minikube's internal registry:    <pre><code>minikube image load locust-k8s-operator:latest\n</code></pre></p> <ol> <li>Package the Helm Chart</li> </ol> <p>Package the Helm chart to create a distributable <code>.tgz</code> file.</p> <pre><code>helm package ./charts/locust-k8s-operator\n</code></pre> <ol> <li>Install the Operator with Helm</li> </ol> <p>Install the Helm chart on your Minikube cluster. The command below overrides the default image settings to use the one you just built and loaded.</p> <p>You can use a <code>values.yaml</code> file to override other settings.</p> <pre><code># values.yaml (optional)\n# Example: Set resource requests and limits for the operator pod\nconfig:\n  loadGenerationPods:\n    resource:\n      cpuRequest: 250m\n      memRequest: 128Mi\n      ephemeralRequest: 300M\n      cpuLimit: 1000m\n      memLimit: 1024Mi\n      ephemeralLimit: 50M\n\n# To leave a resource unbound, Leave the limit empty\n# This is useful when you don't want to set a specific limit.\n# example:\n# config:\n#   loadGenerationPods:\n#     resource:\n#       cpuLimit: \"\"\n#       memLimit: \"\"\n#       ephemeralLimit: \"\"\n</code></pre> <p>Install the chart using the following command. The <code>-f values.yaml</code> flag is optional.</p> <pre><code>helm install locust-operator locust-k8s-operator-*.tgz -f values.yaml \\\n  --set image.repository=locust-k8s-operator \\\n  --set image.tag=latest \\\n  --set image.pullPolicy=IfNotPresent\n</code></pre> <p>This will deploy the operator to your Minikube cluster using the settings defined in your <code>values.yaml</code> file.</p>"},{"location":"local-development/#writing-documentation","title":"Writing Documentation","text":"<p>All documentation is located under the <code>docs/</code> directory. The documentation is hosted on GitHub Pages and updated automatically with each release. To manage and build the documentation, the project uses MkDocs &amp; Material for MkDocs framework.</p> <p>During development, the CI workflow will build the documentation as part of the validation.</p>"},{"location":"metrics_and_dashboards/","title":"Metrics &amp; Dashboards","text":"<p>The Locust Kubernetes Operator is designed with observability in mind, providing out-of-the-box support for Prometheus metrics. This allows you to gain deep insights into your performance tests and the operator's behavior.</p>"},{"location":"metrics_and_dashboards/#prometheus-metrics-exporter","title":"Prometheus Metrics Exporter","text":"<p>By default, the operator deploys a Prometheus metrics exporter alongside each Locust master and worker pod. This exporter collects detailed metrics from the Locust instances and exposes them in a format that Prometheus can scrape.</p>"},{"location":"metrics_and_dashboards/#key-metrics","title":"Key Metrics","text":"<p>Some of the key metrics you can monitor include:</p> <ul> <li><code>locust_requests_total</code>: The total number of requests made.</li> <li><code>locust_requests_failed_total</code>: The total number of failed requests.</li> <li><code>locust_response_time_seconds</code>: The response time of requests.</li> <li><code>locust_users</code>: The number of simulated users.</li> </ul>"},{"location":"metrics_and_dashboards/#configuration","title":"Configuration","text":"<p>To enable Prometheus to scrape these metrics, you'll need to configure a scrape job in your <code>prometheus.yml</code> file. Here's an example configuration:</p> <pre><code>scrape_configs:\n  - job_name: 'locust'\n    kubernetes_sd_configs:\n      - role: pod\n    relabel_configs:\n      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]\n        action: keep\n        regex: true\n      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]\n        action: replace\n        target_label: __metrics_path__\n        regex: (.+)\n      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]\n        action: replace\n        regex: ([^:]+)(?::\\d+)?;(\\d+)\n        replacement: $1:$2\n        target_label: __address__\n      - action: labelmap\n        regex: __meta_kubernetes_pod_label_(.+)\n</code></pre>"},{"location":"metrics_and_dashboards/#grafana-dashboards","title":"Grafana Dashboards","text":"<p>Once you have your metrics flowing into Prometheus, you can create powerful and informative dashboards in Grafana to visualize your test results. You can build panels to track key performance indicators (KPIs) such as response times, request rates, and error rates.</p> <p>There are also community-built Grafana dashboards available for Locust that you can adapt for your needs.</p>"},{"location":"metrics_and_dashboards/#operator-metrics","title":"Operator Metrics","text":"<p>In addition to the Locust-specific metrics, the operator itself exposes a set of metrics through Micronaut's metrics module. These metrics provide insights into the operator's health and performance, including JVM metrics, uptime, and more. You can find these metrics by scraping the operator's pod on the <code>/health</code> endpoint.</p>"},{"location":"pull-request-process/","title":"Pull Request Process","text":"<p>This document outlines the process for submitting pull requests to the Locust K8s Operator project. Following these guidelines helps maintain code quality and ensures a smooth review process.</p>"},{"location":"pull-request-process/#before-creating-a-pull-request","title":"Before Creating a Pull Request","text":"<ol> <li> <p>Discuss Changes First: Before making significant changes, please discuss the proposed changes via an issue in the GitHub repository.</p> </li> <li> <p>Follow Coding Conventions: Ensure your code follows the project's coding standards and conventions.</p> </li> <li> <p>Write Tests: All new features or bug fixes should be covered by appropriate tests. See the integration testing guide for details on integration testing.</p> </li> </ol>"},{"location":"pull-request-process/#pull-request-workflow","title":"Pull Request Workflow","text":"<ol> <li> <p>Fork and Clone: Fork the repository and clone it locally.</p> </li> <li> <p>Create a Branch: Create a branch for your changes using a descriptive name:    <pre><code>git checkout -b feature/your-feature-name\n</code></pre></p> </li> <li> <p>Make Your Changes: Implement your changes, following the project's coding standards.</p> </li> <li> <p>Commit Your Changes: Use the Conventional Commits standard for commit messages. This is important as the commit messages directly influence the content of the CHANGELOG.md and version increments.</p> </li> </ol> <p>Examples of good commit messages:    <pre><code>feat: add support for Locust worker autoscaling\nfix: correct container resource allocation\ndocs: update installation instructions\n</code></pre></p> <ol> <li> <p>Run Tests Locally: Run both unit and integration tests to ensure your changes don't break existing functionality:    <pre><code># Run unit tests\n./gradlew test\n\n# Run integration tests\n./scripts/run-integration-test.sh\n</code></pre></p> </li> <li> <p>Submit Your Pull Request: Push your branch to your fork and submit a pull request to the main repository.</p> </li> </ol>"},{"location":"pull-request-process/#pull-request-requirements","title":"Pull Request Requirements","text":"<ol> <li> <p>Clean Build Dependencies: Ensure any install or build dependencies are removed before the final build.</p> </li> <li> <p>Documentation: Update the documentation with details of changes to interfaces, configuration options, or other important aspects.</p> </li> <li> <p>Commit Messages: Ensure commit messages follow the Conventional Commits standard. This is critical for automated changelog generation and semantic versioning.</p> </li> <li> <p>Tests:</p> </li> <li>Write clean and well-structured tests.</li> <li>Ensure your changes don't cause regressions.</li> <li>All changes (within reason) should be covered by tests.</li> <li>Update existing tests if your changes represent breaking changes.</li> </ol>"},{"location":"pull-request-process/#review-process","title":"Review Process","text":"<ol> <li> <p>Initial Review: Maintainers will review your PR to ensure it meets the project's requirements.</p> </li> <li> <p>CI Checks: The CI system will run tests and other checks against your PR. Make sure these pass.</p> </li> <li> <p>Feedback: Maintainers may request changes or improvements to your PR.</p> </li> <li> <p>Merge: Once approved, a maintainer will merge your PR.</p> </li> </ol>"},{"location":"pull-request-process/#after-your-pr-is-merged","title":"After Your PR is Merged","text":"<ol> <li> <p>Update Your Fork: Keep your fork up to date with the main repository.</p> </li> <li> <p>Celebrate: Thank you for contributing to the Locust K8s Operator project! Your efforts help make the project better for everyone.</p> </li> </ol> <p>Remember that this is a collaborative open-source project. Constructive feedback and discussions are welcome, and all interactions should adhere to the project's Code of Conduct.</p>"},{"location":"roadmap/","title":"Roadmap","text":"<p>The following is a list of planned features and improvements for the Locust Kubernetes Operator. This list is not exhaustive and may change over time.</p> <ul> <li> <p> Enhanced Observability: Provide out-of-the-box Grafana dashboard examples and more detailed Prometheus configuration guides to make monitoring even easier.</p> </li> <li> <p> Event-Driven Actions: Integrate with notification systems like Microsoft Teams or Slack to send alerts on test completion, failure, or other significant events.</p> </li> <li> <p> Advanced Benchmarking: Investigate the feasibility of incorporating external metrics into test results. This would allow for more sophisticated pass/fail criteria, such as assessing the performance of a Kafka-based service by its consumer lag.</p> </li> <li> <p> Dynamic Updates: Add support for updating a <code>LocustTest</code> custom resource while a test is running. This would allow for dynamically adjusting test parameters without restarting the test.</p> </li> <li> <p> Web UI/Dashboard: Explore the possibility of creating a simple web UI or dashboard for managing and monitoring tests directly through the operator.</p> </li> </ul>"}]}