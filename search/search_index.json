{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Locust Kubernetes Operator","text":"<p>Enable performance testing for the modern era!</p> <p>Utilize the full power of Locust in the cloud.</p> <p> </p>"},{"location":"#at-a-glance","title":"At a glance","text":"<p>The Operator is designed to unlock seamless &amp; effortless distributed performance testing in the cloud and enable continues integration for CI / CD. By design, the entire system is cloud native and focuses on automation and CI practices. One strong feature about the system is its ability to horizontally scale to meet any required performance demands.</p>"},{"location":"#what-does-it-offer","title":"What does it offer","text":"<p>Fundamentally, the Operator provide the following as part of its core offerings; cloud native, automation &amp; CI, governance, Observability.</p> <p>Distributed cloud performance testing: Locust is a great and very powerful load testing tool. It is capable of generating a significant amount of load specially when configured correctly. That being said, there is only so much a single instance and vertical scaling can do. Luckily, Locust has a native out of the box support for distributed mode. This Locust Kubernetes Operator project leverage this feature and adds systems and functionalities to address challenges and situations that are exclusive to the cloud context.</p> <p>Low barrier of entry: Utilizing the power of the Operator lowers significantly the barrier of entry to run in the cloud. From an end-user perspective, running a performance test in the cloud becomes a single command operation.</p> <p>Test isolation and Parallel tests: By default, the Operator is able to support any number of Parallel test executions with an absolute guarantee that each test is fully protected from being polluted by the existence of any number of other tests.</p> <p>Automation &amp; CI: By having automation as a core focus point, teams and organizations can build performance testing directly into CI/CD pipelines. Meaning that every new service, feature or system can be potentially tested and validated for performance SLOs / SLAs.</p> <p>Separation of concerns: By using the Operator, engineering teams can focus on building a robust performance test/s and SREs DevOps teams can focus on managing the resources.</p> <p>Governance: Enable organizations to have governance over what / how resources are deployed and run on the cloud.</p> <p>Cloud cost optimization: Using the Operator enables for a more effective control over the cloud cost. Since resources are only deployed when needed and only for as long as needed, the cost of performance testing is kept to a minimum.</p> <p>Observability: For both engineering teams and cloud admins, the Operator unlocks the ability to build observability &amp; monitoring dashboards in order to analyse test results during test runtime or retroactively (interesting for teams) and infrastructure usage and resource monitoring ( interesting for cloud admins, SREs, etc...).</p> <p></p>"},{"location":"#whom-is-it-for","title":"Whom is it for","text":"<p>It is built for...</p> <p></p>"},{"location":"#where-can-it-run","title":"Where can it run","text":"<p>Due to its design, the Operator can be deployed on any kubernetes cluster. Meaning that it is possible to have a full cloud native performance testing system anywhere in a matter of seconds.</p>"},{"location":"#limits","title":"Limits","text":"<p>The only real limit to this approach is the amount of cluster resources a given team or an organization is willing to dedicate to performance testing.</p>"},{"location":"advanced_topics/","title":"Advanced topics","text":"<p>Basic configuration is not always enough to satisfy the performance test needs, for example when needing to work with Kafka and MSK. Below is a collection of topics of an advanced nature. This list will be keep growing as the tool matures more and more.</p>"},{"location":"advanced_topics/#kafka-aws-msk-configuration","title":"Kafka &amp; AWS MSK configuration","text":"<p>Generally speaking, the usage of Kafka in a locustfile is identical to how it would be used anywhere else within the cloud context. Thus, no special setup is needed for the purposes of performance testing with the Operator. That being said, if an organization is using kafka in production, chances are that authenticated kafka is being used. One of the main providers of such managed service is AWS in the form of MSK. For that end, the Operator have an out-of-the-box support for MSK.</p> <p>To enable performance testing with MSK, a central/global Kafka user can be created by the \"cloud admin\" or \"the team\" responsible for the Operator deployment within the organization. The Operator can then be easily configured to inject the configuration of that user as environment variables in all generated resources. Those variables can be used by the test to establish authentication with the kafka broker.</p> Variable Name Description <code>KAFKA_BOOTSTRAP_SERVERS</code> Kafka bootstrap servers <code>KAFKA_SECURITY_ENABLED</code> - <code>KAFKA_SECURITY_PROTOCOL_CONFIG</code> Security protocol. Options: <code>PLAINTEXT</code>, <code>SASL_PLAINTEXT</code>, <code>SASL_SSL</code>, <code>SSL</code> <code>KAFKA_SASL_MECHANISM</code> Authentication mechanism. Options: <code>PLAINTEXT</code>, <code>SCRAM-SHA-256</code>, <code>SCRAM-SHA-512</code> <code>KAFKA_USERNAME</code> The username used to authenticate Kafka clients with the Kafka server <code>KAFKA_PASSWORD</code> The password used to authenticate Kafka clients with the Kafka server"},{"location":"advanced_topics/#dedicated-kubernetes-nodes","title":"Dedicated Kubernetes Nodes","text":"<p>To run test resources on dedicated Kubernetes node(s), the Operator support deploying resources with Affinity and Taint Tolerations.</p>"},{"location":"advanced_topics/#affinity","title":"Affinity","text":"<p>This allows generated resources to have specific Affinity options.</p> <p>Note</p> <p>The Custom Resource Definition Spec is designed with modularity and expandability in mind. This means that although a specific set of Kubernetes Affinity options are supported today, extending this support based on need is a streamlined and easy processes. If additonal support is needed, don't hesitate to open a feature request.</p>"},{"location":"advanced_topics/#affinity-options","title":"Affinity Options","text":"<p>The specification for affinity is defined as follows</p> <code>affinity-spec.yaml</code> <pre><code>apiVersion: locust.io/v1\n...\nspec:\n...\naffinity:\n    nodeAffinity:\n        requiredDuringSchedulingIgnoredDuringExecution\n            &lt;label-key&gt;: &lt;label-value&gt;\n            ...\n...\n</code></pre>"},{"location":"advanced_topics/#node-affinity","title":"Node Affinity","text":"<p>This optional section causes generated pods to declare specific Node Affinity so Kubernetes scheduler becomes aware of this requirement.</p> <p>The implementation from the Custom Resource perspective is strongly influenced by Kubernetes native definition of node affinity. However, the implementation is on purpose slightly simplified in order to allow users to have easier time working with affinity.</p> <p>The <code>nodeAffinity</code> section supports declaring node affinity under <code>requiredDuringSchedulingIgnoredDuringExecution</code>. Meaning that any declared affinity labels must be present in nodes in order for resources to be deployed on them.</p> <p>Example:</p> <p>In the below example, generated pods will declare 3 required labels (keys and values) to be present on nodes before they are scheduled.</p> <code>node-affinity-example.yaml</code> <pre><code>apiVersion: locust.io/v1\n...\nspec:\n    ...\n    affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeAffinityLabel1: locust-cloud-tests\n            nodeAffinityLabel2: performance-nodes\n            nodeAffinityLabel3: high-memory\n            ...\n    ...\n</code></pre>"},{"location":"advanced_topics/#taint-tolerations","title":"Taint Tolerations","text":"<p>This optional sections allows deployed pods to have specific taint(s) tolerations. The features is also modeled to follow closely Kubernetes native definition.</p>"},{"location":"advanced_topics/#spec-breakdown-example","title":"Spec breakdown &amp; example","text":"<code>taint-tolerations-spec.yaml</code> <code>taint-tolerations-example.yaml</code> <pre><code>apiVersion: locust.io/v1\n...\nspec:\n...\ntolerations:\n    - key: &lt;string value&gt;\n      operator: &lt;\"Exists\", \"Equal\"&gt;\n      value: &lt;string value&gt; #(1)!\n      effect: &lt;\"NoSchedule\", \"PreferNoSchedule\", \"NoExecute\"&gt;\n    ...\n</code></pre> <ol> <li>Optional when <code>operator</code> value is set to <code>Exists</code>.</li> </ol> <pre><code>apiVersion: locust.io/v1\n...\nspec:\n    ...\n    tolerations:\n        - key: taint-A\n          operator: Equal\n          value: ssd\n          effect: NoSchedule\n\n        - key: taint-B\n          operator: Exists\n          effect: NoExecute\n        ...\n    ...\n</code></pre>"},{"location":"advanced_topics/#usage-of-a-private-image-registry","title":"Usage of a private image registry","text":"<p>Images from a private image registry can be used through various methods as described in the kubernetes documentation, one of those methods depends on setting <code>imagePullSecrets</code> for pods. This is supported in the operator by simply setting the <code>imagePullSecrets</code> option in the deployed custom resource. For example:</p> locusttest-pull-secret-cr.yaml<pre><code>apiVersion: locust.io/v1\n...\nspec:\n  image: ghcr.io/mycompany/locust:latest #(1)!\n  imagePullSecrets: #(2)!\n    - gcr-secret\n  ...\n</code></pre> <ol> <li>Specify which Locust image to use for both master and worker containers.</li> <li>[Optional] Specify an existing pull secret to use for master and worker pods.</li> </ol>"},{"location":"advanced_topics/#image-pull-policy","title":"Image pull policy","text":"<p>Kubernetes uses the image tag and pull policy to control when kubelet attempts to download (pull) a container image. The image pull policy can be defined through the <code>imagePullPolicy</code> option, as explained in the kubernetes documentation. When using the operator, the <code>imagePullPolicy</code> option can be directly configured in the custom resource. For example:</p> locusttest-pull-policy-cr.yaml<pre><code>apiVersion: locust.io/v1\n...\nspec:\n  image: ghcr.io/mycompany/locust:latest #(1)!\n  imagePullPolicy: Always #(2)!\n  ...\n</code></pre> <ol> <li>Specify which Locust image to use for both master and worker containers.</li> <li>[Optional] Specify the pull policy to use for containers defined within master and worker containers. Supported options include <code>Always</code>, <code>IfNotPresent</code> and <code>Never</code>.</li> </ol>"},{"location":"advanced_topics/#automatic-cleanup-for-finished-master-and-worker-jobs","title":"Automatic Cleanup for Finished Master and Worker Jobs","text":"<p>Once load tests finish, master and worker jobs remain available in Kubernetes. You can set up a time-to-live (TTL) value in the operator's Helm chart, so that kubernetes jobs are eligible for cascading removal once the TTL expires. This means that Master and Worker jobs and their dependent objects (e.g., pods) will be deleted.</p> <p>Note that setting up a TTL will not delete <code>LocustTest</code> or <code>ConfigMap</code> resources.</p> <p>To set a TTL value, override the key <code>ttlSecondsAfterFinished</code> in <code>values.yaml</code>:</p> <code>values.yaml</code> <pre><code>...\nconfig:\n  loadGenerationJobs:\n    # Either leave empty or use an empty string to avoid setting this option\n    ttlSecondsAfterFinished: 3600\n...\n</code></pre> <p>You can also use Helm's CLI arguments: <code>helm install ... --set config.loadGenerationJobs.ttlSecondsAfterFinished=0</code>.</p> <p>Read more about the <code>ttlSecondsAfterFinished</code> parameter in Kubernetes's official documentation.</p>"},{"location":"advanced_topics/#kubernetes-support-for-ttlsecondsafterfinished","title":"Kubernetes Support for <code>ttlSecondsAfterFinished</code>","text":"<p>Support for parameter <code>ttlSecondsAfterFinished</code> was added in Kubernetes v1.12. In case you're deploying the locust operator to a Kubernetes cluster that does not support <code>ttlSecondsAfterFinished</code>, you may leave the Helm key empty or use an empty string. In this case, job definitions will not include the parameter.</p>"},{"location":"contribute/","title":"Contribute","text":"<p>There's plenty to do, come say hi in the issues! \ud83d\udc4b</p> <p>Also check out the CONTRIBUTING.MD \ud83e\udd13</p>"},{"location":"contribute/#project-status","title":"project status","text":"<p>The project is actively maintained and is under continues development and improvement. If you have any request or want to chat, kindly open a ticket. If you wish to contribute code and / or ideas, kindly check the contribution section.</p>"},{"location":"features/","title":"Features","text":"<p>Full list Coming soon!</p>"},{"location":"getting_started/","title":"Getting started","text":"<p>Only few simple steps are needed to get a test up and running in the cluster. The following is a step-by-step guide on how to achieve this.</p>"},{"location":"getting_started/#step-1-write-a-valid-locust-test-script","title":"Step 1: Write a valid Locust test script","text":"<p>For this example, we will be using the following script</p> demo_test.py<pre><code>from locust import HttpUser, task\n\nclass User(HttpUser): # (1)!\n    @task #(2)!\n    def get_employees(self) -&gt; None:\n        \"\"\"Get a list of employees.\"\"\"\n        self.client.get(\"/api/v1/employees\") #(3)!\n</code></pre> <ol> <li>Class representing <code>users</code> that will be simulated by Locust.</li> <li>One or more <code>task</code> that each simulated <code>user</code> will be performing.</li> <li>HTTP call to a specific endpoint.</li> </ol> <p>Note</p> <p>To be able to run performance tests effectivly, an understanding of Locust which is the underline load generation tool is required. All tests must be valid locust tests.</p> <p>Locust provide a very good and detail rich documentation that can be found here.</p>"},{"location":"getting_started/#step-2-write-a-valid-custom-resource-for-locusttest-crd","title":"Step 2: Write a valid  custom resource for LocustTest CRD","text":"<p>A simple custom resource for the previous test can be something like the following example;</p> <p>To streamline this step, intensive-brew should be used. It is a simple cli tool that converts a declarative yaml into a compatible LocustTest kubernetes custom resource.</p> locusttest-cr.yaml<pre><code>apiVersion: locust.io/v1 #(1)!\nkind: LocustTest #(2)!\nmetadata:\n  name: demo.test #(3)!\nspec:\n  image: locustio/locust:latest #(4)!\n  masterCommandSeed: #(5)!\n    --locustfile /lotest/src/demo_test.py\n    --host https://dummy.restapiexample.com\n    --users 100\n    --spawn-rate 3\n    --run-time 3m\n  workerCommandSeed: --locustfile /lotest/src/demo_test.py #(6)!\n  workerReplicas: 3 #(7)!\n  configMap: demo-test-map #(8)!\n</code></pre> <ol> <li>API version based on the deployed LocustTest CRD.</li> <li>Resource kind.</li> <li>The name field used by the operator to infer the names of test generated resources. While this value is insignificant to the Operator    itself, it is important to keep a good convention here since it helps in tracking resources across the cluster when needed.</li> <li>Image to use for the load generation pods</li> <li>Seed command for the master node. The Operator will append to this seed command/s all operational parameters needed for the master    to perform its job e.g. ports, rebalancing settings, timeouts, etc...</li> <li>Seed command for the worker node. The Operator will append to this seed command/s all operational parameters needed for the worker    to perform its job e.g. ports, master node url, master node ports, etc...</li> <li>The amount of worker nodes to spawn in the cluster.</li> <li>[Optional] Name of configMap to mount into the pod</li> </ol>"},{"location":"getting_started/#other-options","title":"Other options","text":""},{"location":"getting_started/#labels-and-annotations","title":"Labels and annotations","text":"<p>You can add labels and annotations to generated Pods. For example:</p> locusttest-cr.yaml<pre><code>apiVersion: locust.io/v1\n...\nspec:\n  image: locustio/locust:latest\n  labels: #(1)!\n    master:\n      locust.io/role: \"master\"\n      myapp.com/testId: \"abc-123\"\n      myapp.com/tenantId: \"xyz-789\"\n    worker:\n      locust.io/role: \"worker\"\n  annotations: #(2)!\n    master:\n      myapp.com/threads: \"1000\"\n      myapp.com/version: \"2.1.0\"\n    worker:\n      myapp.com/version: \"2.1.0\"\n  ...\n</code></pre> <ol> <li>[Optional] Labels are attached to both master and worker pods. They can later be used to identify pods belonging to a particular execution context. This is useful, for example, when tests are deployed programmatically. A launcher application can query the Kubernetes API for specific resources.</li> <li>[Optional] Annotations too are attached to master and worker pods. They can be used to include additional context about a test. For example, configuration parameters of the software system being tested.</li> </ol> <p>Both labels and annotations can be added to the Prometheus configuration, so that metrics are associated with the appropriate information, such as the test and tenant ids. You can read more about this in the Prometheus documentation site.</p>"},{"location":"getting_started/#step-3-deploy-locust-k8s-operator-in-the-cluster","title":"Step 3: Deploy Locust k8s Operator in the cluster.","text":"<p>The recommended way to install the Operator is by using the official HELM chart. Documentation on how to perform that is available here.</p>"},{"location":"getting_started/#step-4-deploy-test-as-a-configmap","title":"Step 4: Deploy test as a configMap","text":"<p>For the purposes of this example, the <code>demo_test.py</code> test previously demonstrated will be deployed into the cluster as a configMap that the Operator will mount to the load generation pods. To deploy the test as a configMap, run the bellow command following this template <code>kubectl create configmap &lt;configMap-name&gt; --from-file &lt;your_test.py&gt;</code>:</p> <ul> <li><code>kubectl create configmap demo-test-map --from-file demo_test.py</code></li> </ul> <p>Fresh cluster resources</p> <p>Fresh cluster resources are allocated for each running test, meaning that tests DO NOT have any cross impact on each other.</p>"},{"location":"getting_started/#step-5-start-the-test-by-deploying-the-locusttest-custom-resource","title":"Step 5: Start the test by deploying the LocustTest custom resource.","text":"<p>Deploying a custom resource, signals to the Operator the desire to start a test and thus the Operator starts creating and scheduling all needed resources. To do that, deploy the custom resource following this template <code>kubectl apply -f &lt;valid_cr&gt;.yaml</code>:</p> <ul> <li><code>kubectl apply -f locusttest-cr.yaml</code></li> </ul>"},{"location":"getting_started/#step-51-check-cluster-for-running-resources","title":"Step 5.1: Check cluster for running resources","text":"<p>At this point, it is possible to check the cluster and all required resources will be running based on the passed configuration in the custom resource.</p> <p>The Operator will create the following resources in the cluster for each valid custom resource:</p> <ul> <li>A kubernetes service for the master node so it is reachable by other worker nodes.</li> <li>A kubernetes Job to manage the master node.</li> <li>A kubernetes Job to manage the worker node.</li> </ul>"},{"location":"getting_started/#step-6-clear-resources-after-test-run","title":"Step 6: Clear resources after test run","text":"<p>In order to remove the cluster resources after a test run, simply remove the custom resource and the Operator will react to this event by cleaning the cluster of all related resources. To delete a resource, run the below command following this template <code>kubectl delete -f &lt;valid_cr&gt;.yaml</code>:</p> <ul> <li><code>kubectl delete -f locusttest-cr.yaml</code></li> </ul>"},{"location":"helm_deploy/","title":"HELM deployment","text":"<p>In order to deploy using helm, follow the below steps:</p> <ol> <li> <p>Add the Operator\u00b4s HELM repo</p> <ul> <li><code>helm repo add locust-k8s-operator https://abdelrhmanhamouda.github.io/locust-k8s-operator/</code></li> </ul> <p>Note</p> <p>If the repo has been added before, run <code>helm repo update</code> in order to pull the latest available release!</p> </li> <li> <p>Install the Operator</p> <ul> <li><code>helm install locust-operator locust-k8s-operator/locust-k8s-operator</code> - The Operator will ready up in around 40-60 seconds</li> <li>This will cause the bellow resources to be deployed in the currently active k8s context &amp; namespace.</li> <li>crd-locusttest.yaml<ul> <li>This CRD is the first part of the Operator pattern. It is needed in order to enable Kubernetes to understand the LocustTest   custom resource and allow its deployment.</li> </ul> </li> <li>serviceaccount-and-roles.yaml<ul> <li>ServiceAccount and Role bindings that enable the Controller to have the needed privilege inside the cluster to watch and   manage the related resources.</li> </ul> </li> <li>deployment.yaml<ul> <li>The Controller responsible for managing and reacting to the cluster resources.</li> </ul> </li> </ul> </li> </ol>"},{"location":"how_does_it_work/","title":"How does it work","text":"<p>To run a performance test, basic configuration is provided through a simple and intuitive kubernetes custom resource. Once deployed the Operator does all the heavy work of creating and scheduling the resources while making sure that all created load generation pods can effectively communicate with each other.</p> <p>To handle the challenge of delivering test script/s from local environment to the cluster and in turn to the deployed locust pods, the Operator support dynamic volume mounting from a configMaps source. This is indicated by a simple optional configuration. Meaning, if the configuration is present, the volume is mounted, and if it is not, no volume is mounted.</p> <p>Since a \"Picture Is Worth a Thousand Words\", here is a gif! </p>"},{"location":"how_does_it_work/#steps-performed-in-demo","title":"Steps performed in demo","text":"<ul> <li>Test configmap created in cluster.</li> <li>LocustTest CR deployed into the cluster.</li> <li>The Operator creating, configuring and scheduling test resources on CR creation event.</li> <li>The Operator cleaning up test resources after test CR has been removed event.</li> </ul>"},{"location":"license/","title":"License","text":"<p>Open source licensed under Apache-2.0 license (see LICENSE file for details).</p>"},{"location":"metrics_and_dashboards/","title":"Metrics &amp; Dashboards","text":"<p>Full instructions Coming soon!</p>"},{"location":"roadmap/","title":"Roadmap","text":"<p>Not in a particular order (list wis updated when features are implemented / planned):</p> <ul> <li>Add traceability labels to generated resources</li> <li>\u2705 Support for deploying test resources with node affinity / node taints</li> <li>Dashboard examples (Grafana + prometheus configuration)</li> <li>Enable event driven actions<ul> <li>Integration with MSTeams: Push notification on test run completion / termination events</li> </ul> </li> <li>UNDER_INVESTIGATION Benchmarking and collection of non-test generated metrics<ul> <li>Investigation is on going to study the feasibility of supplying Locust pods with external metrics that are collected from service system under-test. Test pods can then use this information to assess pass / fail criteria. This is especially useful in non-REST based services e.g. assess kafka(streams) microservice based on its consumer lag performance coming from the kafka broker.</li> </ul> </li> </ul>"}]}