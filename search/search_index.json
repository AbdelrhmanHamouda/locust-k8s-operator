{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Locust Kubernetes Operator \u2693 Enable performance testing for the modern era! Utilize the full power of Locust in the cloud. At a glance \u2693 The Operator is designed to unlock seamless & effortless distributed performance testing in the cloud and enable continues integration for CI / CD . By design, the entire system is cloud native and focuses on automation and CI practices. One strong feature about the system is its ability to horizontally scale to meet any required performance demands. What does it offer \u2693 Fundamentally, the Operator provide the following as part of its core offerings; cloud native , automation & CI , governance , Observability . Distributed cloud performance testing : Locust is a great and very powerful load testing tool. It is capable of generating a significant amount of load specially when configured correctly. That being said, there is only so much a single instance and vertical scaling can do. Luckily, Locust has a native out of the box support for distributed mode. This Locust Kubernetes Operator project leverage this feature and adds systems and functionalities to address challenges and situations that are exclusive to the cloud context. Low barrier of entry : Utilizing the power of the Operator lowers significantly the barrier of entry to run in the cloud. From an end-user perspective, running a performance test in the cloud becomes a single command operation. Test isolation and Parallel tests : By default, the Operator is able to support any number of Parallel test executions with an absolute guarantee that each test is fully protected from being polluted by the existence of any number of other tests. Automation & CI : By having automation as a core focus point, teams and organizations can build performance testing directly into CI/CD pipelines. Meaning that every new service, feature or system can be potentially tested and validated for performance SLOs / SLAs. Separation of concerns : By using the Operator , engineering teams can focus on building a robust performance test/s and SREs DevOps teams can focus on managing the resources. Governance : Enable organizations to have governance over what / how resources are deployed and run on the cloud. Cloud cost optimization : Using the Operator enables for a more effective control over the cloud cost . Since resources are only deployed when needed and only for as long as needed, the cost of performance testing is kept to a minimum. Observability : For both engineering teams and cloud admins, the Operator unlocks the ability to build observability & monitoring dashboards in order to analyse test results during test runtime or retroactively (interesting for teams) and infrastructure usage and resource monitoring ( interesting for cloud admins, SREs, etc...). Whom is it for \u2693 It is built for... Where can it run \u2693 Due to its design, the Operator can be deployed on any kubernetes cluster. Meaning that it is possible to have a full cloud native performance testing system anywhere in a matter of seconds. Limits \u2693 The only real limit to this approach is the amount of cluster resources a given team or an organization is willing to dedicate to performance testing.","title":"Introduction"},{"location":"#locust-kubernetes-operator","text":"Enable performance testing for the modern era! Utilize the full power of Locust in the cloud.","title":"Locust Kubernetes Operator"},{"location":"#at-a-glance","text":"The Operator is designed to unlock seamless & effortless distributed performance testing in the cloud and enable continues integration for CI / CD . By design, the entire system is cloud native and focuses on automation and CI practices. One strong feature about the system is its ability to horizontally scale to meet any required performance demands.","title":"At a glance"},{"location":"#what-does-it-offer","text":"Fundamentally, the Operator provide the following as part of its core offerings; cloud native , automation & CI , governance , Observability . Distributed cloud performance testing : Locust is a great and very powerful load testing tool. It is capable of generating a significant amount of load specially when configured correctly. That being said, there is only so much a single instance and vertical scaling can do. Luckily, Locust has a native out of the box support for distributed mode. This Locust Kubernetes Operator project leverage this feature and adds systems and functionalities to address challenges and situations that are exclusive to the cloud context. Low barrier of entry : Utilizing the power of the Operator lowers significantly the barrier of entry to run in the cloud. From an end-user perspective, running a performance test in the cloud becomes a single command operation. Test isolation and Parallel tests : By default, the Operator is able to support any number of Parallel test executions with an absolute guarantee that each test is fully protected from being polluted by the existence of any number of other tests. Automation & CI : By having automation as a core focus point, teams and organizations can build performance testing directly into CI/CD pipelines. Meaning that every new service, feature or system can be potentially tested and validated for performance SLOs / SLAs. Separation of concerns : By using the Operator , engineering teams can focus on building a robust performance test/s and SREs DevOps teams can focus on managing the resources. Governance : Enable organizations to have governance over what / how resources are deployed and run on the cloud. Cloud cost optimization : Using the Operator enables for a more effective control over the cloud cost . Since resources are only deployed when needed and only for as long as needed, the cost of performance testing is kept to a minimum. Observability : For both engineering teams and cloud admins, the Operator unlocks the ability to build observability & monitoring dashboards in order to analyse test results during test runtime or retroactively (interesting for teams) and infrastructure usage and resource monitoring ( interesting for cloud admins, SREs, etc...).","title":"What does it offer"},{"location":"#whom-is-it-for","text":"It is built for...","title":"Whom is it for"},{"location":"#where-can-it-run","text":"Due to its design, the Operator can be deployed on any kubernetes cluster. Meaning that it is possible to have a full cloud native performance testing system anywhere in a matter of seconds.","title":"Where can it run"},{"location":"#limits","text":"The only real limit to this approach is the amount of cluster resources a given team or an organization is willing to dedicate to performance testing.","title":"Limits"},{"location":"advanced_topics/","text":"Advanced topics \u2693 Basic configuration is not always enough to satisfy the performance test needs, for example when needing to work with Kafka and MSK. Below is a collection of topics of an advanced nature. This list will be keep growing as the tool matures more and more. Kafka & AWS MSK configuration \u2693 Generally speaking, the usage of Kafka in a locustfile is identical to how it would be used anywhere else within the cloud context. Thus, no special setup is needed for the purposes of performance testing with the Operator . That being said, if an organization is using kafka in production, chances are that authenticated kafka is being used. One of the main providers of such managed service is AWS in the form of MSK . For that end, the Operator have an out-of-the-box support for MSK. To enable performance testing with MSK , a central/global Kafka user can be created by the \"cloud admin\" or \"the team\" responsible for the Operator deployment within the organization. The Operator can then be easily configured to inject the configuration of that user as environment variables in all generated resources. Those variables can be used by the test to establish authentication with the kafka broker. Variable Name Description KAFKA_BOOTSTRAP_SERVERS Kafka bootstrap servers KAFKA_SECURITY_ENABLED - KAFKA_SECURITY_PROTOCOL_CONFIG Security protocol. Options: PLAINTEXT , SASL_PLAINTEXT , SASL_SSL , SSL KAFKA_SASL_MECHANISM Authentication mechanism. Options: PLAINTEXT , SCRAM-SHA-256 , SCRAM-SHA-512 KAFKA_USERNAME The username used to authenticate Kafka clients with the Kafka server KAFKA_PASSWORD The password used to authenticate Kafka clients with the Kafka server","title":"Advanced topics"},{"location":"advanced_topics/#advanced-topics","text":"Basic configuration is not always enough to satisfy the performance test needs, for example when needing to work with Kafka and MSK. Below is a collection of topics of an advanced nature. This list will be keep growing as the tool matures more and more.","title":"Advanced topics"},{"location":"advanced_topics/#kafka-aws-msk-configuration","text":"Generally speaking, the usage of Kafka in a locustfile is identical to how it would be used anywhere else within the cloud context. Thus, no special setup is needed for the purposes of performance testing with the Operator . That being said, if an organization is using kafka in production, chances are that authenticated kafka is being used. One of the main providers of such managed service is AWS in the form of MSK . For that end, the Operator have an out-of-the-box support for MSK. To enable performance testing with MSK , a central/global Kafka user can be created by the \"cloud admin\" or \"the team\" responsible for the Operator deployment within the organization. The Operator can then be easily configured to inject the configuration of that user as environment variables in all generated resources. Those variables can be used by the test to establish authentication with the kafka broker. Variable Name Description KAFKA_BOOTSTRAP_SERVERS Kafka bootstrap servers KAFKA_SECURITY_ENABLED - KAFKA_SECURITY_PROTOCOL_CONFIG Security protocol. Options: PLAINTEXT , SASL_PLAINTEXT , SASL_SSL , SSL KAFKA_SASL_MECHANISM Authentication mechanism. Options: PLAINTEXT , SCRAM-SHA-256 , SCRAM-SHA-512 KAFKA_USERNAME The username used to authenticate Kafka clients with the Kafka server KAFKA_PASSWORD The password used to authenticate Kafka clients with the Kafka server","title":"Kafka &amp; AWS MSK configuration"},{"location":"contribute/","text":"Contribute \u2693 There's plenty to do, come say hi in the issues ! \ud83d\udc4b Also check out the CONTRIBUTING.MD \ud83e\udd13 project status \u2693 The project is actively maintained and is under continues development and improvement. If you have any request or want to chat, kindly open a ticket. If you wish to contribute code and / or ideas, kindly check the contribution section.","title":"Contribute"},{"location":"contribute/#contribute","text":"There's plenty to do, come say hi in the issues ! \ud83d\udc4b Also check out the CONTRIBUTING.MD \ud83e\udd13","title":"Contribute"},{"location":"contribute/#project-status","text":"The project is actively maintained and is under continues development and improvement. If you have any request or want to chat, kindly open a ticket. If you wish to contribute code and / or ideas, kindly check the contribution section.","title":"project status"},{"location":"example/","text":"Welcome to MkDocs \u2693 For full documentation visit mkdocs.org . Commands \u2693 mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout \u2693 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files. Test \u2693 Warning Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Hello this is something thos is something else Ctrl + Alt + Del Text can be deleted and replacement text added . This can also be combined into one a single operation. Highlighting is also possible and comments can be added inline . Formatting can also be applied to blocks by putting the opening and closing tags on separate lines and adding new lines between the tags and the content. Lorem ipsum[^1] dolor sit amet, consectetur adipiscing elit. 1 Task List item 1 item A item B more text item a item b item c item C item 2 item 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. \u21a9","title":"Welcome to MkDocs"},{"location":"example/#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"example/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"example/#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"example/#test","text":"Warning Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Hello this is something thos is something else Ctrl + Alt + Del Text can be deleted and replacement text added . This can also be combined into one a single operation. Highlighting is also possible and comments can be added inline . Formatting can also be applied to blocks by putting the opening and closing tags on separate lines and adding new lines between the tags and the content. Lorem ipsum[^1] dolor sit amet, consectetur adipiscing elit. 1 Task List item 1 item A item B more text item a item b item c item C item 2 item 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. \u21a9","title":"Test"},{"location":"features/","text":"Features \u2693 Full list Coming soon!","title":"Features"},{"location":"features/#features","text":"Full list Coming soon!","title":"Features"},{"location":"getting_started/","text":"Getting started \u2693 Only few simple steps are needed to get a test up and running in the cluster. The following is a step-by-step guide on how to achieve this. Step 1: Write a valid Locust test script \u2693 For this example, we will be using the following script demo_test.py from locust import HttpUser , task class User ( HttpUser ): # (1)! @task #(2)! def get_employees ( self ) -> None : \"\"\"Get a list of employees.\"\"\" self . client . get ( \"/api/v1/employees\" ) #(3)! Class representing users that will be simulated by Locust. One or more task that each simulated user will be performing. HTTP call to a specific endpoint. Note To be able to run performance tests effectivly, an understanding of Locust which is the underline load generation tool is required. All tests must be valid locust tests. Locust provide a very good and detail rich documentation that can be found here . Step 2: Write a valid custom resource for LocustTest CRD \u2693 A simple custom resource for the previous test can be something like the following example; To streamline this step, intensive-brew should be used. It is a simple cli tool that converts a declarative yaml into a compatible LocustTest kubernetes custom resource. ( Coming soon! ) locusttest-cr.yaml apiVersion : locust.io/v1 #(1)! kind : LocustTest #(2)! metadata : name : demo.test #(3)! spec : image : locustio/locust:latest #(4)! masterCommandSeed : #(5)! --locustfile /lotest/src/demo_test.py --host https://dummy.restapiexample.com --users 100 --spawn-rate 3 --run-time 3m workerCommandSeed : --locustfile /lotest/src/demo_test.py #(6)! workerReplicas : 3 #(7)! configMap : demo-test-map #(8)! API version based on the deployed LocustTest CRD. Resource kind. The name field used by the operator to infer the names of test generated resources. While this value is insignificant to the Operator itself, it is important to keep a good convention here since it helps in tracking resources across the cluster when needed. Image to use for the load generation pods Seed command for the master node. The Operator will append to this seed command/s all operational parameters needed for the master to perform its job e.g. ports, rebalancing settings, timeouts, etc... Seed command for the worker node. The Operator will append to this seed command/s all operational parameters needed for the worker to perform its job e.g. ports, master node url, master node ports, etc... The amount of worker nodes to spawn in the cluster. [Optional] Name of configMap to mount into the pod Step 3: Deploy Locust k8s Operator in the cluster. \u2693 The recommended way to install the Operator is by using the official HELM chart. Documentation on how to perform that is available here . Step 4: Deploy test as a configMap \u2693 For the purposes of this example, the demo_test.py test previously demonstrated will be deployed into the cluster as a configMap that the Operator will mount to the load generation pods. To deploy the test as a configMap, run the bellow command following this template kubectl create configmap <configMap-name> --from-file <your_test.py> : kubectl create configmap demo-test-map --from-file demo_test.py Fresh cluster resources Fresh cluster resources are allocated for each running test, meaning that tests DO NOT have any cross impact on each other. Step 5: Start the test by deploying the LocustTest custom resource. \u2693 Deploying a custom resource , signals to the Operator the desire to start a test and thus the Operator starts creating and scheduling all needed resources. To do that, deploy the custom resource following this template kubectl apply -f <valid_cr>.yaml : kubectl apply -f locusttest-cr.yaml Step 5.1: Check cluster for running resources \u2693 At this point, it is possible to check the cluster and all required resources will be running based on the passed configuration in the custom resource. The Operator will create the following resources in teh cluster for each valid custom resource: A kubernetes service for the master node so it is reachable by other worker nodes. A kubernetes Job to manage the master node. A kubernetes Job to manage the worker node. Step 6: Clear resources after test run \u2693 In order to remove the cluster resources after a test run, simply remove the custom resource and the Operator will react to this event by cleaning the cluster of all related resources. To delete a resource, run the below command following this template kubectl delete -f <valid_cr>.yaml : kubectl delete -f locusttest-cr.yaml","title":"Getting Started"},{"location":"getting_started/#getting-started","text":"Only few simple steps are needed to get a test up and running in the cluster. The following is a step-by-step guide on how to achieve this.","title":"Getting started"},{"location":"getting_started/#step-1-write-a-valid-locust-test-script","text":"For this example, we will be using the following script demo_test.py from locust import HttpUser , task class User ( HttpUser ): # (1)! @task #(2)! def get_employees ( self ) -> None : \"\"\"Get a list of employees.\"\"\" self . client . get ( \"/api/v1/employees\" ) #(3)! Class representing users that will be simulated by Locust. One or more task that each simulated user will be performing. HTTP call to a specific endpoint. Note To be able to run performance tests effectivly, an understanding of Locust which is the underline load generation tool is required. All tests must be valid locust tests. Locust provide a very good and detail rich documentation that can be found here .","title":"Step 1: Write a valid Locust test script"},{"location":"getting_started/#step-2-write-a-valid-custom-resource-for-locusttest-crd","text":"A simple custom resource for the previous test can be something like the following example; To streamline this step, intensive-brew should be used. It is a simple cli tool that converts a declarative yaml into a compatible LocustTest kubernetes custom resource. ( Coming soon! ) locusttest-cr.yaml apiVersion : locust.io/v1 #(1)! kind : LocustTest #(2)! metadata : name : demo.test #(3)! spec : image : locustio/locust:latest #(4)! masterCommandSeed : #(5)! --locustfile /lotest/src/demo_test.py --host https://dummy.restapiexample.com --users 100 --spawn-rate 3 --run-time 3m workerCommandSeed : --locustfile /lotest/src/demo_test.py #(6)! workerReplicas : 3 #(7)! configMap : demo-test-map #(8)! API version based on the deployed LocustTest CRD. Resource kind. The name field used by the operator to infer the names of test generated resources. While this value is insignificant to the Operator itself, it is important to keep a good convention here since it helps in tracking resources across the cluster when needed. Image to use for the load generation pods Seed command for the master node. The Operator will append to this seed command/s all operational parameters needed for the master to perform its job e.g. ports, rebalancing settings, timeouts, etc... Seed command for the worker node. The Operator will append to this seed command/s all operational parameters needed for the worker to perform its job e.g. ports, master node url, master node ports, etc... The amount of worker nodes to spawn in the cluster. [Optional] Name of configMap to mount into the pod","title":"Step 2: Write a valid  custom resource for LocustTest CRD"},{"location":"getting_started/#step-3-deploy-locust-k8s-operator-in-the-cluster","text":"The recommended way to install the Operator is by using the official HELM chart. Documentation on how to perform that is available here .","title":"Step 3: Deploy Locust k8s Operator in the cluster."},{"location":"getting_started/#step-4-deploy-test-as-a-configmap","text":"For the purposes of this example, the demo_test.py test previously demonstrated will be deployed into the cluster as a configMap that the Operator will mount to the load generation pods. To deploy the test as a configMap, run the bellow command following this template kubectl create configmap <configMap-name> --from-file <your_test.py> : kubectl create configmap demo-test-map --from-file demo_test.py Fresh cluster resources Fresh cluster resources are allocated for each running test, meaning that tests DO NOT have any cross impact on each other.","title":"Step 4: Deploy test as a configMap"},{"location":"getting_started/#step-5-start-the-test-by-deploying-the-locusttest-custom-resource","text":"Deploying a custom resource , signals to the Operator the desire to start a test and thus the Operator starts creating and scheduling all needed resources. To do that, deploy the custom resource following this template kubectl apply -f <valid_cr>.yaml : kubectl apply -f locusttest-cr.yaml","title":"Step 5: Start the test by deploying the LocustTest custom resource."},{"location":"getting_started/#step-51-check-cluster-for-running-resources","text":"At this point, it is possible to check the cluster and all required resources will be running based on the passed configuration in the custom resource. The Operator will create the following resources in teh cluster for each valid custom resource: A kubernetes service for the master node so it is reachable by other worker nodes. A kubernetes Job to manage the master node. A kubernetes Job to manage the worker node.","title":"Step 5.1: Check cluster for running resources"},{"location":"getting_started/#step-6-clear-resources-after-test-run","text":"In order to remove the cluster resources after a test run, simply remove the custom resource and the Operator will react to this event by cleaning the cluster of all related resources. To delete a resource, run the below command following this template kubectl delete -f <valid_cr>.yaml : kubectl delete -f locusttest-cr.yaml","title":"Step 6: Clear resources after test run"},{"location":"helm_deploy/","text":"HELM deployment \u2693 Full instructions Coming soon!","title":"Deploy Operator with HELM"},{"location":"helm_deploy/#helm-deployment","text":"Full instructions Coming soon!","title":"HELM deployment"},{"location":"how_does_it_work/","text":"How does it work \u2693 To run a performance test, basic configuration is provided through a simple and intuitive kubernetes custom resource. Once deployed the Operator does all the heavy work of creating and scheduling the resources while making sure that all created load generation pods can effectively communicate with each other. To handle the challenge of delivering test script/s from local environment to the cluster and in turn to the deployed locust pods, the Operator support dynamic volume mounting from a configMaps source. This is indicated by a simple optional configuration. Meaning, if the configuration is present, the volume is mounted, and if it is not, no volume is mounted. Since a \" Picture Is Worth a Thousand Words \", here is a gif! Steps performed in demo \u2693 Test configmap created in cluster. LocustTest CR deployed into the cluster. The Operator creating, configuring and scheduling test resources on CR creation event. The Operator cleaning up test resources after test CR has been removed event. contribution section.","title":"How does it work"},{"location":"how_does_it_work/#how-does-it-work","text":"To run a performance test, basic configuration is provided through a simple and intuitive kubernetes custom resource. Once deployed the Operator does all the heavy work of creating and scheduling the resources while making sure that all created load generation pods can effectively communicate with each other. To handle the challenge of delivering test script/s from local environment to the cluster and in turn to the deployed locust pods, the Operator support dynamic volume mounting from a configMaps source. This is indicated by a simple optional configuration. Meaning, if the configuration is present, the volume is mounted, and if it is not, no volume is mounted. Since a \" Picture Is Worth a Thousand Words \", here is a gif!","title":"How does it work"},{"location":"how_does_it_work/#steps-performed-in-demo","text":"Test configmap created in cluster. LocustTest CR deployed into the cluster. The Operator creating, configuring and scheduling test resources on CR creation event. The Operator cleaning up test resources after test CR has been removed event. contribution section.","title":"Steps performed in demo"},{"location":"license/","text":"License \u2693 Open source licensed under Apache-2.0 license (see LICENSE file for details).","title":"License"},{"location":"license/#license","text":"Open source licensed under Apache-2.0 license (see LICENSE file for details).","title":"License"},{"location":"metrics_and_dashboards/","text":"Metrics & Dashboards \u2693 Full instructions Coming soon!","title":"Metrics & Dashboards"},{"location":"metrics_and_dashboards/#metrics-dashboards","text":"Full instructions Coming soon!","title":"Metrics &amp; Dashboards"},{"location":"roadmap/","text":"Roadmap \u2693 Not in a particular order (list wis updated when features are implemented / planned): Add traceability labels to generated resources Support for deploying test resources with node affinity / node taints Dashboard examples (Grafana + prometheus configuration) Enable event driven actions Integration with MSTeams: Push notification on test run completion / termination events UNDER_INVESTIGATION Benchmarking and collection of non-test generated metrics Investigation is on going to study the feasibility of supplying Locust pods with external metrics that are collected from service system under-test. Test pods can then use this information to assess pass / fail criteria. This is especially useful in non-REST based services e.g. assess kafka(streams) microservice based on its consumer lag performance coming from the kafka broker.","title":"Roadmap"},{"location":"roadmap/#roadmap","text":"Not in a particular order (list wis updated when features are implemented / planned): Add traceability labels to generated resources Support for deploying test resources with node affinity / node taints Dashboard examples (Grafana + prometheus configuration) Enable event driven actions Integration with MSTeams: Push notification on test run completion / termination events UNDER_INVESTIGATION Benchmarking and collection of non-test generated metrics Investigation is on going to study the feasibility of supplying Locust pods with external metrics that are collected from service system under-test. Test pods can then use this information to assess pass / fail criteria. This is especially useful in non-REST based services e.g. assess kafka(streams) microservice based on its consumer lag performance coming from the kafka broker.","title":"Roadmap"}]}